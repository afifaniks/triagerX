{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "emMMsF-tnEFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install python-dotenv\n",
        "!python3 -m pip install wandb\n",
        "!pip install loguru"
      ],
      "metadata": {
        "id": "bKO_8txVpjw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login e7c8c5e645594de1e62241483f608a2c4a485fd5"
      ],
      "metadata": {
        "id": "gM-HKtdc-z94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzMpvhQanBLs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.combine import SMOTEENN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# internal imports"
      ],
      "metadata": {
        "id": "trAvNuQlwKcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.dataset.processor import DatasetProcessor\n",
        "# from triagerx.model.roberta_cnn import RobertaCNNClassifier\n",
        "# from triagerx.model.roberta_fcn import RobertaFCNClassifier\n",
        "# from triagerx.trainer.train_config import TrainConfig\n",
        "# from triagerx.trainer.model_trainer import ModelTrainer\n",
        "# from triagerx.evaluation.evaluator import Evaluator\n",
        "# from triagerx.dataset.triage_dataset import TriageDataset"
      ],
      "metadata": {
        "id": "uO0CON62_UZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DatasetProcessor"
      ],
      "metadata": {
        "id": "FsCGurZG_eEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.dataset.processor import DatasetProcessor\n",
        "\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "\n",
        "\n",
        "class DatasetProcessor:\n",
        "    @staticmethod\n",
        "    def load_dataframe(path: str) -> pd.DataFrame:\n",
        "        logger.debug(f\"Loading dataframe: {path}\")\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        logger.debug(\"Cleaning dataset...\")\n",
        "        df[\"text\"] = df[\"text\"].str.replace(\n",
        "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
        "            \" \",\n",
        "        )\n",
        "        df[\"text\"] = df[\"text\"].str.replace(\" +\", \" \", regex=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_dataframe(df: pd.DataFrame, sample_threshold: int = 0) -> pd.DataFrame:\n",
        "        logger.debug(\n",
        "            f\"Filtering developers based on minimum contribution: {sample_threshold}...\"\n",
        "        )\n",
        "        df = df[df[\"assignees\"].notna()]\n",
        "        developers = df[\"assignees\"].value_counts()\n",
        "        filtered_developers = developers.index[developers >= sample_threshold]\n",
        "        df = df[df[\"assignees\"].isin(filtered_developers)]\n",
        "\n",
        "        logger.debug(\"Generating 'text' field...\")\n",
        "        df[\"text\"] = df.apply(\n",
        "            lambda x: \"Title: \"\n",
        "            + str(x[\"issue_title\"])\n",
        "            + \"\\nLabels: \"\n",
        "            + str(x[\"labels\"])\n",
        "            + \"\\nDescription: \"\n",
        "            + str(x[\"issue_body\"]),\n",
        "            axis=1,\n",
        "        )\n",
        "\n",
        "        min_length = 15\n",
        "        logger.debug(f\"Dropping rows with 'text' length < {min_length}...\")\n",
        "        df = df[df[\"text\"].str.len().gt(min_length)]\n",
        "\n",
        "        df[\"owner_id\"] = pd.factorize(df[\"assignees\"])[0]\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def process_dataset(path: str, sample_threshold: int = 0) -> pd.DataFrame:\n",
        "        df = DatasetProcessor.load_dataframe(path=path)\n",
        "        df = DatasetProcessor.prepare_dataframe(\n",
        "            df=df, sample_threshold=sample_threshold\n",
        "        )\n",
        "        df = DatasetProcessor.clean_data(df=df)\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "gpoVIZguriXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RobertaCNNClassifier"
      ],
      "metadata": {
        "id": "6RYKpkKv_sNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.model.roberta_cnn import RobertaCNNClassifier\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "class RobertaCNNClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name: str, output_size, embed_size=1024, dropout=0.1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.base_model = RobertaModel.from_pretrained(\n",
        "            model_name, output_hidden_states=True\n",
        "        )\n",
        "        filter_sizes = [3, 4, 5, 6]\n",
        "        num_filters = 256\n",
        "        self._tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self._convs = nn.ModuleList(\n",
        "            [nn.Conv2d(4, num_filters, (K, embed_size)) for K in filter_sizes]\n",
        "        )\n",
        "        self._dropout = nn.Dropout(dropout)\n",
        "        self._fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "        self._relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.base_model(input_ids, attention_mask=attention_mask)[2][-4:]\n",
        "        x = torch.stack(x, dim=1)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self._convs]\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self._dropout(x)\n",
        "        logit = self._fc(x)\n",
        "\n",
        "        return logit\n",
        "\n",
        "    def tokenizer(self) -> RobertaTokenizer:\n",
        "        return self._tokenizer\n"
      ],
      "metadata": {
        "id": "N0eMlWy3r6BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RobertaFCNClassifier"
      ],
      "metadata": {
        "id": "st_r16KF_12v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.model.roberta_fcn import RobertaFCNClassifier\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class RobertaFCNClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        output_size,\n",
        "        embed_size=1024,\n",
        "        dropout=0.1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.base = AutoModel.from_pretrained(model_name)\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.linear = nn.Linear(embed_size, embed_size // 2)\n",
        "        self.linear2 = nn.Linear(embed_size // 2, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, attention_mask):\n",
        "        _, pooler_out = self.base(\n",
        "            input_ids=input_id, attention_mask=attention_mask, return_dict=False\n",
        "        )\n",
        "        pooler_out = self.relu(pooler_out)\n",
        "        drop_out = self.dropout(pooler_out)\n",
        "        linear_out = self.linear(drop_out)\n",
        "        linear_out = self.relu(linear_out)\n",
        "        drop_out = self.dropout2(linear_out)\n",
        "        linear_out = self.linear2(drop_out)\n",
        "\n",
        "        return linear_out\n",
        "\n",
        "    def tokenizer(self) -> PreTrainedTokenizer:\n",
        "        return self._tokenizer"
      ],
      "metadata": {
        "id": "IgqH8-Asr6q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainConfig"
      ],
      "metadata": {
        "id": "qArzW_ge_8hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.trainer.train_config import TrainConfig\n",
        "\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "class TrainConfig(BaseModel):\n",
        "    optimizer: Optimizer\n",
        "    criterion: nn.Module\n",
        "    train_dataset: pd.DataFrame\n",
        "    validation_dataset: pd.DataFrame\n",
        "    learning_rate: float\n",
        "    batch_size: int\n",
        "    epochs: int\n",
        "    output_file: str\n",
        "    scheduler: Optional[Any] = None\n",
        "    sampler: Optional[Sampler] = None\n",
        "    wandb: Optional[Dict] = None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n"
      ],
      "metadata": {
        "id": "JfsqsR8ZrnI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelTrainer"
      ],
      "metadata": {
        "id": "29HIHABYBRkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.trainer.model_trainer import ModelTrainer\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from loguru import logger\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import wandb\n",
        "# from triagerx.dataset.triage_dataset import TriageDataset\n",
        "# from triagerx.trainer.train_config import TrainConfig\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, config: TrainConfig):\n",
        "        self._config = config\n",
        "\n",
        "    def _init_wandb(self):\n",
        "        wandb.init(**self._config.wandb)\n",
        "\n",
        "    def train(self, model: nn.Module):\n",
        "        tokenizer = model.tokenizer()\n",
        "        criterion = self._config.criterion\n",
        "        optimizer = self._config.optimizer\n",
        "        train_data = self._config.train_dataset\n",
        "        validation_data = self._config.validation_dataset\n",
        "        sampler = self._config.sampler\n",
        "\n",
        "        train = TriageDataset(train_data, tokenizer)\n",
        "        val = TriageDataset(validation_data, tokenizer)\n",
        "\n",
        "        if self._config.wandb:\n",
        "            logger.debug(\"Initializing wandb...\")\n",
        "            self._init_wandb()\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            dataset=train,\n",
        "            batch_size=self._config.batch_size,\n",
        "            shuffle=False if sampler else True,\n",
        "            sampler=sampler,\n",
        "        )\n",
        "        val_dataloader = DataLoader(val, batch_size=self._config.batch_size)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        best_loss = float(\"inf\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logger.debug(f\"Selected compute device: {device}\")\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "        for epoch_num in range(self._config.epochs):\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input[\"attention_mask\"].to(device)\n",
        "                input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input[\"attention_mask\"].to(device)\n",
        "                    input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "\n",
        "                    all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
        "                    all_labels.append(val_label.cpu().numpy())\n",
        "\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            all_preds = np.concatenate(all_preds)\n",
        "            all_labels = np.concatenate(all_labels)\n",
        "\n",
        "            precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
        "                all_preds, all_labels, average=\"macro\"\n",
        "            )\n",
        "\n",
        "            self._log_step(\n",
        "                epoch_num,\n",
        "                total_acc_train,\n",
        "                total_acc_val,\n",
        "                total_loss_train,\n",
        "                total_loss_val,\n",
        "                precision,\n",
        "                recall,\n",
        "                f1_score,\n",
        "                train_data,\n",
        "                validation_data,\n",
        "            )\n",
        "\n",
        "            val_loss = total_loss_val / len(validation_data)\n",
        "\n",
        "            if self._config.scheduler:\n",
        "                self._config.scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                logger.success(\"Found new best model. Saving weights...\")\n",
        "                torch.save(model.state_dict(), self._config.output_file)\n",
        "                best_loss = val_loss\n",
        "\n",
        "        if self._config.wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "    def _log_step(\n",
        "        self,\n",
        "        epoch_num,\n",
        "        total_acc_train,\n",
        "        total_acc_val,\n",
        "        total_loss_train,\n",
        "        total_loss_val,\n",
        "        precision,\n",
        "        recall,\n",
        "        f1_score,\n",
        "        train_data,\n",
        "        validation_data,\n",
        "    ):\n",
        "        log = f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                    | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                    | Val Loss: {total_loss_val / len(validation_data): .3f} \\\n",
        "                    | Val Accuracy: {total_acc_val / len(validation_data): .3f} \\\n",
        "                    | Precision: {precision: .3f} \\\n",
        "                    | Recall: {recall: .3f} \\\n",
        "                    | F1-score: {f1_score: .3f}\"\n",
        "\n",
        "        logger.info(log)\n",
        "\n",
        "        if self._config.wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_acc\": total_acc_train / len(train_data),\n",
        "                    \"train_loss\": total_loss_train / len(train_data),\n",
        "                    \"val_acc\": total_acc_val / len(validation_data),\n",
        "                    \"val_loss\": total_loss_val / len(validation_data),\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"f1-score\": f1_score,\n",
        "                }\n",
        "            )\n"
      ],
      "metadata": {
        "id": "NRUP5geBsAbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluator"
      ],
      "metadata": {
        "id": "rR_bTR4wBfYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.evaluation.evaluator import Evaluator\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from loguru import logger\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def calculate_top_k_accuray(\n",
        "        self, model: nn.Module, k: int, X_test: pd.DataFrame, y_test: np.array\n",
        "    ):\n",
        "        has_cuda = torch.cuda.is_available()\n",
        "\n",
        "        if has_cuda:\n",
        "            model = model.cuda()\n",
        "\n",
        "        tokenizer = model.tokenizer()\n",
        "\n",
        "        y_preds = []\n",
        "\n",
        "        logger.debug(\"Calculating predications...\")\n",
        "        for i in range(len(X_test)):\n",
        "            dx = X_test.iloc[i]\n",
        "\n",
        "            data = tokenizer(\n",
        "                dx[\"text\"], padding=\"max_length\", max_length=512, truncation=True\n",
        "            )\n",
        "            ids, mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
        "\n",
        "            ids = torch.tensor([ids])\n",
        "            mask = torch.tensor([mask])\n",
        "\n",
        "            if has_cuda:\n",
        "                ids = ids.cuda()\n",
        "                mask = mask.cuda()\n",
        "\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_pred = softmax(model(ids, mask))\n",
        "\n",
        "            y_preds.append(y_pred)\n",
        "\n",
        "        y_numpy = []\n",
        "\n",
        "        for y in y_preds:\n",
        "            y_numpy.append(y.cpu().numpy())\n",
        "\n",
        "        y_preds = np.array(y_numpy)[:, 0, :]\n",
        "\n",
        "        logger.debug(f\"Calculating top {k} score...\")\n",
        "\n",
        "        score = top_k_accuracy_score(y_test, y_preds, k=k)\n",
        "        logger.info(f\"Top {k} score: {score}\")\n",
        "\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "VYqZuiGRBh4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TriageDataset"
      ],
      "metadata": {
        "id": "EGXcduId_mcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from triagerx.dataset.triage_dataset import TriageDataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from loguru import logger\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class TriageDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        feature: str = \"text\",\n",
        "        target: str = \"owner_id\",\n",
        "    ):\n",
        "        logger.debug(\"Generating torch dataset...\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.labels = [label for label in df[target]]\n",
        "        logger.debug(\"Tokenizing texts...\")\n",
        "        self.texts = [\n",
        "            self.tokenizer(\n",
        "                text,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            for text in df[feature]\n",
        "        ]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n"
      ],
      "metadata": {
        "id": "iV6uqATXsoY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc8RNv0CnBLw"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBaBV-RZnBLy"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/gdrive/MyDrive/TriagerX/data/openj9/merged_data_dated.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Bc0yn4nBLz"
      },
      "outputs": [],
      "source": [
        "sample_threshold = 5\n",
        "\n",
        "df = DatasetProcessor.process_dataset(dataset_path, sample_threshold=sample_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTR5BAT7nBL1"
      },
      "outputs": [],
      "source": [
        "df[\"assignees\"].value_counts().plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoPfGc6nnBL1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"owner_id\"])\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.2, stratify=train_df[\"owner_id\"])\n",
        "\n",
        "train_df = train_df.fillna(\"\")\n",
        "valid_df = valid_df.fillna(\"\")\n",
        "test_df = test_df.fillna(\"\")\n",
        "\n",
        "train_df = train_df.iloc[:, [2,3,4,5,6,7,8,9,12,13]]\n",
        "valid_df = valid_df.iloc[:, [2,3,4,5,6,7,8,9,12,13]]\n",
        "test_df = test_df.iloc[:, [2,3,4,5,6,7,8,9,12,13]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorizer = CountVectorizer()\n",
        "# vectorizer.fit(train_df.iloc[:, [2]] .values.ravel())\n",
        "# X_train=vectorizer.transform(train_df.iloc[:, [2]] .values.ravel())\n",
        "# X_test=vectorizer.transform(test_df.iloc[:, [2]] .values.ravel())\n",
        "# X_train\n",
        "# # X_train=X_train.toarray()\n",
        "# # X_test=X_test.toarray()"
      ],
      "metadata": {
        "id": "H44lFU1IyHHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sme = SMOTEENN(random_state=42)\n",
        "# smtom = SMOTETomek(random_state=42)\n",
        "# x_train = pd.DataFrame(X_train)\n",
        "# X_resampled, y_resampled = smtom.fit_resample(X_train, train_df.loc[:, [\"owner_id\"]])"
      ],
      "metadata": {
        "id": "zD_LNOjQxkV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "# counter = Counter(train_df.loc[:, :\"text\"])\n",
        "# counter"
      ],
      "metadata": {
        "id": "4iIWFQppqjg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaDhZcVUnBL2"
      },
      "outputs": [],
      "source": [
        "assert len(train_df.owner_id.unique()) == len(test_df.owner_id.unique()) == len(valid_df.owner_id.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5wlZX9ZnBL2"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwaxw1nOnBL3"
      },
      "outputs": [],
      "source": [
        "model = RobertaFCNClassifier(\n",
        "    model_name=\"roberta-large\",\n",
        "    output_size=len(train_df.owner_id.unique()),\n",
        "    embed_size=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A8cn2N2nBL3"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoalDwUFnBL4"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(train_df[\"owner_id\"])\n",
        "num_samples = sum(class_counts)\n",
        "labels = train_df[\"owner_id\"].to_list() #corresponding labels of samples\n",
        "\n",
        "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1-ui8VJ6Tjz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Pypt0Qv0Tw2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continue from saved states\n",
        "\n",
        "# MODEL_PATH = \"/content/gdrive/MyDrive/TriagerX/output/RobertaFCNClassifier_5_stratify_labels_WeightedRandomSampler.pt\"\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#   model.load_state_dict(torch.load(MODEL_PATH))\n",
        "# else:\n",
        "#   model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "Qs9V9M_p6fuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkxVwf52nBL4"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-5\n",
        "epochs = 7 # up to 35 runs\n",
        "batch_size = 7\n",
        "\n",
        "# Create sampler\n",
        "# counts = np.bincount(train_df[\"owner_id\"])\n",
        "# labels_weights = 1. / counts\n",
        "# weights = labels_weights[train_df[\"owner_id\"]]\n",
        "# sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "sampler_name = sampler.__class__.__name__ if sampler else \"None\"\n",
        "model_name = model.__class__.__name__\n",
        "\n",
        "output_file = f\"{model_name}_{sample_threshold}_stratify_labels_{sampler_name}\"\n",
        "output_path = f\"/content/gdrive/MyDrive/TriagerX/output/{output_file}.pt\"\n",
        "\n",
        "wandb_config = {\n",
        "        \"project\": \"triagerx\",\n",
        "        \"name\": f\"run_{output_file}\",\n",
        "        \"config\": {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": \"Roberta-CNN\",\n",
        "        \"dataset\": \"openj9\",\n",
        "        \"epochs\": epochs,\n",
        "    }\n",
        "}\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=10, factor=0.1, threshold=1e-8)\n",
        "\n",
        "train_config = TrainConfig(\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_dataset=train_df,\n",
        "    validation_dataset=valid_df,\n",
        "    learning_rate=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    output_file=output_path,\n",
        "    sampler=sampler,\n",
        "    scheduler=scheduler,\n",
        "    wandb=wandb_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saved runs"
      ],
      "metadata": {
        "id": "-bicEQlPH735"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer = ModelTrainer(train_config)\n",
        "# trainer.train(model=model)"
      ],
      "metadata": {
        "id": "1JeWTnfw1fRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer = ModelTrainer(train_config)\n",
        "# trainer.train(model=model)\n"
      ],
      "metadata": {
        "id": "tQ4EacW7qwCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# current run"
      ],
      "metadata": {
        "id": "UTLBl09LH_ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ModelTrainer(train_config)\n",
        "trainer.train(model=model)\n"
      ],
      "metadata": {
        "id": "YpOPlu2opxYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "fcyvTCf0UEup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n"
      ],
      "metadata": {
        "id": "gcROFgNH-Xgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81pa-McxnBL5"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ZUZyaOuNd9wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from loguru import logger\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def calculate_top_k_accuray(\n",
        "        self, model: nn.Module, k: int, X_test: pd.DataFrame, y_test: np.array\n",
        "    ):\n",
        "        has_cuda = torch.cuda.is_available()\n",
        "\n",
        "        if has_cuda:\n",
        "            model = model.cuda()\n",
        "\n",
        "        tokenizer = model.tokenizer()\n",
        "\n",
        "        y_preds = []\n",
        "\n",
        "        logger.debug(\"Calculating predications...\")\n",
        "        for i in range(len(X_test)):\n",
        "            dx = X_test.iloc[i]\n",
        "\n",
        "            data = tokenizer(\n",
        "                dx[\"text\"], padding=\"max_length\", max_length=512, truncation=True\n",
        "            )\n",
        "            ids, mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
        "\n",
        "            ids = torch.tensor([ids])\n",
        "            mask = torch.tensor([mask])\n",
        "\n",
        "            if has_cuda:\n",
        "                ids = ids.cuda()\n",
        "                mask = mask.cuda()\n",
        "\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_pred = softmax(model(ids, mask))\n",
        "\n",
        "            y_preds.append(y_pred)\n",
        "\n",
        "        y_numpy = []\n",
        "\n",
        "        for y in y_preds:\n",
        "            y_numpy.append(y.cpu().numpy())\n",
        "\n",
        "        y_preds = np.array(y_numpy)[:, 0, :]\n",
        "\n",
        "        logger.debug(f\"Calculating top {k} score...\")\n",
        "\n",
        "        score = top_k_accuracy_score(y_test, y_preds, k=k)\n",
        "        logger.info(f\"Top {k} score: {score}\")\n",
        "\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "Xvdd5RDwd2kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = RobertaCNNClassifier(\n",
        "#     model_name=\"roberta-base\",\n",
        "#     output_size=len(train_df.owner_id.unique()),\n",
        "#     embed_size=768\n",
        "# )\n",
        "\n",
        "model = RobertaFCNClassifier(\n",
        "    model_name=\"roberta-large\",\n",
        "    output_size=len(train_df.owner_id.unique()),\n",
        "    embed_size=1024\n",
        ")\n"
      ],
      "metadata": {
        "id": "YhYGe44qO7xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/gdrive/MyDrive/TriagerX/output/RobertaFCNClassifier_5_stratify_labels_WeightedRandomSampler.pt\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.load_state_dict(torch.load(MODEL_PATH))\n",
        "else:\n",
        "  model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "id": "5Houu2n-O_WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluator()\n"
      ],
      "metadata": {
        "id": "llo8ik7jPDui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBA8n2znBL5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = TriageDataset(test_df, model.tokenizer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkbcq7CdnBL6"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(dataset, 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q1pmVOZnBL6"
      },
      "outputs": [],
      "source": [
        "test_df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYzXdFj8nBL6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for val_input, val_label in loader:\n",
        "        val_label = val_label.to(device)\n",
        "        mask = val_input[\"attention_mask\"].to(device)\n",
        "        input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "        output = nn.Softmax(dim=1)(output)\n",
        "        conf, classes = output.topk(10, dim=1)\n",
        "\n",
        "        print(\"Pred\", output.argmax(dim=1))\n",
        "        print(\"Label\", val_label.long())\n",
        "        print(\"Top 3\", classes)\n",
        "\n",
        "        # batch_loss = criterion(output, val_label.long())\n",
        "        # total_loss_val += batch_loss.item()\n",
        "\n",
        "        # acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "\n",
        "        # all_preds.append(output.cpu().numpy())\n",
        "        # all_labels.append(val_label.cpu().numpy())\n",
        "\n",
        "        # total_acc_val += acc\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijzdSb-CnBL7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXVDb9ZKnBL7"
      },
      "outputs": [],
      "source": [
        "x_df = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "975OtI12nBL7"
      },
      "outputs": [],
      "source": [
        "x_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naR517i3nBL7"
      },
      "outputs": [],
      "source": [
        "x_df[\"assignee_w_id\"] = x_df.apply(lambda x: str(x[\"assignees\"]) + \" \" + str(x[\"owner_id\"]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOsbp2wjnBL7"
      },
      "outputs": [],
      "source": [
        "x_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAS3mQyfnBL7"
      },
      "outputs": [],
      "source": [
        "x_df[\"assignee_w_id\"].value_counts()[:20].plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl3rILMPnBL8"
      },
      "outputs": [],
      "source": [
        "def predict(data, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_input, val_label in loader:\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_input[\"attention_mask\"].to(device)\n",
        "            input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "            output = nn.Softmax(dim=1)(output)\n",
        "            # conf, classes = output.topk(3, dim=1)\n",
        "\n",
        "            return output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X7MIxJnnBL8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hk2EuClnBL8"
      },
      "outputs": [],
      "source": [
        "test_df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jemXfSfnBL8"
      },
      "outputs": [],
      "source": [
        "test_df[25:26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb0uc_VjnBL8"
      },
      "outputs": [],
      "source": [
        "dx = test_df[25:26]\n",
        "print(dx[\"assignees\"])\n",
        "\n",
        "dataset = TriageDataset(dx, model.tokenizer())\n",
        "\n",
        "predict(dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vp2NJXCnBL8"
      },
      "outputs": [],
      "source": [
        "# all_preds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xDbew8enBL9"
      },
      "outputs": [],
      "source": [
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqB6R_1wnBL9"
      },
      "outputs": [],
      "source": [
        "# set(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_ZmzJIYnBL9"
      },
      "outputs": [],
      "source": [
        "all_preds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2rouhwknBL9"
      },
      "outputs": [],
      "source": [
        "sum(all_preds == all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71nMgTQenBL9"
      },
      "outputs": [],
      "source": [
        "# set(all_labels) - set(all_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXG7PSN5nBL9"
      },
      "outputs": [],
      "source": [
        "# Top 3 Predictions\n",
        "evaluator.calculate_top_k_accuray(model, k=3, X_test=test_df, y_test=test_df[\"owner_id\"].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bsVxUAqnBL-"
      },
      "outputs": [],
      "source": [
        "# Top 5 Predictions\n",
        "evaluator.calculate_top_k_accuray(model, k=5, X_test=test_df, y_test=test_df[\"owner_id\"].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp2tp0lHnBMD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nnwnL5gnBMD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FsCGurZG_eEq",
        "6RYKpkKv_sNG",
        "st_r16KF_12v",
        "qArzW_ge_8hf",
        "29HIHABYBRkT",
        "rR_bTR4wBfYu",
        "EGXcduId_mcp",
        "lc8RNv0CnBLw"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}