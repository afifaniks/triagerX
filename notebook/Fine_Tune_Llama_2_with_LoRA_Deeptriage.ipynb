{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afifaniks/triagerX/blob/main/notebook/Fine_Tune_Llama_2_with_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Sep 18 01:26:52 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100 80G...  On   | 00000000:17:00.0 Off |                    0 |\n",
            "| N/A   35C    P0    53W / 300W |      0MiB / 80994MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2bggKK3lgKs",
        "outputId": "d570e997-6c69-4edd-b431-8ec244130613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate==0.21.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (0.21.0)\n",
            "Requirement already satisfied: peft==0.4.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (0.4.0)\n",
            "Requirement already satisfied: bitsandbytes==0.40.2 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (0.40.2)\n",
            "Requirement already satisfied: transformers==4.31.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (4.31.0)\n",
            "Requirement already satisfied: trl==0.4.7 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (0.4.7)\n",
            "Requirement already satisfied: torch in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (2.0.1)\n",
            "Requirement already satisfied: scipy in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (1.11.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from accelerate==0.21.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from accelerate==0.21.0) (23.1)\n",
            "Requirement already satisfied: psutil in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from accelerate==0.21.0) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: safetensors in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from peft==0.4.0) (0.3.3)\n",
            "Requirement already satisfied: filelock in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (3.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (0.17.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (2023.8.8)\n",
            "Requirement already satisfied: requests in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: datasets in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from trl==0.4.7) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
            "Requirement already satisfied: wheel in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
            "Requirement already satisfied: cmake in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (0.3.7)\n",
            "Requirement already satisfied: pandas in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (2.1.0)\n",
            "Requirement already satisfied: xxhash in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from datasets->trl==0.4.7) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.7) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.7) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.7) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.7) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 torch scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TmE5BjO3lzF8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    StoppingCriteria, \n",
        "    StoppingCriteriaList\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUq6owChvS7v"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wYIkKHSTmeku"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "data_path = \"data/deeptriage/classifier_data_0.csv\"\n",
        "new_model = \"llama-2-7b-deeptriage\"\n",
        "\n",
        "# Set QLoRA configuration\n",
        "lora_r = 64 # Attention dimension/rank\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "\n",
        "# Set bitsandbytes configuration\n",
        "use_4bit = True #For  4-bit precision base model loading\n",
        "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
        "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
        "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
        "\n",
        "\n",
        "# Set training params\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 8\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = 250\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True # Group sequences into batches with same length saves memory and speeds up training considerably\n",
        "save_steps = 0\n",
        "logging_steps = 10\n",
        "\n",
        "# Set SFT parameters\n",
        "max_seq_length = None\n",
        "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
        "device_map = {\"\": 0} # Load the entire model on the GPU 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3CTCj2Ltw57V"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMFwBu5vxy39"
      },
      "source": [
        "## Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "19f2acdc4ebd43a1b21a89fa8c79d90e",
            "c2128587cf4f4469a0f1f27e5f51628b",
            "8d5480f9851349728618636a3185eb13",
            "899a26acba144449bc7b590417c561f8",
            "ca50e1cc35a24f6cbae9ecf8ba070d24",
            "81d8ea37dec8477bb10fc32d23dbbc9b",
            "8f71bdb314994245a82e563aff25f760",
            "b154512f829e42ef9fbad60e3d6ce25b",
            "d211d4e7e67c497a9776178ff4c4ff81",
            "1f093aaa5a444bcfa582c989783453df",
            "74501a139b2947148f56bbd1eae0478d"
          ]
        },
        "id": "sDZHUos2xh5-",
        "outputId": "662eb7d5-5ff1-480e-baec-0fcc0a8958f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3edb92f74124605b79e9e13a26885e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config, # Using it for optimized model loading\n",
        "    device_map=device_map\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i8ZJWPfUOGlX"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(model, tokenizer, prompt, max_length=200):\n",
        "  pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length, stopping_criteria=stopping_criteria)\n",
        "  result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "\n",
        "  return result[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "Input length of input_ids is 265, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: All default search engine settings were wiped out\n",
            "Issue Description: The DhcpProxyScriptFetcher implementation (net/proxy/dhcp_proxy_script_fetcher_win.cc and net/proxy/dhcp_proxy_script_adapter_fetcher_win.cc) currently uses base::WorkerPool, which we plan to deprecate (see issue 251774).It would also make a lot of sense to use only a limited number of threads to fetch DHCP PAC information.Therefore, this bug tracks switching this implementation to use base::SequencedWorkerPool with a limit of 10 threads, that is owned by the DhcpProxyScriptFetcher and handed to each DhcpProxyScriptAdapterFetcher.  An equivalent to the current base::WorkerPool::PostTaskAndReply should be to PostTaskAndReply on a TaskRunner retrieved using SequencedWorkerPool::GetTaskRunnerWithShutdownBehavior(CONTINUE_ON_SHUTDOWN).This should be relatively straightforward and I have a possible volunteer in mind, so I'm tagging it as a GoodFirstBug. Who can fix it? [/INST] \n"
          ]
        }
      ],
      "source": [
        "print(inference(model, tokenizer, \"Issue Title: All default search engine settings were wiped out\\nIssue Description: The DhcpProxyScriptFetcher implementation (net/proxy/dhcp_proxy_script_fetcher_win.cc and net/proxy/dhcp_proxy_script_adapter_fetcher_win.cc) currently uses base::WorkerPool, which we plan to deprecate (see issue 251774).It would also make a lot of sense to use only a limited number of threads to fetch DHCP PAC information.Therefore, this bug tracks switching this implementation to use base::SequencedWorkerPool with a limit of 10 threads, that is owned by the DhcpProxyScriptFetcher and handed to each DhcpProxyScriptAdapterFetcher.  An equivalent to the current base::WorkerPool::PostTaskAndReply should be to PostTaskAndReply on a TaskRunner retrieved using SequencedWorkerPool::GetTaskRunnerWithShutdownBehavior(CONTINUE_ON_SHUTDOWN).This should be relatively straightforward and I have a possible volunteer in mind, so I'm tagging it as a GoodFirstBug. Who can fix it?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpYryLFKzpIn"
      },
      "source": [
        "## Setup Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vgnELGJpPnA9"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"csv\", data_files=data_path, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_dataset(data):\n",
        "    output_texts = []\n",
        "    \n",
        "    for i in range(len(data[\"issue_title\"])):\n",
        "        formatted_text = f\"<s><INST>Issue Title:\\n{data['issue_title'][i]}\" \\\n",
        "        + f\"\\nIssue Description:\\n{data['description'][i]}\\nWho can fix this issue?\\n</INST>The issue can be fixed by: {data['owner'][i]}</s>\"\n",
        "        output_texts.append(formatted_text)\n",
        "        print(formatted_text)\n",
        "\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgrZaZ4xPoQk",
        "outputId": "8d38fa84-6ffb-43c8-88a3-bb699f5c34e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "    else:\n",
        "      print(f\"Using {compute_dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "BXbdmdjSx2Dl",
        "outputId": "a14f260a-8f57-4485-bdf4-a8da595d3e9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 05:19, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.191600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.409500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.142000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.813600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.128000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.122000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.746600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.969100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.983400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.904200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.004900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.904700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.670600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.595800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.984900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.908500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.665500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=packing,\n",
        "    formatting_func=format_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkv7uLpS9rsK"
      },
      "source": [
        "## Memory Cleanup to Save Fine-Tuned Model (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIHF0-tN9DIK"
      },
      "outputs": [],
      "source": [
        "# del model\n",
        "# del trainer\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSVDDPzl-DxO"
      },
      "source": [
        "## Merge LoRA Weights with Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('NousResearch/Llama-2-7b-chat-hf', 'llama-2-7b-deeptriage')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name, new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1833fb4ab4684ab681b1ef3bfd4784ae",
            "7e7e3e6269474d0b8aab1904950011c9",
            "0a6fb80cef35403e8e63d3263b90b56d",
            "c53be97cf137424ca62a88525c999eb7",
            "eb45e61e696d4432a9ff9ad17c72c573",
            "0fbf0ff9aacf4811be576bf3551a3084",
            "fa65424238124b3895a379f7c455c255",
            "f2735dc579e748418d8177984f9d515f",
            "707c14d9b1f74795a2c806bc9cb1df48",
            "7396ee85e9184547af60a1051f57b37c",
            "f471423ca79b4834a259ac8764f984cc"
          ]
        },
        "id": "k6pB8nlZ99wQ",
        "outputId": "943e4d66-e5f8-43d4-b9c2-e906dc98171f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b60eed55c144277b2226ae0242a3c14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: NaCl 3D busted - command buffer initialization failure\n",
            "Issue Description: What steps will reproduce the problem?1. Start Chrome2. Attempt to run a Native Client 3D module (e.g. angrybots)3. Aw, snap. Note line Failed to create context in log below.I have only tested this on MacOS. Two most recent canary builds (16.0.909, 16.0.910) fail, as well as head of tree. I have a browser build @105335 (13 Oct) and it works fine, so it appears to be a change between 105335 and whatever the rev for Canary 16.0.909 would be.dhcp-172-19-0-123:src bradchen$ /Applications/Google\\ Chrome\\ Canary.app/Contents/MacOS/Google\\ Chrome\\ Canary  --enable-nacl[34896:2307:20612385773456:ERROR:process_util_mac.mm(283)] Invalid process[34907,2954604544:04:52:38.029878] NaCl_page_alloc_randomized: 0xde7a2a0b[34907,2954604544:04:52:38.030035] NaCl_page_alloc_randomized: hint 0x5e7a0000[34907,2954604544:04:52:38.030136] NaClMakePcrelThunk: got addr 0x5e7a0000[34903,2953392128:04:52:38.099973] PluginReverseInterface::StartupInitializationComplete[34903,2953392128:04:52:38.100020] PluginReverseInterface::StartupInitializationComplete: invoking CBUnityModule.[34909:263:20627425695745:ERROR:gpu_command_buffer_stub.cc(225)] Failed to create context.[34903:263:20627425900735:ERROR:command_buffer_proxy.cc(136)] Failed to initialize command buffer service.[SRPC:NACL:34907,1056900288:11:52:38.539000] NaClSrpcRpcWait(channel=0x3efc0bb0): EOF is received instead of response. Probably, the other side (usually, nacl module or browser plugin) crashed.  Who can this issue [/INST]  This issue can be assigned to: bradchen@chromium.org \n",
            "Who can fix this issue?\n",
            "The issue can be fixed by: bradchen@chromium.org \n"
          ]
        }
      ],
      "source": [
        "issue_title = \"NaCl 3D busted - command buffer initialization failure\"\n",
        "issue_description = \"What steps will reproduce the problem?1. Start Chrome2. Attempt to run a Native Client 3D module (e.g. angrybots)3. \"\"Aw, snap\"\". Note line \"\"Failed to create context\"\" in log below.I have only tested this on MacOS. Two most recent canary builds (16.0.909, 16.0.910) fail, as well as head of tree. I have a browser build @105335 (13 Oct) and it works fine, so it appears to be a change between 105335 and whatever the rev for Canary 16.0.909 would be.dhcp-172-19-0-123:src bradchen$ /Applications/Google\\ Chrome\\ Canary.app/Contents/MacOS/Google\\ Chrome\\ Canary  --enable-nacl[34896:2307:20612385773456:ERROR:process_util_mac.mm(283)] Invalid process[34907,2954604544:04:52:38.029878] NaCl_page_alloc_randomized: 0xde7a2a0b[34907,2954604544:04:52:38.030035] NaCl_page_alloc_randomized: hint 0x5e7a0000[34907,2954604544:04:52:38.030136] NaClMakePcrelThunk: got addr 0x5e7a0000[34903,2953392128:04:52:38.099973] PluginReverseInterface::StartupInitializationComplete[34903,2953392128:04:52:38.100020] PluginReverseInterface::StartupInitializationComplete: invoking CBUnityModule.[34909:263:20627425695745:ERROR:gpu_command_buffer_stub.cc(225)] Failed to create context.[34903:263:20627425900735:ERROR:command_buffer_proxy.cc(136)] Failed to initialize command buffer service.[SRPC:NACL:34907,1056900288:11:52:38.539000] NaClSrpcRpcWait(channel=0x3efc0bb0): EOF is received instead of response. Probably, the other side (usually, nacl module or browser plugin) crashed. \"\n",
        "print(inference(model, tokenizer, f\"Issue Title: {issue_title}\\nIssue Description: {issue_description} Who can this issue\", 3000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: All default search engine settings were wiped out\n",
            "Issue Description: The DhcpProxyScriptFetcher implementation (net/proxy/dhcp_proxy_script_fetcher_win.cc and net/proxy/dhcp_proxy_script_adapter_fetcher_win.cc) currently uses base::WorkerPool, which we plan to deprecate (see issue 251774).It would also make a lot of sense to use only a limited number of threads to fetch DHCP PAC information.Therefore, this bug tracks switching this implementation to use base::SequencedWorkerPool with a limit of 10 threads, that is owned by the DhcpProxyScriptFetcher and handed to each DhcpProxyScriptAdapterFetcher.  An equivalent to the current base::WorkerPool::PostTaskAndReply should be to PostTaskAndReply on a TaskRunner retrieved using SequencedWorkerPool::GetTaskRunnerWithShutdownBehavior(CONTINUE_ON_SHUTDOWN).This should be relatively straightforward and I have a possible volunteer in mind, so I'm tagging it as a GoodFirstBug.\n",
            "Who can fix this issue? [/INST]  The issue can be fixed by: jamescook@chromium.org.\n"
          ]
        }
      ],
      "source": [
        "print(inference(model, tokenizer, \"Issue Title: All default search engine settings were wiped out\\nIssue Description: The DhcpProxyScriptFetcher implementation (net/proxy/dhcp_proxy_script_fetcher_win.cc and net/proxy/dhcp_proxy_script_adapter_fetcher_win.cc) currently uses base::WorkerPool, which we plan to deprecate (see issue 251774).It would also make a lot of sense to use only a limited number of threads to fetch DHCP PAC information.Therefore, this bug tracks switching this implementation to use base::SequencedWorkerPool with a limit of 10 threads, that is owned by the DhcpProxyScriptFetcher and handed to each DhcpProxyScriptAdapterFetcher.  An equivalent to the current base::WorkerPool::PostTaskAndReply should be to PostTaskAndReply on a TaskRunner retrieved using SequencedWorkerPool::GetTaskRunnerWithShutdownBehavior(CONTINUE_ON_SHUTDOWN).This should be relatively straightforward and I have a possible volunteer in mind, so I'm tagging it as a GoodFirstBug.\\nWho can fix this issue?\", 1500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: tpmc needs to produce a better error message when it fails to open the TPM device\n",
            "Issue Description: Currently the open fails quietly, then tpmc fails at sending a command and the error message is horrible (forgot to call TlclLibInit()?)\n",
            "Who can fix this issue? [/INST]  The issue can be fixed by: james.k.brown@chromium.org.\n"
          ]
        }
      ],
      "source": [
        "print(inference(model, tokenizer, \"Issue Title: tpmc needs to produce a better error message when it fails to open the TPM device\\nIssue Description: Currently the open fails quietly, then tpmc fails at sending a command and the error message is horrible (forgot to call TlclLibInit()?)\\nWho can fix this issue?\", 400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: Scrolling Issue\n",
            "Issue Description: Scroll down works but scroll up did not work.\n",
            "Who can fix this issue? [/INST]  The issue can be fixed by: kyotaro@chromium.org \n"
          ]
        }
      ],
      "source": [
        "print(inference(model, tokenizer, \"Issue Title: Scrolling Issue\\nIssue Description: Scroll down works but scroll up did not work.\\nWho can fix this issue?\", 1500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] Issue Title: Resolution change videos are not playing\n",
            "Issue Description: Could not play video if the resolution changes during playback.\n",
            "Who can fix this issue? [/INST]  The issue can be fixed by: james@chromium.org \n"
          ]
        }
      ],
      "source": [
        "print(inference(model, tokenizer, \"Issue Title: Resolution change videos are not playing\\nIssue Description: Could not play video if the resolution changes during playback.\\nWho can fix this issue?\", 1500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jnAEJ8v-1Ht"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IYntY6hBvB3"
      },
      "source": [
        "## Test Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIjoAH25EChy"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     del tokenizer\n",
        "#     del model\n",
        "#     del base_model\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQJhepYq_HD9"
      },
      "outputs": [],
      "source": [
        "hf_custom_model_path = f\"afifaniks/{new_model}\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nk3yLosa84z"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, hf_custom_model_path)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-27hlAGLVoW",
        "outputId": "010e075a-df76-44ee-a94e-390e951d946b"
      },
      "outputs": [],
      "source": [
        "print(inference(model, tokenizer, \"Why birds don't have wheels?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuIwhgQRMOqv",
        "outputId": "50b0e620-8744-4b26-aeaf-7b0b6aa8a296"
      },
      "outputs": [],
      "source": [
        "print(inference(model, tokenizer, \"Can you write some Delphi code that uses named pipes?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB2P8YIJS5QQ",
        "outputId": "fea9f76a-07aa-4dd5-a99c-4e721b362bd5"
      },
      "outputs": [],
      "source": [
        "print(inference(model, tokenizer, \"Which is a species of fish? Tope or Rope\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lMFwBu5vxy39",
        "UpYryLFKzpIn"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a6fb80cef35403e8e63d3263b90b56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2735dc579e748418d8177984f9d515f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_707c14d9b1f74795a2c806bc9cb1df48",
            "value": 2
          }
        },
        "0fbf0ff9aacf4811be576bf3551a3084": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1833fb4ab4684ab681b1ef3bfd4784ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e7e3e6269474d0b8aab1904950011c9",
              "IPY_MODEL_0a6fb80cef35403e8e63d3263b90b56d",
              "IPY_MODEL_c53be97cf137424ca62a88525c999eb7"
            ],
            "layout": "IPY_MODEL_eb45e61e696d4432a9ff9ad17c72c573"
          }
        },
        "19f2acdc4ebd43a1b21a89fa8c79d90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2128587cf4f4469a0f1f27e5f51628b",
              "IPY_MODEL_8d5480f9851349728618636a3185eb13",
              "IPY_MODEL_899a26acba144449bc7b590417c561f8"
            ],
            "layout": "IPY_MODEL_ca50e1cc35a24f6cbae9ecf8ba070d24"
          }
        },
        "1f093aaa5a444bcfa582c989783453df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707c14d9b1f74795a2c806bc9cb1df48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7396ee85e9184547af60a1051f57b37c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74501a139b2947148f56bbd1eae0478d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e7e3e6269474d0b8aab1904950011c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fbf0ff9aacf4811be576bf3551a3084",
            "placeholder": "​",
            "style": "IPY_MODEL_fa65424238124b3895a379f7c455c255",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81d8ea37dec8477bb10fc32d23dbbc9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899a26acba144449bc7b590417c561f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f093aaa5a444bcfa582c989783453df",
            "placeholder": "​",
            "style": "IPY_MODEL_74501a139b2947148f56bbd1eae0478d",
            "value": " 2/2 [01:11&lt;00:00, 32.36s/it]"
          }
        },
        "8d5480f9851349728618636a3185eb13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b154512f829e42ef9fbad60e3d6ce25b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d211d4e7e67c497a9776178ff4c4ff81",
            "value": 2
          }
        },
        "8f71bdb314994245a82e563aff25f760": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b154512f829e42ef9fbad60e3d6ce25b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2128587cf4f4469a0f1f27e5f51628b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d8ea37dec8477bb10fc32d23dbbc9b",
            "placeholder": "​",
            "style": "IPY_MODEL_8f71bdb314994245a82e563aff25f760",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c53be97cf137424ca62a88525c999eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7396ee85e9184547af60a1051f57b37c",
            "placeholder": "​",
            "style": "IPY_MODEL_f471423ca79b4834a259ac8764f984cc",
            "value": " 2/2 [01:12&lt;00:00, 32.91s/it]"
          }
        },
        "ca50e1cc35a24f6cbae9ecf8ba070d24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d211d4e7e67c497a9776178ff4c4ff81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb45e61e696d4432a9ff9ad17c72c573": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2735dc579e748418d8177984f9d515f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f471423ca79b4834a259ac8764f984cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa65424238124b3895a379f7c455c255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
