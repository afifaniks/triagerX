{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from triagerx.dataset.processor import DatasetProcessor\n",
    "from triagerx.model.lbt_p import LBTPClassifier\n",
    "from triagerx.model.roberta_cnn import RobertaCNNClassifier\n",
    "from triagerx.model.roberta_fcn import RobertaFCNClassifier\n",
    "from triagerx.trainer.model_trainer import ModelTrainer\n",
    "from triagerx.trainer.train_config import TrainConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/deeptriage/gc_20.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0b48fc65ad4ea7ad9db8dcbc26afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1973556/3347204740.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues: 109979\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/deeptriage/gc_20.json\"\n",
    "\n",
    "df = pd.read_json(dataset_path)\n",
    "df = df[df[\"owner\"].notna()]\n",
    "\n",
    "def clean_data(df):\n",
    "    df['text'] = df['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n",
    "    df[\"text\"] = df['text'].str.replace(\" +\", \" \", regex=True)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"text\"] = df.apply(\n",
    "            lambda x: \"Title: \"\n",
    "            + str(x[\"issue_title\"])\n",
    "            + \"\\nDescription: \"\n",
    "            + str(x[\"description\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "    \n",
    "    min_length = 15\n",
    "    df = df[df[\"text\"].str.len().gt(min_length)]\n",
    "\n",
    "    # df[\"owner_id\"] = pd.factorize(df[\"assignees\"])[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = prepare_dataframe(df)\n",
    "df = clean_data(df)\n",
    "\n",
    "num_issues = len(df)\n",
    "\n",
    "print(f\"Total number of issues: {num_issues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per block: 10998\n",
      "Training data: 7030, Validation data: 6095\n"
     ]
    }
   ],
   "source": [
    "num_cv = 10\n",
    "sample_threshold=20\n",
    "samples_per_block = len(df) // num_cv + 1\n",
    "print(f\"Samples per block: {samples_per_block}\")\n",
    "\n",
    "block = 1\n",
    "X_df = df[:samples_per_block*block]\n",
    "y_df = df[samples_per_block*block : samples_per_block * (block+1)]\n",
    "\n",
    "\n",
    "developers = X_df[\"owner\"].value_counts()\n",
    "filtered_developers = developers.index[developers >= sample_threshold]\n",
    "X_df = X_df[X_df[\"owner\"].isin(filtered_developers)]\n",
    "\n",
    "train_owners = set(X_df[\"owner\"])\n",
    "test_owners = set(y_df[\"owner\"])\n",
    "\n",
    "unwanted = list(test_owners - train_owners)\n",
    "\n",
    "y_df = y_df[~y_df[\"owner\"].isin(unwanted)]\n",
    "\n",
    "print(f\"Training data: {len(X_df)}, Validation data: {len(y_df)}\")\n",
    "\n",
    "lbl2idx = {}\n",
    "\n",
    "for idx, dev in enumerate(train_owners):\n",
    "    lbl2idx[dev] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df[\"owner_id\"] = X_df[\"owner\"].apply(lambda owner: lbl2idx[owner])\n",
    "y_df[\"owner_id\"] = y_df[\"owner\"].apply(lambda owner: lbl2idx[owner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = similarity_model.encode(X_df.issue_title.to_list(), batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>owner_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11001</th>\n",
       "      <td>abarth@chromium.org</td>\n",
       "      <td>Mixed content warning can be removed</td>\n",
       "      <td>\\nWhen vising an HTTPS URL, if the page includ...</td>\n",
       "      <td>Title: Mixed content warning can be removed\\nD...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11002</th>\n",
       "      <td>sleffler@chromium.org</td>\n",
       "      <td>no openvpn debugging from onc</td>\n",
       "      <td>\\nThe current onc setups for openvpn don't sup...</td>\n",
       "      <td>Title: no openvpn debugging from onc\\nDescript...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       owner                           issue_title  \\\n",
       "11001    abarth@chromium.org  Mixed content warning can be removed   \n",
       "11002  sleffler@chromium.org         no openvpn debugging from onc   \n",
       "\n",
       "                                             description  \\\n",
       "11001  \\nWhen vising an HTTPS URL, if the page includ...   \n",
       "11002  \\nThe current onc setups for openvpn don't sup...   \n",
       "\n",
       "                                                    text  owner_id  \n",
       "11001  Title: Mixed content warning can be removed\\nD...        10  \n",
       "11002  Title: no openvpn debugging from onc\\nDescript...         6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = y_df.iloc[1:3]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mixed content warning can be removed', 'no openvpn debugging from onc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.issue_title.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                      sleffler@chromium.org\n",
       "issue_title                        no openvpn debugging from onc\n",
       "description    \\nThe current onc setups for openvpn don't sup...\n",
       "text           Title: no openvpn debugging from onc\\nDescript...\n",
       "owner_id                                                       6\n",
       "Name: 11002, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[0.6413, 0.4781, 0.4575, 0.4440, 0.4376],\n",
      "        [0.5500, 0.5143, 0.5022, 0.4979, 0.4928]]),\n",
      "indices=tensor([[2439, 5086, 4028, 4544, 5709],\n",
      "        [4936, 1770, 6491,  198, 6055]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1973556/2494374222.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  topk = torch.topk(torch.tensor(cos), k=5)\n"
     ]
    }
   ],
   "source": [
    "test_embed = similarity_model.encode(row.issue_title.to_list())\n",
    "cos = util.cos_sim(test_embed, all_embeddings)\n",
    "topk = torch.topk(torch.tensor(cos), k=5)\n",
    "print(topk)\n",
    "\n",
    "# similar = X_df.iloc[topk.indices.numpy()][[\"issue_title\", \"owner\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                      sleffler@chromium.org\n",
       "issue_title                        no openvpn debugging from onc\n",
       "description    \\nThe current onc setups for openvpn don't sup...\n",
       "text           Title: no openvpn debugging from onc\\nDescript...\n",
       "owner_id                                                       6\n",
       "Name: 11002, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.iloc[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar_devs(issues, k=5):\n",
    "    test_embed = similarity_model.encode(issues)\n",
    "    cos = util.cos_sim(test_embed, all_embeddings)\n",
    "    topk = torch.topk(torch.tensor(cos), k=k)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for it in topk.indices.numpy():\n",
    "        similarities.append(X_df.iloc[it][\"owner_id\"].to_list())\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1973556/2188373634.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  topk = torch.topk(torch.tensor(cos), k=k)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[10, 96, 47, 25, 94], [118, 136, 34, 136, 6]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_similar_devs(row.issue_title.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineLoss(nn.Module):\n",
    "    def __init__(self, class_weights) -> None:\n",
    "        super().__init__()\n",
    "        self._ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        prediction,\n",
    "        labels\n",
    "    ) -> torch.Tensor:\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(len(prediction)):\n",
    "            loss += self._ce(prediction[i], labels)\n",
    "            # print(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LBTPClassifier(\n",
    "    output_size=len(X_df.owner_id.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(X_df[\"owner_id\"])\n",
    "num_samples = sum(class_counts)\n",
    "labels = X_df[\"owner_id\"].to_list() #corresponding labels of samples\n",
    "\n",
    "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=X_df[\"owner_id\"].unique(), y=X_df[\"owner_id\"].to_numpy())\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 50\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_name = sampler.__class__.__name__ if sampler else \"None\"\n",
    "model_name = model.__class__.__name__\n",
    "\n",
    "output_file = f\"dt_lbtp_cv{block}_weighted_ce_{model_name}_20_{sampler_name}\"\n",
    "output_path = f\"/home/mdafifal.mamun/notebooks/triagerX/output/{output_file}.pt\"\n",
    "\n",
    "wandb_config = {\n",
    "        \"project\": \"triagerx_dt_cv\",\n",
    "        \"name\": f\"run_{output_file}\",\n",
    "        \"config\": {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Roberta-FCN\",\n",
    "        \"dataset\": \"deeptriage\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    "}\n",
    "\n",
    "criterion = CombineLoss(class_weights_tensor)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=10, factor=0.1, threshold=1e-8)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_dataset=X_df,\n",
    "    validation_dataset=y_df,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    output_file=output_path,\n",
    "    sampler=sampler,\n",
    "    scheduler=scheduler,\n",
    "    wandb=wandb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class TriageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        feature: str = \"text\",\n",
    "        target: str = \"owner_id\",\n",
    "    ):\n",
    "        logger.debug(\"Generating torch dataset...\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = [label for label in df[target]]\n",
    "        logger.debug(\"Tokenizing texts...\")\n",
    "        self.texts = [\n",
    "            [row.issue_title, self.tokenizer(\n",
    "                row[feature],\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )]\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.rand(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1973556/2188373634.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  topk = torch.topk(torch.tensor(cos), k=k)\n"
     ]
    }
   ],
   "source": [
    "sims = get_top_k_similar_devs(X_df.iloc[3:5].issue_title.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66, 66, 123, 124, 66], [106, 106, 106, 106, 115]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 8, 6, 3],\n",
       "        [7, 8, 9, 5, 1]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   4,   8,   6,   3,  66,  66, 123, 124,  66],\n",
       "        [  7,   8,   9,   5,   1, 106, 106, 106, 106, 115]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = tt.topk(5, 1, True, True)\n",
    "\n",
    "\n",
    "out = []\n",
    "\n",
    "torch.concat((indices, torch.tensor(sims)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from sklearn.metrics import precision_recall_fscore_support, top_k_accuracy_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "# from triagerx.dataset.triage_dataset import TriageDataset\n",
    "from triagerx.trainer.train_config import TrainConfig\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        self._config = config\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        wandb.init(**self._config.wandb)\n",
    "\n",
    "    def train(self, model: nn.Module):\n",
    "        tokenizer = model.tokenizer()\n",
    "        criterion = self._config.criterion\n",
    "        optimizer = self._config.optimizer\n",
    "        train_data = self._config.train_dataset\n",
    "        validation_data = self._config.validation_dataset\n",
    "        sampler = self._config.sampler\n",
    "\n",
    "        train = TriageDataset(train_data, tokenizer)\n",
    "        val = TriageDataset(validation_data, tokenizer)\n",
    "\n",
    "        if self._config.wandb:\n",
    "            logger.debug(\"Initializing wandb...\")\n",
    "            self._init_wandb()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=train,\n",
    "            batch_size=self._config.batch_size,\n",
    "            shuffle=False if sampler else True,\n",
    "            sampler=sampler,\n",
    "        )\n",
    "        val_dataloader = DataLoader(val, batch_size=self._config.batch_size)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        best_loss = float(\"inf\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logger.debug(f\"Selected compute device: {device}\")\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        for epoch_num in range(self._config.epochs):\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input[1][\"attention_mask\"].to(device)\n",
    "                input_id = train_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                output = torch.sum(torch.stack(output), 0)\n",
    "\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            correct_top_k = 0\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input[1][\"attention_mask\"].to(device)\n",
    "                    input_id = val_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                    base_texts = val_input[0]\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "\n",
    "                    output = torch.sum(torch.stack(output), 0)\n",
    "                    _, top_k_predictions = output.topk(5, 1, True, True)\n",
    "                    sim_scores = get_top_k_similar_devs(base_texts)\n",
    "\n",
    "                    top_k_predictions = torch.concat((top_k_predictions, torch.tensor(sim_scores)), dim=1)\n",
    "\n",
    "                    top_k_predictions = top_k_predictions.t()\n",
    "\n",
    "                    correct_top_k += (\n",
    "                        top_k_predictions.eq(\n",
    "                            val_label.view(1, -1).expand_as(top_k_predictions)\n",
    "                        )\n",
    "                        .sum()\n",
    "                        .item()\n",
    "                    )\n",
    "\n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "\n",
    "                    all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
    "                    all_labels.append(val_label.cpu().numpy())\n",
    "\n",
    "                    total_acc_val += acc\n",
    "\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "                all_labels, all_preds, average=\"macro\"\n",
    "            )\n",
    "\n",
    "            top10 = correct_top_k / len(validation_data)\n",
    "\n",
    "            self._log_step(\n",
    "                epoch_num,\n",
    "                total_acc_train,\n",
    "                total_acc_val,\n",
    "                total_loss_train,\n",
    "                total_loss_val,\n",
    "                precision,\n",
    "                recall,\n",
    "                f1_score,\n",
    "                train_data,\n",
    "                validation_data,\n",
    "                top10,\n",
    "            )\n",
    "\n",
    "            val_loss = total_loss_val / len(validation_data)\n",
    "\n",
    "            if self._config.scheduler:\n",
    "                self._config.scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                logger.success(\"Found new best model. Saving weights...\")\n",
    "                torch.save(model.state_dict(), self._config.output_file)\n",
    "                best_loss = val_loss\n",
    "\n",
    "        if self._config.wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "    def _log_step(\n",
    "        self,\n",
    "        epoch_num,\n",
    "        total_acc_train,\n",
    "        total_acc_val,\n",
    "        total_loss_train,\n",
    "        total_loss_val,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        topk,\n",
    "    ):\n",
    "        log = f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                    | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                    | Val Loss: {total_loss_val / len(validation_data): .3f} \\\n",
    "                    | Val Accuracy: {total_acc_val / len(validation_data): .3f} \\\n",
    "                    | Top 10: {topk} \\\n",
    "                    | Precision: {precision: .3f} \\\n",
    "                    | Recall: {recall: .3f} \\\n",
    "                    | F1-score: {f1_score: .3f}\"\n",
    "\n",
    "        logger.info(log)\n",
    "\n",
    "        if self._config.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_acc\": total_acc_train / len(train_data),\n",
    "                    \"train_loss\": total_loss_train / len(train_data),\n",
    "                    \"val_acc\": total_acc_val / len(validation_data),\n",
    "                    \"val_loss\": total_loss_val / len(validation_data),\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1-score\": f1_score,\n",
    "                    \"top10\": topk,\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-16 13:01:08.856\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.dataset.triage_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-02-16 13:01:08.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.dataset.triage_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n",
      "\u001b[32m2024-02-16 13:01:19.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.dataset.triage_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-02-16 13:01:19.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.dataset.triage_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n",
      "\u001b[32m2024-02-16 13:01:28.379\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1mInitializing wandb...\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafifaniks\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      " 68%|██████▊   | 321/469 [02:56<01:21,  1.82it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mdafifal.mamun/notebooks/triagerX/wandb/run-20240216_130132-8lvza4nx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afifaniks/triagerx_dt_cv/runs/8lvza4nx' target=\"_blank\">run_dt_lbtp_cv1_weighted_ce_LBTPClassifier_20_WeightedRandomSampler</a></strong> to <a href='https://wandb.ai/afifaniks/triagerx_dt_cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afifaniks/triagerx_dt_cv' target=\"_blank\">https://wandb.ai/afifaniks/triagerx_dt_cv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afifaniks/triagerx_dt_cv/runs/8lvza4nx' target=\"_blank\">https://wandb.ai/afifaniks/triagerx_dt_cv/runs/8lvza4nx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-16 13:01:41.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mSelected compute device: cuda\u001b[0m\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 469/469 [04:18<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:08:36.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 1 | Train Loss:  1.333                     | Train Accuracy:  0.031                     | Val Loss:  1.306                     | Val Accuracy:  0.041                     | Top 10: 0.19524200164068908                     | Precision:  0.032                     | Recall:  0.035                     | F1-score:  0.014\u001b[0m\n",
      "\u001b[32m2024-02-16 13:08:36.287\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:15:32.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 2 | Train Loss:  1.149                     | Train Accuracy:  0.171                     | Val Loss:  1.170                     | Val Accuracy:  0.103                     | Top 10: 0.3601312551271534                     | Precision:  0.117                     | Recall:  0.108                     | F1-score:  0.069\u001b[0m\n",
      "\u001b[32m2024-02-16 13:15:32.899\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:22:29.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 3 | Train Loss:  0.956                     | Train Accuracy:  0.292                     | Val Loss:  1.068                     | Val Accuracy:  0.143                     | Top 10: 0.4574241181296144                     | Precision:  0.136                     | Recall:  0.158                     | F1-score:  0.099\u001b[0m\n",
      "\u001b[32m2024-02-16 13:22:29.418\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:29:26.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 4 | Train Loss:  0.796                     | Train Accuracy:  0.407                     | Val Loss:  0.992                     | Val Accuracy:  0.185                     | Top 10: 0.5136997538966366                     | Precision:  0.186                     | Recall:  0.199                     | F1-score:  0.151\u001b[0m\n",
      "\u001b[32m2024-02-16 13:29:26.045\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:36:22.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 5 | Train Loss:  0.653                     | Train Accuracy:  0.511                     | Val Loss:  0.942                     | Val Accuracy:  0.195                     | Top 10: 0.5596390484003282                     | Precision:  0.204                     | Recall:  0.212                     | F1-score:  0.163\u001b[0m\n",
      "\u001b[32m2024-02-16 13:36:22.549\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n",
      "/home/mdafifal.mamun/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-02-16 13:43:19.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36m_log_step\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEpochs: 6 | Train Loss:  0.543                     | Train Accuracy:  0.596                     | Val Loss:  0.910                     | Val Accuracy:  0.211                     | Top 10: 0.5845775225594749                     | Precision:  0.223                     | Recall:  0.231                     | F1-score:  0.185\u001b[0m\n",
      "\u001b[32m2024-02-16 13:43:19.693\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mtriagerx.trainer.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "100%|██████████| 469/469 [04:17<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(train_config)\n",
    "trainer.train(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
