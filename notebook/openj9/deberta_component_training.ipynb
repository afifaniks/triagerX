{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from loguru import logger\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, PreTrainedTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_split(x):\n",
    "    x_split = str(x).split(\",\")\n",
    "\n",
    "    for s in x_split:\n",
    "        if \"comp:\" in s.lower():\n",
    "            return s.strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/openj9_processed.csv\"\n",
    "\n",
    "raw_df = pd.read_csv(dataset_path)\n",
    "print(len(raw_df))\n",
    "raw_df = raw_df.rename(columns={\"assignees\": \"owner\", \"issue_body\": \"description\"})\n",
    "# df = df[df[\"owner\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All issues: 7348\n",
      "Excluding pull: 7348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7348/7348 [00:00<00:00, 92651.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues: 7348\n"
     ]
    }
   ],
   "source": [
    "def clean_issue_description(text):\n",
    "    # Remove leading and trailing white spaces\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Remove stack traces\n",
    "    # cleaned_text = re.sub(r'Traceback.*?Error.*?$', '', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove URLs\n",
    "    # cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
    "    \n",
    "    # Remove special characters and punctuation marks\n",
    "    # cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Remove hexadecimal codes\n",
    "    cleaned_text = re.sub(r'0x[\\da-fA-F]+', '<HEX>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b[0-9a-fA-F]{16}\\b', '<HEX>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\b', '<TIMESTAMP>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s*[-+]?\\d*\\.\\d+([eE][-+]?\\d+)?', '<FLOAT_VALUE>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'=\\s*-?\\d+', '= <PARAM_VALUE>', cleaned_text)\n",
    "    \n",
    "    # Remove code snippets\n",
    "    # cleaned_text = re.sub(r'```.*?```', '', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove directory paths\n",
    "    # cleaned_text = re.sub(r'\\/\\S+\\/', '', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_data(df):\n",
    "    df['text'] = df['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)\n",
    "    df[\"text\"] = df['text'].str.replace(\" +\", \" \", regex=True)\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_issue_description)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"labels\"].notna()]\n",
    "    print(f\"All issues: {len(df)}\")\n",
    "    print(f\"Excluding pull: {len(df)}\")\n",
    "    df = df[~df[\"issue_url\"].str.contains(\"/pull/\")]\n",
    "    \n",
    "    df[\"component\"] = df[\"labels\"].apply(component_split)\n",
    "    \n",
    "    df[\"text\"] = df.progress_apply(\n",
    "            lambda x: \"Title: \"\n",
    "            + str(x[\"issue_title\"])\n",
    "            # + \"\\nIssue Labels: \"\n",
    "            # + str(x[\"labels\"])\n",
    "            # + \"\\nIssue Topic: \"\n",
    "            # + str(x[\"topic_label\"])\n",
    "            + \"\\nDescription: \"\n",
    "            + str(x[\"description\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "    \n",
    "    min_length = 15\n",
    "    df = df[df[\"text\"].str.len().gt(min_length)]\n",
    "\n",
    "    # df[\"owner_id\"] = pd.factorize(df[\"assignees\"])[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = prepare_dataframe(raw_df)\n",
    "df = clean_data(df)\n",
    "df = df.sort_values(by=\"issue_number\")\n",
    "\n",
    "num_issues = len(df)\n",
    "\n",
    "print(f\"Total number of issues: {num_issues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ci.eclipse.org/openj9/job/Test-sanity.system-JDK10-linux_x86-64/100\n",
      "```\n",
      "===============================================\n",
      "Running test DaaLoadTest_daa1_0 ...\n",
      "===============================================\n",
      "DaaLoadTest_daa1_0 Start Time: Thu Oct 18 07:10:01 2018 Epoch Time (ms): 1539846601869\n",
      "test with NoOptions\n",
      "STF 07:10:02.042 - =========================   S T F   =========================\n",
      "systemtest-prereqs has been processed, and set to: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqsRetrieving amount of free space on drive containing /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0\n",
      "There is 90314 Mb free\n",
      "STF 07:10:02.048 - ==================   G E N E R A T I O N   ==================\n",
      "STF 07:10:02.050 - Checking JVM: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../\n",
      "STF 07:10:02.050 - Starting process to generate scripts: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/..//bin/java  -Dlog4j.skipJansi=true -Djava.system.class.loader=net.adoptopenjdk.stf.runner.StfClassLoader -classpath /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../../jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-api-2.3.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../../jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-core-2.3.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/scripts/../bin net.adoptopenjdk.stf.runner.StfRunner -properties \"/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/stf_parameters.properties, , /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/config/stf.properties\" -testDir \"/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest\"\n",
      "GEN 07:10:03.288 - Found test. Project: 'openj9.test.load' class: 'DaaLoadTest.class' Dir: '/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.load/bin'\n",
      "GEN 07:10:03.295 - Found test. Project: 'openj9.test.load' class: 'net.openj9.stf.DaaLoadTest'\n",
      "GEN Classpath directories used by project 'openj9.test.load': \n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.load/bin\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/bin\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.load/bin\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-api-2.3.jar\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-core-2.3.jar\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit-4.12/junit-4.12.jar\n",
      "GEN   /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit-4.12/hamcrest-core-1.3.jar\n",
      "GEN 07:10:03.667 - Using Mode NoOptions. Values = ''\n",
      "GEN 07:10:03.767 - \n",
      "GEN 07:10:03.767 - Test command summary:\n",
      "GEN 07:10:03.769 -   Step  Stage   Command           Description\n",
      "GEN 07:10:03.769 -  -----+--------+-----------------+------------\n",
      "GEN 07:10:03.770 -     1  execute  Run java          Run daa load test\n",
      "STF 07:10:03.796 - \n",
      "STF 07:10:03.796 - Script generation completed\n",
      "STF 07:10:03.796 - \n",
      "STF 07:10:03.796 - \n",
      "STF 07:10:03.796 - =======================   S E T U P   =======================\n",
      "STF 07:10:03.796 - Running setup: perl /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/setUp.pl\n",
      "STF 07:10:03.866 - SETUP stage completed\n",
      "STF 07:10:03.871 - \n",
      "STF 07:10:03.871 - ====================   E X E C U T E -   ====================\n",
      "STF 07:10:03.871 - Running execute: perl /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/execute.pl\n",
      "STF 07:10:03.938 - \n",
      "STF 07:10:03.938 - Java version\n",
      "STF 07:10:03.938 - Running: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java -version\n",
      "openjdk version \"10.0.2-internal\" 2018-07-17\n",
      "OpenJDK Runtime Environment (build 10.0.2-internal+0-adhoc.jenkins.Build-JDK10-linuxx86-64)\n",
      "Eclipse OpenJ9 VM (build master-e448d21, JRE 10 Linux amd64-64-Bit 20181018_459 (JIT enabled, AOT enabled)\n",
      "OpenJ9   - e448d21\n",
      "OMR      - 2bc0d2a\n",
      "JCL      - 72f6b68 based on jdk-10.0.2+13)\n",
      "STF 07:10:04.149 - \n",
      "STF 07:10:04.149 - +------ Step 1 - Run daa load test\n",
      "STF 07:10:04.149 - | Run foreground process\n",
      "STF 07:10:04.149 - |   Program:     /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java\n",
      "STF 07:10:04.149 - |   Mnemonic:    DLT\n",
      "STF 07:10:04.149 - |   Echo:        ECHO_ON\n",
      "STF 07:10:04.149 - |   Expectation: CLEAN_RUN within 5h\n",
      "STF 07:10:04.149 - |\n",
      "STF 07:10:04.150 - Running command: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java -Xnocompressedrefs -classpath /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.load/bin:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/bin:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-api-2.3.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j-2.3/log4j-core-2.3.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit-4.12/junit-4.12.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit-4.12/hamcrest-core-1.3.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.daa/bin net.adoptopenjdk.loadTest.LoadTest -resultsDir /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results -resultsPrefix 1.DLT. -reportFailureLimit 1 -abortAtFailureLimit 10 -maxTotalLogFileSpace 200M -maxSingleLogSize 1/25 -suite.daa.threadCount 2 -suite.daa.totalNumberTests 1900 -suite.daa.inventoryFile /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory/openj9.test.load/config/inventories/daa/daa1.xml -suite.daa.inventoryExcludeFile none -suite.daa.selection random -suite.daa.seed -1 -suite.daa.repeatCount 1 -suite.daa.thinkingTime 0ms..0ms\n",
      "STF 07:10:04.150 - Redirecting stderr to /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.stderr\n",
      "STF 07:10:04.150 - Redirecting stdout to /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.stdout\n",
      "STF 07:10:04.160 - Monitoring processes: DLT\n",
      "DLT 07:10:05.392 - Load test parameters\n",
      "DLT 07:10:05.395 -   Time limited         = false\n",
      "DLT 07:10:05.396 -   abortIfOutOfMemory   = true\n",
      "DLT 07:10:05.396 -   reportFailureLimit   = 1\n",
      "DLT 07:10:05.397 -   abortAtFailureLimit  = 10\n",
      "DLT 07:10:05.399 -   maxTotalLogFileSpace = 209715200\n",
      "DLT 07:10:05.400 -   maxSingleLogSize     = 8388608\n",
      "DLT 07:10:05.400 - Parameters for suite 0\n",
      "DLT 07:10:05.401 -   Suite name     = daa\n",
      "DLT 07:10:05.402 -   Number threads = 2\n",
      "DLT 07:10:05.403 -   Supplied seed  = -1\n",
      "DLT 07:10:05.404 -   Inventory file = /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory/openj9.test.load/config/inventories/daa/daa1.xml\n",
      "DLT 07:10:05.404 -   Exclude file   = none\n",
      "DLT 07:10:05.405 -   Number tests   = 1900\n",
      "DLT 07:10:05.406 -   Repeat count   = 1\n",
      "DLT 07:10:05.407 -   Thinking time  = 0ms..0ms\n",
      "DLT 07:10:05.407 -   Selection mode = random\n",
      "DLT 07:10:05.409 -   Actual seed    = 1541935198988\n",
      "DLT 07:10:05.423 - Parsing inventory file. Root=/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory File=openj9.test.load/config/inventories/daa/daa1.xml\n",
      "DLT 07:10:05.438 - Final test list:\n",
      "DLT 07:10:05.442 -   0 ArbitraryJava[net.openj9.test.simple.ConvertDecimal invokeTest]  Weighting=1 \n",
      "DLT 07:10:05.442 -   1 ArbitraryJava[net.openj9.test.simple.MarshalUnmarshalBinary invokeTest]  Weighting=1 \n",
      "DLT 07:10:05.443 -   2 JUnit[net.openj9.test.arithmetics.TestArithmeticOperations]  Weighting=1 \n",
      "DLT 07:10:05.443 -   3 JUnit[net.openj9.test.arithmetics.TestArithmeticInline]  Weighting=1 \n",
      "DLT 07:10:05.443 -   4 JUnit[net.openj9.test.arithmetics.TestArithmetics]  Weighting=1 \n",
      "DLT 07:10:05.443 -   5 JUnit[net.openj9.test.arithmetics.TestComparisonEquals]  Weighting=1 \n",
      "DLT 07:10:05.444 -   6 JUnit[net.openj9.test.arithmetics.TestPDComparisons]  Weighting=1 \n",
      "DLT 07:10:05.444 -   7 JUnit[net.openj9.test.arithmetics.TestPerformance]  Weighting=1 \n",
      "DLT 07:10:05.444 -   8 JUnit[net.openj9.test.arithmetics.TestSubExceptions]  Weighting=1 \n",
      "DLT 07:10:05.444 -   9 JUnit[net.openj9.test.arithmetics.TestValidityChecking]  Weighting=1 \n",
      "DLT 07:10:05.444 -   10 JUnit[net.openj9.test.binaryData.LongIntegerComparison]  Weighting=1 \n",
      "DLT 07:10:05.444 -   11 JUnit[net.openj9.test.binaryData.TestOptimizer]  Weighting=1 \n",
      "DLT 07:10:05.445 -   12 JUnit[net.openj9.test.decimals.TestBD2PD2BD]  Weighting=1 \n",
      "DLT 07:10:05.445 -   13 JUnit[net.openj9.test.decimals.TestDecimalData]  Weighting=1 \n",
      "DLT 07:10:05.445 -   14 JUnit[net.openj9.test.decimals.TestDecimalData2]  Weighting=1 \n",
      "DLT 07:10:05.445 -   15 JUnit[net.openj9.test.PD2Primitive.TestPD2Primitives2PD]  Weighting=1 \n",
      "DLT 07:10:05.445 -   16 JUnit[net.openj9.test.PDMoveShifts.TestShiftsAndConvert]  Weighting=1 \n",
      "DLT 07:10:05.445 -   17 JUnit[net.openj9.test.PDMoveShifts.TestPDMove]  Weighting=1 \n",
      "DLT 07:10:05.445 -   18 ArbitraryJava[net.openj9.test.PDMoveShifts.ShiftTestRunner invokeTest]  Weighting=1 \n",
      "DLT 07:10:05.491 - Starting thread. Suite=0 thread=0\n",
      "DLT 07:10:05.493 - Starting thread. Suite=0 thread=1\n",
      "DLT stderr #0: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x6be706) [0x7fcb2dc02706]\n",
      "DLT stderr #1: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x6cbd1d) [0x7fcb2dc0fd1d]\n",
      "DLT stderr #2: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x1247e9) [0x7fcb2d6687e9]\n",
      "DLT stderr #3: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+0x1f42d) [0x7fcb2f0f542d]\n",
      "DLT stderr #4: /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390) [0x7fcb34eac390]\n",
      "DLT stderr #5: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x5afdbc) [0x7fcb2daf3dbc]\n",
      "DLT stderr #6: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x580fad) [0x7fcb2dac4fad]\n",
      "DLT stderr #7: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x58104b) [0x7fcb2dac504b]\n",
      "DLT stderr #8: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x5810f6) [0x7fcb2dac50f6]\n",
      "DLT stderr #9: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x5811de) [0x7fcb2dac51de]\n",
      "DLT stderr #10: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x58163b) [0x7fcb2dac563b]\n",
      "DLT stderr #11: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x57ce25) [0x7fcb2dac0e25]\n",
      "DLT stderr #12: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x57df32) [0x7fcb2dac1f32]\n",
      "DLT stderr #13: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x3d85bd) [0x7fcb2d91c5bd]\n",
      "DLT stderr #14: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x131c5a) [0x7fcb2d675c5a]\n",
      "DLT stderr #15: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x132c16) [0x7fcb2d676c16]\n",
      "DLT stderr #16: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+0x204f7) [0x7fcb2f0f64f7]\n",
      "DLT stderr #17: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x1344b1) [0x7fcb2d6784b1]\n",
      "DLT stderr #18: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x134951) [0x7fcb2d678951]\n",
      "DLT stderr #19: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x135000) [0x7fcb2d679000]\n",
      "DLT stderr #20: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x1352da) [0x7fcb2d6792da]\n",
      "DLT stderr #21: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x13538f) [0x7fcb2d67938f]\n",
      "DLT stderr #22: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+0x204f7) [0x7fcb2f0f64f7]\n",
      "DLT stderr #23: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+0x1356d6) [0x7fcb2d6796d6]\n",
      "DLT stderr #24: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9thr29.so(+0xdf63) [0x7fcb2fa5ef63]\n",
      "DLT stderr #25: /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7fcb34ea26ba]\n",
      "DLT stderr #26: function clone+0x6d [0x7fcb355dd41d]\n",
      "DLT stderr Unhandled exception\n",
      "DLT stderr Type=Segmentation error vmState=0x000507ff\n",
      "DLT stderr J9Generic_Signal_Number=00000004 Signal_Number=0000000b Error_Value=00000000 Signal_Code=00000001\n",
      "DLT stderr Handler1=00007FCB2FD05D40 Handler2=00007FCB2F0F51F0 InaccessibleAddress=0000000000000000\n",
      "DLT stderr RDI=00007FCA340FD390 RSI=00007FCA34198E50 RAX=0000000000000000 RBX=00007FCA340FD390\n",
      "DLT stderr RCX=0000000000000254 RDX=00007FCA35D80020 R8=0000000000000000 R9=0000000000000000\n",
      "DLT stderr R10=0000000000000000 R11=00007FCA34206C20 R12=00007FCA34198E50 R13=000000000000324A\n",
      "DLT stderr R14=00007FCA34198E50 R15=0000000000000000\n",
      "DLT stderr RIP=00007FCB2DAF3DBC GS=0000 FS=0000 RSP=00007FCA9480DE40\n",
      "DLT stderr EFlags=0000000000010206 CS=0033 RBP=00007FCA35D80020 ERR=0000000000000004\n",
      "DLT stderr TRAPNO=000000000000000E OLDMASK=0000000000000000 CR2=0000000000000000\n",
      "DLT stderr xmm0 0000000000000000 (f: 0.000000, d: 0.000000e+00)\n",
      "DLT stderr xmm1 0000000000000000 (f: 0.000000, d: 0.000000e+00)\n",
      "DLT stderr xmm2 00007fca32268c90 (f: 841387136.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm3 00007fca3226f690 (f: 841414272.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm4 00007fca35d764b0 (f: 903308480.000000, d: 6.941942e-310)\n",
      "DLT stderr xmm5 00007fca3226f690 (f: 841414272.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm6 00007fca8cd941f8 (f: 2363048448.000000, d: 6.942014e-310)\n",
      "DLT stderr xmm7 0000000000000000 (f: 0.000000, d: 0.000000e+00)\n",
      "DLT stderr xmm8 00007fca322f6280 (f: 841966208.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm9 00007fca322f2120 (f: 841949440.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm10 00007fca322f7b40 (f: 841972544.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm11 00007fca323848a0 (f: 842549376.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm12 00007fca322ffce0 (f: 842005760.000000, d: 6.941939e-310)\n",
      "DLT stderr xmm13 00007fca35d787c0 (f: 903317440.000000, d: 6.941942e-310)\n",
      "DLT stderr xmm14 00007fca35e3a820 (f: 904112128.000000, d: 6.941942e-310)\n",
      "DLT stderr xmm15 00007fca35e318a0 (f: 904075392.000000, d: 6.941942e-310)\n",
      "DLT stderr Module=/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so\n",
      "DLT stderr Module_base_address=00007FCB2D544000\n",
      "DLT stderr \n",
      "DLT stderr Method_being_compiled=net/openj9/test/decimals/TestDecimalData2.testConvertBigDecimalNormals()V\n",
      "DLT stderr Target=2_90_20181018_459 (Linux 4.4.0-134-generic)\n",
      "DLT stderr CPU=amd64 (4 logical CPUs) (0x1f2ae3000 RAM)\n",
      "DLT stderr ----------- Stack Backtrace -----------\n",
      "DLT stderr (0x00007FCB2DAF3DBC [libj9jit29.so+0x5afdbc])\n",
      "DLT stderr (0x00007FCB2DAC4FAD [libj9jit29.so+0x580fad])\n",
      "DLT stderr (0x00007FCB2DAC504B [libj9jit29.so+0x58104b])\n",
      "DLT stderr (0x00007FCB2DAC50F6 [libj9jit29.so+0x5810f6])\n",
      "DLT stderr (0x00007FCB2DAC51DE [libj9jit29.so+0x5811de])\n",
      "DLT stderr (0x00007FCB2DAC563B [libj9jit29.so+0x58163b])\n",
      "DLT stderr (0x00007FCB2DAC0E25 [libj9jit29.so+0x57ce25])\n",
      "DLT stderr (0x00007FCB2DAC1F32 [libj9jit29.so+0x57df32])\n",
      "DLT stderr (0x00007FCB2D91C5BD [libj9jit29.so+0x3d85bd])\n",
      "DLT stderr (0x00007FCB2D675C5A [libj9jit29.so+0x131c5a])\n",
      "DLT stderr (0x00007FCB2D676C16 [libj9jit29.so+0x132c16])\n",
      "DLT stderr (0x00007FCB2F0F64F7 [libj9prt29.so+0x204f7])\n",
      "DLT stderr (0x00007FCB2D6784B1 [libj9jit29.so+0x1344b1])\n",
      "DLT stderr (0x00007FCB2D678951 [libj9jit29.so+0x134951])\n",
      "DLT stderr (0x00007FCB2D679000 [libj9jit29.so+0x135000])\n",
      "DLT stderr (0x00007FCB2D6792DA [libj9jit29.so+0x1352da])\n",
      "DLT stderr (0x00007FCB2D67938F [libj9jit29.so+0x13538f])\n",
      "DLT stderr (0x00007FCB2F0F64F7 [libj9prt29.so+0x204f7])\n",
      "DLT stderr (0x00007FCB2D6796D6 [libj9jit29.so+0x1356d6])\n",
      "DLT stderr (0x00007FCB2FA5EF63 [libj9thr29.so+0xdf63])\n",
      "DLT stderr (0x00007FCB34EA26BA [libpthread.so.0+0x76ba])\n",
      "DLT stderr clone+0x6d (0x00007FCB355DD41D [libc.so.6+0x10741d])\n",
      "```\n",
      "\n",
      "=========\n",
      "\n",
      "Title: Test-sanity.system-JDK10-linux_x86-64 DaaLoadTest_daa1_0 crash vmState=<HEX>\n",
      "Description: \n",
      "```\n",
      "===============================================\n",
      "Running test DaaLoadTest_daa1_0 ...\n",
      "===============================================\n",
      "DaaLoadTest_daa1_0 Start Time: Thu Oct 18 07:10:01 2018 Epoch Time (ms): 1539846601869\n",
      "test with NoOptions\n",
      "STF <TIMESTAMP> - ========================= S T F =========================\n",
      "systemtest-prereqs has been processed, and set to: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqsRetrieving amount of free space on drive containing /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0\n",
      "There is 90314 Mb free\n",
      "STF <TIMESTAMP> - ================== G E N E R A T I O N ==================\n",
      "STF <TIMESTAMP> - Checking JVM: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../\n",
      "STF <TIMESTAMP> - Starting process to generate scripts: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/..//bin/java -Dlog4j.skipJansi=true -Djava.system.class.loader=net.adoptopenjdk.stf.runner.StfClassLoader -classpath /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../../jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-api<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../../jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-core<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/scripts/../bin net.adoptopenjdk.stf.runner.StfRunner -properties \"/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/stf_parameters.properties, , /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/config/stf.properties\" -testDir \"/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest\"\n",
      "GEN <TIMESTAMP> - Found test. Project: 'openj9.test.load' class: 'DaaLoadTest.class' Dir: '/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.load/bin'\n",
      "GEN <TIMESTAMP> - Found test. Project: 'openj9.test.load' class: 'net.openj9.stf.DaaLoadTest'\n",
      "GEN Classpath directories used by project 'openj9.test.load': \n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.load/bin\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/bin\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.load/bin\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-api<FLOAT_VALUE>.jar\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-core<FLOAT_VALUE>.jar\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit<FLOAT_VALUE>/junit<FLOAT_VALUE>.jar\n",
      "GEN /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit<FLOAT_VALUE>/hamcrest-core<FLOAT_VALUE>.jar\n",
      "GEN <TIMESTAMP> - Using Mode NoOptions. Values = ''\n",
      "GEN <TIMESTAMP> - \n",
      "GEN <TIMESTAMP> - Test command summary:\n",
      "GEN <TIMESTAMP> - Step Stage Command Description\n",
      "GEN <TIMESTAMP> - -----+--------+-----------------+------------\n",
      "GEN <TIMESTAMP> - 1 execute Run java Run daa load test\n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - Script generation completed\n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - ======================= S E T U P =======================\n",
      "STF <TIMESTAMP> - Running setup: perl /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/setUp.pl\n",
      "STF <TIMESTAMP> - SETUP stage completed\n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - ==================== E X E C U T E - ====================\n",
      "STF <TIMESTAMP> - Running execute: perl /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/execute.pl\n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - Java version\n",
      "STF <TIMESTAMP> - Running: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java -version\n",
      "openjdk version \"<FLOAT_VALUE><FLOAT_VALUE>-internal\" 2018-07-17\n",
      "OpenJDK Runtime Environment (build<FLOAT_VALUE><FLOAT_VALUE>-internal+0-adhoc.jenkins.Build-JDK10-linuxx86-64)\n",
      "Eclipse OpenJ9 VM (build master-e448d21, JRE 10 Linux amd64-64-Bit 20181018_459 (JIT enabled, AOT enabled)\n",
      "OpenJ9 - e448d21\n",
      "OMR - 2bc0d2a\n",
      "JCL - 72f6b68 based on jdk<FLOAT_VALUE><FLOAT_VALUE>+13)\n",
      "STF <TIMESTAMP> - \n",
      "STF <TIMESTAMP> - +------ Step 1 - Run daa load test\n",
      "STF <TIMESTAMP> - | Run foreground process\n",
      "STF <TIMESTAMP> - | Program: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java\n",
      "STF <TIMESTAMP> - | Mnemonic: DLT\n",
      "STF <TIMESTAMP> - | Echo: ECHO_ON\n",
      "STF <TIMESTAMP> - | Expectation: CLEAN_RUN within 5h\n",
      "STF <TIMESTAMP> - |\n",
      "STF <TIMESTAMP> - Running command: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/bin/../bin/java -Xnocompressedrefs -classpath /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.load/bin:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/stf/stf.core/bin:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-api<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/log4j<FLOAT_VALUE>/log4j-core<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit<FLOAT_VALUE>/junit<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/systemtest_prereqs/junit<FLOAT_VALUE>/hamcrest-core<FLOAT_VALUE>.jar:/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/jvmtest/systemtest/openj9-systemtest/openj9.test.daa/bin net.adoptopenjdk.loadTest.LoadTest -resultsDir /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results -resultsPrefix 1.DLT. -reportFailureLimit 1 -abortAtFailureLimit 10 -maxTotalLogFileSpace 200M -maxSingleLogSize 1/25 -suite.daa.threadCount 2 -suite.daa.totalNumberTests 1900 -suite.daa.inventoryFile /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory/openj9.test.load/config/inventories/daa/daa1.xml -suite.daa.inventoryExcludeFile none -suite.daa.selection random -suite.daa.seed -1 -suite.daa.repeatCount 1 -suite.daa.thinkingTime 0ms.<FLOAT_VALUE>ms\n",
      "STF <TIMESTAMP> - Redirecting stderr to /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.stderr\n",
      "STF <TIMESTAMP> - Redirecting stdout to /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.stdout\n",
      "STF <TIMESTAMP> - Monitoring processes: DLT\n",
      "DLT <TIMESTAMP> - Load test parameters\n",
      "DLT <TIMESTAMP> - Time limited = false\n",
      "DLT <TIMESTAMP> - abortIfOutOfMemory = true\n",
      "DLT <TIMESTAMP> - reportFailureLimit = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - abortAtFailureLimit = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - maxTotalLogFileSpace = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - maxSingleLogSize = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Parameters for suite 0\n",
      "DLT <TIMESTAMP> - Suite name = daa\n",
      "DLT <TIMESTAMP> - Number threads = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Supplied seed = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Inventory file = /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory/openj9.test.load/config/inventories/daa/daa1.xml\n",
      "DLT <TIMESTAMP> - Exclude file = none\n",
      "DLT <TIMESTAMP> - Number tests = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Repeat count = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Thinking time = <PARAM_VALUE>ms.<FLOAT_VALUE>ms\n",
      "DLT <TIMESTAMP> - Selection mode = random\n",
      "DLT <TIMESTAMP> - Actual seed = <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Parsing inventory file. Root=/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdk-tests/TestConfig/scripts/testKitGen/../../../TestConfig/test_output_15398463188030/DaaLoadTest_daa1_0/20181018-071002-DaaLoadTest/results/1.DLT.inventory File=openj9.test.load/config/inventories/daa/daa1.xml\n",
      "DLT <TIMESTAMP> - Final test list:\n",
      "DLT <TIMESTAMP> - 0 ArbitraryJava[net.openj9.test.simple.ConvertDecimal invokeTest] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 1 ArbitraryJava[net.openj9.test.simple.MarshalUnmarshalBinary invokeTest] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 2 JUnit[net.openj9.test.arithmetics.TestArithmeticOperations] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 3 JUnit[net.openj9.test.arithmetics.TestArithmeticInline] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 4 JUnit[net.openj9.test.arithmetics.TestArithmetics] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 5 JUnit[net.openj9.test.arithmetics.TestComparisonEquals] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 6 JUnit[net.openj9.test.arithmetics.TestPDComparisons] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 7 JUnit[net.openj9.test.arithmetics.TestPerformance] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 8 JUnit[net.openj9.test.arithmetics.TestSubExceptions] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 9 JUnit[net.openj9.test.arithmetics.TestValidityChecking] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 10 JUnit[net.openj9.test.binaryData.LongIntegerComparison] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 11 JUnit[net.openj9.test.binaryData.TestOptimizer] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 12 JUnit[net.openj9.test.decimals.TestBD2PD2BD] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 13 JUnit[net.openj9.test.decimals.TestDecimalData] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 14 JUnit[net.openj9.test.decimals.TestDecimalData2] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 15 JUnit[net.openj9.test.PD2Primitive.TestPD2Primitives2PD] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 16 JUnit[net.openj9.test.PDMoveShifts.TestShiftsAndConvert] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 17 JUnit[net.openj9.test.PDMoveShifts.TestPDMove] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - 18 ArbitraryJava[net.openj9.test.PDMoveShifts.ShiftTestRunner invokeTest] Weighting= <PARAM_VALUE> \n",
      "DLT <TIMESTAMP> - Starting thread. Suite= <PARAM_VALUE> thread= <PARAM_VALUE>\n",
      "DLT <TIMESTAMP> - Starting thread. Suite= <PARAM_VALUE> thread= <PARAM_VALUE>\n",
      "DLT stderr #0: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #1: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #2: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #3: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #4: /lib/x86_64-linux-gnu/libpthread.so<FLOAT_VALUE>(+<HEX>) [<HEX>]\n",
      "DLT stderr #5: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #6: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #7: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #8: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #9: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #10: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #11: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #12: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #13: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #14: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #15: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #16: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #17: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #18: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #19: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #20: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #21: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #22: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9prt29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #23: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #24: /home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9thr29.so(+<HEX>) [<HEX>]\n",
      "DLT stderr #25: /lib/x86_64-linux-gnu/libpthread.so<FLOAT_VALUE>(+<HEX>) [<HEX>]\n",
      "DLT stderr #26: function clone+<HEX> [<HEX>]\n",
      "DLT stderr Unhandled exception\n",
      "DLT stderr Type=Segmentation error vmState=<HEX>\n",
      "DLT stderr J9Generic_Signal_Number= <PARAM_VALUE> Signal_Number= <PARAM_VALUE>b Error_Value= <PARAM_VALUE> Signal_Code= <PARAM_VALUE>\n",
      "DLT stderr Handler1=<HEX> Handler2=<HEX> InaccessibleAddress=<HEX>\n",
      "DLT stderr RDI=<HEX> RSI=<HEX> RAX=<HEX> RBX=<HEX>\n",
      "DLT stderr RCX=<HEX> RDX=<HEX> R8=<HEX> R9=<HEX>\n",
      "DLT stderr R10=<HEX> R11=<HEX> R12=<HEX> R13=<HEX>\n",
      "DLT stderr R14=<HEX> R15=<HEX>\n",
      "DLT stderr RIP=<HEX> GS= <PARAM_VALUE> FS= <PARAM_VALUE> RSP=<HEX>\n",
      "DLT stderr EFlags=<HEX> CS= <PARAM_VALUE> RBP=<HEX> ERR=<HEX>\n",
      "DLT stderr TRAPNO=<HEX> OLDMASK=<HEX> CR2=<HEX>\n",
      "DLT stderr xmm0 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm1 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm2 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm3 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm4 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm5 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm6 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm7 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm8 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm9 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm10 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm11 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm12 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm13 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm14 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr xmm15 <HEX> (f:<FLOAT_VALUE>, d:<FLOAT_VALUE>)\n",
      "DLT stderr Module=/home/jenkins/workspace/Test-sanity.system-JDK10-linux_x86-64/openjdkbinary/j2sdk-image/lib/default/libj9jit29.so\n",
      "DLT stderr Module_base_address=<HEX>\n",
      "DLT stderr \n",
      "DLT stderr Method_being_compiled=net/openj9/test/decimals/TestDecimalData2.testConvertBigDecimalNormals()V\n",
      "DLT stderr Target= <PARAM_VALUE>_90_20181018_459 (Linux<FLOAT_VALUE><FLOAT_VALUE>-134-generic)\n",
      "DLT stderr CPU=amd64 (4 logical CPUs) (<HEX> RAM)\n",
      "DLT stderr ----------- Stack Backtrace -----------\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9prt29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9prt29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9jit29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libj9thr29.so+<HEX>])\n",
      "DLT stderr (<HEX> [libpthread.so<FLOAT_VALUE>+<HEX>])\n",
      "DLT stderr clone+<HEX> (<HEX> [libc.so<FLOAT_VALUE>+<HEX>])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "idx = 3340\n",
    "\n",
    "print(raw_df[raw_df.issue_number == idx].description.values[0])\n",
    "print(\"\\n=========\\n\")\n",
    "print(df[df.issue_number == idx].text.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic_hot\"] = pd.get_dummies(df[\"topic_id\"]).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.0273155365139246, 0.0460672564804554, 0.07449760288000107, 0.04469861090183258, 0.042723797261714935, 0.04860711470246315, 0.048042912036180496, 0.050896868109703064, 0.07637537270784378, 0.038187261670827866, 0.05329722911119461, 0.05156336724758148, 0.03238295391201973, 0.05075827240943909, 0.03402712196111679, 0.06701575219631195, 0.05794834718108177, 0.09218842536211014, 0.03155062720179558, 0.031855564564466476]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().iloc[1][\"topic_probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df[\"component\"].values:\n",
    "    if val is None:\n",
    "        continue\n",
    "    \n",
    "    split = val.split(\",\")\n",
    "    \n",
    "    for s in split:\n",
    "        components.add(s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_values = df[\"component\"].value_counts()\n",
    "filtered_components = component_values.index[component_values >= 20]\n",
    "\n",
    "df = df[df[\"component\"].isin(filtered_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_component(source_df, train_size=0.8):\n",
    "    grouped = source_df.groupby('component')\n",
    "\n",
    "    # Initialize two empty lists to store the split datasets\n",
    "    dataset_1 = []\n",
    "    dataset_2 = []\n",
    "\n",
    "    # Iterate over each group\n",
    "    for _, group_df in grouped:\n",
    "        # Split the group into two halves\n",
    "        first_idx = int(len(group_df) * train_size)\n",
    "        group_half_1 = group_df.iloc[:first_idx]\n",
    "        group_half_2 = group_df.iloc[first_idx:]\n",
    "        \n",
    "        # Append each half to the respective dataset\n",
    "        dataset_1.append(group_half_1)\n",
    "        dataset_2.append(group_half_2)\n",
    "\n",
    "    return pd.concat(dataset_1, ignore_index=True), pd.concat(dataset_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"issue_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2781 309\n"
     ]
    }
   ],
   "source": [
    "components = [\"comp:vm\", \"comp:jvmti\", \"comp:jclextensions\", \"comp:test\", \"comp:build\", \"comp:gc\"]\n",
    "filtered_df = df[df[\"component\"].isin(components)]\n",
    "\n",
    "# Splitting parition by size\n",
    "total_data = len(filtered_df)\n",
    "train_size = int(total_data*0.9)\n",
    "test_size = total_data - train_size\n",
    "df_train = filtered_df[:train_size]\n",
    "df_test = filtered_df[train_size:]\n",
    "\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component\n",
       "comp:vm               1591\n",
       "comp:test              528\n",
       "comp:build             397\n",
       "comp:gc                212\n",
       "comp:jclextensions      36\n",
       "comp:jvmti              17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.component.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component\n",
       "comp:vm               202\n",
       "comp:test              47\n",
       "comp:gc                33\n",
       "comp:jvmti             11\n",
       "comp:build              8\n",
       "comp:jclextensions      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.component.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_train.component.unique()) == set(df_test.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1316834/3718604143.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"component_id\"] = [label2idx[component] for component in df_train[\"component\"].values]\n",
      "/tmp/ipykernel_1316834/3718604143.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"component_id\"] = [label2idx[component] for component in df_test[\"component\"].values]\n"
     ]
    }
   ],
   "source": [
    "# Generate component ids\n",
    "label2idx = {label: idx for idx, label in enumerate(list(df_train[\"component\"].unique()))}\n",
    "df_train[\"component_id\"] = [label2idx[component] for component in df_train[\"component\"].values]\n",
    "df_test[\"component_id\"] = [label2idx[component] for component in df_test[\"component\"].values]\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size 2224 557 309\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size\", len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0403, 0.0788, 0.0247, 0.0942, 0.1848, 0.0248, 0.0778, 0.0678, 0.0480,\n",
       "        0.0763, 0.0446, 0.0414, 0.0000, 0.0247, 0.0000, 0.0259, 0.0000, 0.1002,\n",
       "        0.0197, 0.0259])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([float(x.strip()) for x in df.iloc[1][\"topic_probs\"][1:-1].split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        feature: str = \"text\",\n",
    "        target: str = \"component_id\",\n",
    "    ):\n",
    "        logger.debug(\"Generating torch dataset...\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = [label for label in df[target]]\n",
    "        # self.embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "        logger.debug(\"Tokenizing texts...\")\n",
    "        self.texts = [\n",
    "            (row[feature], self.tokenizer(\n",
    "                row[feature],\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ), torch.tensor([float(x.strip()) for x in row.topic_probs[1:-1].split(\",\")]))\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBTPClassifierTopic(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_size, topic_size, unfrozen_layers=4, dropout=0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        model_name = \"microsoft/deberta-base\"\n",
    "        self.base_model = AutoModel.from_pretrained(\n",
    "            model_name, output_hidden_states=True\n",
    "        )\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze embedding layers\n",
    "        for p in self.base_model.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Freeze encoder layers till last {unfrozen_layers} layers\n",
    "        for i in range(0, self.base_model.config.num_hidden_layers - unfrozen_layers):\n",
    "            for p in self.base_model.encoder.layer[i].parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        filter_sizes = [3, 4, 5, 6]\n",
    "        self._num_filters = 256\n",
    "        self._max_tokens = 512\n",
    "        self._embed_size = self.base_model.config.hidden_size\n",
    "        self.unfrozen_layers = unfrozen_layers\n",
    "        self.conv_blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(1, self._num_filters, (K, self._embed_size)),\n",
    "                            nn.BatchNorm2d(self._num_filters),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(),\n",
    "                            nn.MaxPool1d(self._max_tokens - (K - 1)),\n",
    "                            nn.Flatten(start_dim=1),\n",
    "                        )\n",
    "                        for K in filter_sizes\n",
    "                    ]\n",
    "                )\n",
    "                for _ in range(unfrozen_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifiers = nn.ModuleList(\n",
    "            [\n",
    "                # nn.Linear(\n",
    "                #     len(filter_sizes) * self._num_filters + topic_size, output_size\n",
    "                # )\n",
    "                nn.Linear(\n",
    "                    len(filter_sizes) * self._num_filters, output_size\n",
    "                )\n",
    "                for _ in range(unfrozen_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tok_type, topic_id):\n",
    "        outputs = []\n",
    "\n",
    "        base_out = self.base_model(input_ids=input_ids, token_type_ids=tok_type, attention_mask=attention_mask)\n",
    "        # pooler_out = base_out.last_hidden_state.squeeze(0)\n",
    "        hidden_states = base_out.hidden_states[-self.unfrozen_layers :]\n",
    "\n",
    "        for i in range(self.unfrozen_layers):\n",
    "            batch_size, sequence_length, hidden_size = hidden_states[i].size()\n",
    "            x = [\n",
    "                conv(hidden_states[i].view(batch_size, 1, sequence_length, hidden_size))\n",
    "                for conv in self.conv_blocks[i]\n",
    "            ]\n",
    "            # Concatanating outputs of the conv block of different filter sizes\n",
    "            x = torch.cat(x, dim=1)\n",
    "            x = self.dropout(x)\n",
    "            # x = torch.cat([x, topic_id], dim=1)\n",
    "            x = self.classifiers[i](x)\n",
    "\n",
    "            outputs.append(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def tokenizer(self) -> AutoTokenizer:\n",
    "        return self._tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineLoss(nn.Module):\n",
    "    def __init__(self, weights = None) -> None:\n",
    "        super().__init__()\n",
    "        self._ce = nn.CrossEntropyLoss(weight=weights)\n",
    "    def forward(\n",
    "        self,\n",
    "        prediction,\n",
    "        labels\n",
    "    ) -> torch.Tensor:\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(len(prediction)):\n",
    "            loss += self._ce(prediction[i], labels)\n",
    "            # print(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_test.component.unique()) == set(df_val.component.unique()) == set(df_train.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df_train[\"component\"].unique())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(df_train[\"component_id\"])\n",
    "num_samples = sum(class_counts)\n",
    "labels = df_train[\"component_id\"].to_list() # corresponding labels of samples\n",
    "\n",
    "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "# weights_load_location = f\"/work/disa_lab/projects/triagerx/models/deberta_component_prediction.pt\"\n",
    "weights_save_location = f\"/work/disa_lab/projects/triagerx/models/deberta_component_prediction_notopic_deberta_base_sptokens_6class.pt\"\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "\n",
    "model = LBTPClassifierTopic(len(df_train.component_id.unique()), topic_size=20, unfrozen_layers=4, dropout=0.2)\n",
    "# Load best checkpoint\n",
    "# model.load_state_dict(torch.load(weights_load_location))\n",
    "criterion = CombineLoss(weights=None)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=2, factor=0.1, threshold=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-27 14:09:27.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-04-27 14:09:27.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n",
      "\u001b[32m2024-04-27 14:09:32.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-04-27 14:09:32.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prepare torch dataset from train and validation splits\n",
    "train = TriageDataset(df_train, model.tokenizer())\n",
    "val = TriageDataset(df_val, model.tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:myasb3e4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b36575e2834543825cb3ac27686310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">component_predictiondeberta_base_notopic_sptokens_6_classes</strong> at: <a href='https://wandb.ai/afifaniks/openj9/runs/myasb3e4' target=\"_blank\">https://wandb.ai/afifaniks/openj9/runs/myasb3e4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240427_140826-myasb3e4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:myasb3e4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d8eab07d27420382db25dbe96ca542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111911551819908, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mdafifal.mamun/notebooks/triagerX/wandb/run-20240427_140933-409ed1y0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afifaniks/openj9/runs/409ed1y0' target=\"_blank\">component_predictiondeberta_base_notopic_sptokens_6_classes</a></strong> to <a href='https://wandb.ai/afifaniks/openj9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afifaniks/openj9' target=\"_blank\">https://wandb.ai/afifaniks/openj9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afifaniks/openj9/runs/409ed1y0' target=\"_blank\">https://wandb.ai/afifaniks/openj9/runs/409ed1y0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/afifaniks/openj9/runs/409ed1y0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb419f99240>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"openj9\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=f\"component_predictiondeberta_base_notopic_sptokens_{num_classes}_classes\", \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"Deberta-LBT-P\",\n",
    "    \"dataset\": \"openj9\",\n",
    "    \"epochs\": epochs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False if sampler else True,\n",
    "    sampler=sampler,\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-27 14:09:49.012\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mSelected compute device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.debug(f\"Selected compute device: {device}\")\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step(\n",
    "        epoch_num,\n",
    "        total_acc_train,\n",
    "        total_acc_val,\n",
    "        total_loss_train,\n",
    "        total_loss_val,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        topk,\n",
    "    ):\n",
    "        log = f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                    | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                    | Val Loss: {total_loss_val / len(validation_data): .3f} \\\n",
    "                    | Val Accuracy: {total_acc_val / len(validation_data): .3f} \\\n",
    "                    | Top 3: {topk} \\\n",
    "                    | Precision: {precision: .3f} \\\n",
    "                    | Recall: {recall: .3f} \\\n",
    "                    | F1-score: {f1_score: .3f}\"\n",
    "\n",
    "        logger.info(log)\n",
    "        wandb.log({\n",
    "            \"train_acc\": total_acc_train / len(train_data), \n",
    "            \"train_loss\": total_loss_train / len(train_data),\n",
    "            \"val_acc\": total_acc_val / len(validation_data),\n",
    "            \"val_loss\": total_loss_val / len(validation_data),\n",
    "            \"top3_acc\": topk,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1_score\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.93it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.18it/s]\n",
      "\u001b[32m2024-04-27 14:11:13.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 1 | Train Loss:  0.576                     | Train Accuracy:  0.556                     | Val Loss:  0.718                     | Val Accuracy:  0.309                     | Top 3: 0.7881508078994613                     | Precision:  0.409                     | Recall:  0.505                     | F1-score:  0.368\u001b[0m\n",
      "\u001b[32m2024-04-27 14:11:13.494\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.90it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.20it/s]\n",
      "\u001b[32m2024-04-27 14:12:39.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 2 | Train Loss:  0.334                     | Train Accuracy:  0.791                     | Val Loss:  0.606                     | Val Accuracy:  0.469                     | Top 3: 0.8886894075403949                     | Precision:  0.465                     | Recall:  0.603                     | F1-score:  0.474\u001b[0m\n",
      "\u001b[32m2024-04-27 14:12:39.892\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.91it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s]\n",
      "\u001b[32m2024-04-27 14:14:05.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 3 | Train Loss:  0.236                     | Train Accuracy:  0.867                     | Val Loss:  0.447                     | Val Accuracy:  0.664                     | Top 3: 0.9658886894075404                     | Precision:  0.630                     | Recall:  0.630                     | F1-score:  0.603\u001b[0m\n",
      "\u001b[32m2024-04-27 14:14:05.800\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:15:31.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 4 | Train Loss:  0.179                     | Train Accuracy:  0.911                     | Val Loss:  0.420                     | Val Accuracy:  0.700                     | Top 3: 0.9676840215439856                     | Precision:  0.629                     | Recall:  0.663                     | F1-score:  0.621\u001b[0m\n",
      "\u001b[32m2024-04-27 14:15:31.553\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:16:57.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 5 | Train Loss:  0.142                     | Train Accuracy:  0.933                     | Val Loss:  0.371                     | Val Accuracy:  0.732                     | Top 3: 0.9766606822262118                     | Precision:  0.681                     | Recall:  0.662                     | F1-score:  0.642\u001b[0m\n",
      "\u001b[32m2024-04-27 14:16:57.243\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:18:23.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 6 | Train Loss:  0.114                     | Train Accuracy:  0.954                     | Val Loss:  0.370                     | Val Accuracy:  0.734                     | Top 3: 0.9820466786355476                     | Precision:  0.680                     | Recall:  0.686                     | F1-score:  0.663\u001b[0m\n",
      "\u001b[32m2024-04-27 14:18:23.193\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s]\n",
      "\u001b[32m2024-04-27 14:19:48.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 7 | Train Loss:  0.098                     | Train Accuracy:  0.962                     | Val Loss:  0.334                     | Val Accuracy:  0.759                     | Top 3: 0.9802513464991023                     | Precision:  0.672                     | Recall:  0.662                     | F1-score:  0.652\u001b[0m\n",
      "\u001b[32m2024-04-27 14:19:48.930\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:21:14.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 8 | Train Loss:  0.079                     | Train Accuracy:  0.976                     | Val Loss:  0.381                     | Val Accuracy:  0.749                     | Top 3: 0.9784560143626571                     | Precision:  0.673                     | Recall:  0.682                     | F1-score:  0.657\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s]\n",
      "\u001b[32m2024-04-27 14:22:38.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 9 | Train Loss:  0.068                     | Train Accuracy:  0.978                     | Val Loss:  0.334                     | Val Accuracy:  0.761                     | Top 3: 0.9748653500897666                     | Precision:  0.678                     | Recall:  0.665                     | F1-score:  0.656\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:24:02.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 10 | Train Loss:  0.058                     | Train Accuracy:  0.980                     | Val Loss:  0.309                     | Val Accuracy:  0.779                     | Top 3: 0.9820466786355476                     | Precision:  0.693                     | Recall:  0.656                     | F1-score:  0.659\u001b[0m\n",
      "\u001b[32m2024-04-27 14:24:02.997\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s]\n",
      "\u001b[32m2024-04-27 14:25:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 11 | Train Loss:  0.051                     | Train Accuracy:  0.983                     | Val Loss:  0.309                     | Val Accuracy:  0.783                     | Top 3: 0.9820466786355476                     | Precision:  0.760                     | Recall:  0.670                     | F1-score:  0.688\u001b[0m\n",
      "\u001b[32m2024-04-27 14:25:28.750\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s]\n",
      "\u001b[32m2024-04-27 14:26:54.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 12 | Train Loss:  0.041                     | Train Accuracy:  0.990                     | Val Loss:  0.305                     | Val Accuracy:  0.776                     | Top 3: 0.9820466786355476                     | Precision:  0.727                     | Recall:  0.666                     | F1-score:  0.681\u001b[0m\n",
      "\u001b[32m2024-04-27 14:26:54.398\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s]\n",
      "\u001b[32m2024-04-27 14:28:20.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 13 | Train Loss:  0.037                     | Train Accuracy:  0.993                     | Val Loss:  0.317                     | Val Accuracy:  0.794                     | Top 3: 0.9802513464991023                     | Precision:  0.739                     | Recall:  0.702                     | F1-score:  0.703\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:29:44.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 14 | Train Loss:  0.033                     | Train Accuracy:  0.992                     | Val Loss:  0.362                     | Val Accuracy:  0.776                     | Top 3: 0.9784560143626571                     | Precision:  0.658                     | Recall:  0.667                     | F1-score:  0.651\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.26it/s]\n",
      "\u001b[32m2024-04-27 14:31:08.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 15 | Train Loss:  0.029                     | Train Accuracy:  0.995                     | Val Loss:  0.293                     | Val Accuracy:  0.788                     | Top 3: 0.9820466786355476                     | Precision:  0.663                     | Recall:  0.654                     | F1-score:  0.657\u001b[0m\n",
      "\u001b[32m2024-04-27 14:31:08.423\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s]\n",
      "\u001b[32m2024-04-27 14:32:34.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 16 | Train Loss:  0.025                     | Train Accuracy:  0.997                     | Val Loss:  0.303                     | Val Accuracy:  0.788                     | Top 3: 0.9838420107719928                     | Precision:  0.742                     | Recall:  0.628                     | F1-score:  0.655\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s]\n",
      "\u001b[32m2024-04-27 14:33:58.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 17 | Train Loss:  0.021                     | Train Accuracy:  0.997                     | Val Loss:  0.324                     | Val Accuracy:  0.772                     | Top 3: 0.9802513464991023                     | Precision:  0.722                     | Recall:  0.654                     | F1-score:  0.671\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:35:22.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 18 | Train Loss:  0.020                     | Train Accuracy:  0.995                     | Val Loss:  0.322                     | Val Accuracy:  0.792                     | Top 3: 0.9802513464991023                     | Precision:  0.765                     | Recall:  0.657                     | F1-score:  0.682\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s]\n",
      "\u001b[32m2024-04-27 14:36:46.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 19 | Train Loss:  0.017                     | Train Accuracy:  0.998                     | Val Loss:  0.309                     | Val Accuracy:  0.785                     | Top 3: 0.9802513464991023                     | Precision:  0.739                     | Recall:  0.633                     | F1-score:  0.657\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:38:10.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 20 | Train Loss:  0.015                     | Train Accuracy:  0.999                     | Val Loss:  0.323                     | Val Accuracy:  0.777                     | Top 3: 0.9820466786355476                     | Precision:  0.696                     | Recall:  0.636                     | F1-score:  0.650\u001b[0m\n",
      "Training Steps: 100%|██████████| 223/223 [01:16<00:00,  2.92it/s]\n",
      "Validation Steps: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s]\n",
      "\u001b[32m2024-04-27 14:39:35.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 21 | Train Loss:  0.015                     | Train Accuracy:  0.999                     | Val Loss:  0.328                     | Val Accuracy:  0.783                     | Top 3: 0.9838420107719928                     | Precision:  0.726                     | Recall:  0.677                     | F1-score:  0.675\u001b[0m\n",
      "\u001b[32m2024-04-27 14:39:35.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mEarly stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader, desc=\"Training Steps\"):\n",
    "        # print(train_input)\n",
    "        train_label = train_label.to(device)\n",
    "        mask = train_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "        input_id = train_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "        tok_type = train_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "        repr = train_input[2].to(device)\n",
    "        # print(tok_type.shape, input_id.shape, mask.shape)\n",
    "        # print(repr.dtype, input_id.dtype, mask.dtype)\n",
    "\n",
    "        output = model(input_id, mask, tok_type, repr)\n",
    "\n",
    "        batch_loss = criterion(output, train_label.long())\n",
    "        total_loss_train += batch_loss.item()\n",
    "\n",
    "        output = torch.sum(torch.stack(output), 0)\n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        \n",
    "        total_acc_train += acc\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    correct_top_k = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val_input, val_label in tqdm(val_dataloader, desc=\"Validation Steps\"):\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "            input_id = val_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "            tok_type = val_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "            repr = val_input[2].to(device)\n",
    "\n",
    "            output = model(input_id, mask, tok_type, repr)\n",
    "\n",
    "            batch_loss = criterion(output, val_label.long())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            output = torch.sum(torch.stack(output), 0)\n",
    "            _, top_k_predictions = output.topk(3, 1, True, True)\n",
    "\n",
    "            top_k_predictions = top_k_predictions.t()\n",
    "\n",
    "            correct_top_k += (\n",
    "                top_k_predictions.eq(\n",
    "                    val_label.view(1, -1).expand_as(top_k_predictions)\n",
    "                )\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "\n",
    "            all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.append(val_label.cpu().numpy())\n",
    "\n",
    "            total_acc_val += acc\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    top10 = correct_top_k / len(df_val)\n",
    "\n",
    "    log_step(\n",
    "        epoch_num,\n",
    "        total_acc_train,\n",
    "        total_acc_val,\n",
    "        total_loss_train,\n",
    "        total_loss_val,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        df_train,\n",
    "        df_val,\n",
    "        top10,\n",
    "    )\n",
    "\n",
    "    val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        patience_counter = 0\n",
    "        logger.success(\"Found new best model. Saving weights...\")\n",
    "        torch.save(model.state_dict(), weights_save_location)\n",
    "        best_loss = val_loss\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            logger.info(\"Early stopping...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08433d915c614cea9e1e3610cdd266cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1-score</td><td>▁▃▆▆▇▇▇▇▇▇███▇▇▇▇█▇▇▇</td></tr><tr><td>precision</td><td>▁▂▅▅▆▆▆▆▆▇█▇▇▆▆█▇█▇▇▇</td></tr><tr><td>recall</td><td>▁▄▅▇▇▇▇▇▇▆▇▇█▇▆▅▆▆▆▆▇</td></tr><tr><td>top3_acc</td><td>▁▅▇▇█████████████████</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇██████████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▆▇▇▇█▇█████████████</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1-score</td><td>0.67507</td></tr><tr><td>precision</td><td>0.72601</td></tr><tr><td>recall</td><td>0.67682</td></tr><tr><td>top3_acc</td><td>0.98384</td></tr><tr><td>train_acc</td><td>0.99865</td></tr><tr><td>train_loss</td><td>0.01527</td></tr><tr><td>val_acc</td><td>0.78276</td></tr><tr><td>val_loss</td><td>0.32778</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">component_predictiondeberta_base_notopic_sptokens_6_classes</strong> at: <a href='https://wandb.ai/afifaniks/openj9/runs/409ed1y0' target=\"_blank\">https://wandb.ai/afifaniks/openj9/runs/409ed1y0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240427_140933-409ed1y0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load(weights_save_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/disa_lab/projects/triagerx/models/deberta_component_prediction_notopic_deberta_base_sptokens_6class.pt'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-27 14:39:43.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-04-27 14:39:43.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_ds = TriageDataset(df_test, model.tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(test_ds, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings for all train data\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "all_embeddings = similarity_model.encode(df_train.issue_title.to_list(), batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar_devs(issues, k=5, threshold=0.7):\n",
    "    test_embed = similarity_model.encode(issues)\n",
    "    cos = util.cos_sim(test_embed, all_embeddings)\n",
    "    topk_values, topk_indices = torch.topk(cos, k=k)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for idx, sim_score in zip(topk_indices, topk_values):\n",
    "        sim_threshold = sim_score >= threshold\n",
    "        filtered_idx = idx[sim_threshold].numpy()\n",
    "        similarities.append(df_train.iloc[filtered_idx][\"owner_id\"].unique().tolist())\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_val = 0\n",
    "total_loss_val = 0\n",
    "correct_top_k = 0\n",
    "correct_top_k_wo_sim = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "device=\"cuda\"\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for val_input, val_label in loader:\n",
    "        val_label = val_label.to(device)\n",
    "        mask = val_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "        input_id = val_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "        tok_type = val_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "        repr = val_input[2].to(device)\n",
    "\n",
    "        output = model(input_id, mask, tok_type, repr)\n",
    "\n",
    "\n",
    "\n",
    "        output = torch.sum(torch.stack(output), 0)\n",
    "\n",
    "        #wo similarity\n",
    "        _, top_k_wo_sim = output.topk(3, 1, True, True)\n",
    "\n",
    "        top_k_wo_sim = top_k_wo_sim.t()\n",
    "\n",
    "        correct_top_k_wo_sim += (\n",
    "            top_k_wo_sim.eq(\n",
    "                val_label.view(1, -1).expand_as(top_k_wo_sim)\n",
    "            )\n",
    "            .sum()\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "\n",
    "        # with similarity\n",
    "        # _, top_k_predictions = output.topk(10, 1, True, True)\n",
    "        # similar_preds = get_top_k_similar_devs(val_input[0], threshold=0.65)\n",
    "\n",
    "        # unique_preds = []\n",
    "\n",
    "        # for top, sim in zip(top_k_predictions, similar_preds):\n",
    "        #     # print(top, sim)\n",
    "            \n",
    "        #     copy_pred = top.cpu().numpy().tolist()\n",
    "        #     top_preds = top.cpu().numpy().tolist()[:5]\n",
    "\n",
    "        #     for s in sim:\n",
    "        #         if s not in top_preds:\n",
    "        #             top_preds.append(s)\n",
    "            \n",
    "        #     if len(top_preds) < 10:\n",
    "        #         top_preds = top_preds + copy_pred[5:5 + 10 - len(top_preds)]\n",
    "            \n",
    "        #     unique_preds.append(top_preds)\n",
    "\n",
    "        # unique_preds = torch.tensor(unique_preds).cuda()\n",
    "        # top_k_predictions = unique_preds.t()\n",
    "\n",
    "        # correct_top_k += (\n",
    "        #     top_k_predictions.eq(\n",
    "        #         val_label.view(1, -1).expand_as(top_k_predictions)\n",
    "        #     )\n",
    "        #     .sum()\n",
    "        #     .item()\n",
    "        # )\n",
    "\n",
    "        # # break\n",
    "\n",
    "        # acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "\n",
    "        all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.append(val_label.cpu().numpy())\n",
    "\n",
    "        # total_acc_val += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction without Similarity: 298, 0.9644012944983819\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct Prediction without Similarity: {correct_top_k_wo_sim}, {correct_top_k_wo_sim / len(df_test)}\")\n",
    "# print(f\"Correct Prediction with Similarity: {correct_top_k}, {correct_top_k / len(y_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 3, 5, 6, 1, 2, 1, 1, 2, 1, 3, 2, 2, 1, 1, 2, 2, 2, 2, 1,\n",
       "       4, 2, 2, 2, 1, 1, 3, 1, 2, 7, 1, 2, 3, 1, 2, 6, 2, 1, 5, 2, 6, 6,\n",
       "       2, 4, 1, 2, 1, 6, 2, 1, 1, 3, 1, 1, 6, 1, 2, 1, 1, 1, 1, 6, 1, 4,\n",
       "       2, 4, 2, 1, 2, 2, 1, 1, 2, 1, 2, 6, 0, 4, 4, 1, 6, 2, 5, 6, 1, 2,\n",
       "       1, 1, 2, 2, 2, 2, 1, 3, 2, 1, 6, 5, 3, 1, 2, 2, 1, 3, 2, 6, 2, 5,\n",
       "       2, 2, 4, 2, 6, 2, 3, 5, 1, 1, 1, 2, 1, 1, 1, 4, 2, 1, 5, 1, 6, 2,\n",
       "       0, 1, 1, 1, 6, 6, 1, 2, 5, 1, 3, 1, 5, 3, 2, 2, 1, 2, 2, 1, 5, 1,\n",
       "       6, 6, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 5, 2, 6, 1, 6,\n",
       "       6, 6, 2, 1, 1, 0, 1, 1, 6, 1, 4, 6, 2, 2, 3, 1, 2, 1, 1, 2, 2, 6,\n",
       "       1, 3, 4, 4, 1, 5, 2, 2, 1, 0, 4, 2, 1, 2, 2, 6, 1, 2, 5, 2, 1, 6,\n",
       "       6, 4, 2, 5, 1, 1, 2, 2, 2, 4, 1, 2, 2, 2, 1, 1, 2, 1, 3, 2, 4, 3,\n",
       "       0, 4, 1, 1, 1, 1, 3, 5, 1, 1, 2, 4, 1, 2, 1, 2, 3, 5, 2, 1, 1, 6,\n",
       "       1, 6, 1, 1, 4, 2, 1, 1, 1, 4, 6, 6, 0, 2, 3, 3, 1, 2, 6, 6, 4, 3,\n",
       "       1, 2, 1, 6, 2, 6, 1, 0, 6, 2, 2, 5, 2, 4, 3, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 6, 1, 3, 2, 7, 1, 1, 6, 1, 5, 1, 6, 2, 3, 1, 3, 2, 4,\n",
       "       1, 1, 1, 2, 2, 0, 1, 1, 1, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 6, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 6, 6, 1, 3, 3, 2, 1,\n",
       "       1, 2, 6, 2, 6, 2, 1, 5, 5, 2, 6, 3, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "       1, 1, 1, 6, 0, 6, 2, 2, 1, 2, 6, 1, 1, 1, 5, 2, 2, 1, 6, 1, 1, 1,\n",
       "       2, 1, 1, 2, 1, 1, 6, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2,\n",
       "       2, 1, 1, 6, 5, 2, 4, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 5, 0, 2,\n",
       "       3, 2, 6, 1, 5, 2, 2, 3, 6, 2, 1, 2, 3, 1, 4, 2, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 6, 2, 2, 2, 5, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 3, 5, 6, 1, 2, 1, 1, 2, 1, 3, 2, 2, 1, 1, 2, 2, 2, 2, 1,\n",
       "       4, 2, 2, 2, 1, 1, 3, 1, 2, 7, 1, 2, 3, 1, 2, 6, 2, 1, 5, 2, 6, 6,\n",
       "       2, 4, 1, 2, 1, 6, 2, 1, 1, 3, 1, 1, 6, 1, 2, 1, 1, 1, 1, 6, 1, 4,\n",
       "       2, 4, 2, 1, 2, 2, 1, 1, 2, 1, 2, 6, 0, 4, 4, 1, 6, 2, 5, 6, 1, 2,\n",
       "       1, 1, 2, 2, 2, 2, 1, 3, 2, 1, 6, 5, 3, 1, 2, 2, 1, 3, 2, 6, 2, 5,\n",
       "       2, 2, 4, 2, 6, 2, 3, 5, 1, 1, 1, 2, 1, 1, 1, 4, 2, 1, 5, 1, 6, 2,\n",
       "       0, 1, 1, 1, 6, 6, 1, 2, 5, 1, 3, 1, 5, 3, 2, 2, 1, 2, 2, 1, 5, 1,\n",
       "       6, 6, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 5, 2, 6, 1, 6,\n",
       "       6, 6, 2, 1, 1, 0, 1, 1, 6, 1, 4, 6, 2, 2, 3, 1, 2, 1, 1, 2, 2, 6,\n",
       "       1, 3, 4, 4, 1, 5, 2, 2, 1, 0, 4, 2, 1, 2, 2, 6, 1, 2, 5, 2, 1, 6,\n",
       "       6, 4, 2, 5, 1, 1, 2, 2, 2, 4, 1, 2, 2, 2, 1, 1, 2, 1, 3, 2, 4, 3,\n",
       "       0, 4, 1, 1, 1, 1, 3, 5, 1, 1, 2, 4, 1, 2, 1, 2, 3, 5, 2, 1, 1, 6,\n",
       "       1, 6, 1, 1, 4, 2, 1, 1, 1, 4, 6, 6, 0, 2, 3, 3, 1, 2, 6, 6, 4, 3,\n",
       "       1, 2, 1, 6, 2, 6, 1, 0, 6, 2, 2, 5, 2, 4, 3, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 6, 1, 3, 2, 7, 1, 1, 6, 1, 5, 1, 6, 2, 3, 1, 3, 2, 4,\n",
       "       1, 1, 1, 2, 2, 0, 1, 1, 1, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 6, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 6, 6, 1, 3, 3, 2, 1,\n",
       "       1, 2, 6, 2, 6, 2, 1, 5, 5, 2, 6, 3, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "       1, 1, 1, 6, 0, 6, 2, 2, 1, 2, 6, 1, 1, 1, 5, 2, 2, 1, 6, 1, 1, 1,\n",
       "       2, 1, 1, 2, 1, 1, 6, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2,\n",
       "       2, 1, 1, 6, 5, 2, 4, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 5, 0, 2,\n",
       "       3, 2, 6, 1, 5, 2, 2, 3, 6, 2, 1, 2, 3, 1, 4, 2, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 6, 2, 2, 2, 5, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_np = np.concatenate(all_preds)\n",
    "all_labels_np = np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 32, does not match size of target_names, 2518. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/trx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2567\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2561\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2563\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[1;32m   2564\u001b[0m             )\n\u001b[1;32m   2565\u001b[0m         )\n\u001b[1;32m   2566\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2573\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 32, does not match size of target_names, 2518. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels_np, all_preds_np, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {\n",
    "    row[\"owner_id\"]: row[\"owner\"]\n",
    "    for _, row in y_df.iterrows()\n",
    "}\n",
    "\n",
    "labels = y_df.owner_id.to_list()\n",
    "labels = sorted(set(labels))\n",
    "labels = [f\"{idx}: {idx2label[idx]}\" for idx in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(owner):\n",
    "    print(\"Training topic distribution\")\n",
    "    print(\"=======================================\")\n",
    "    print(X_df[X_df.owner == owner].topic_label.value_counts())\n",
    "\n",
    "    print(\"\\n\\nTesting topic distribution\")\n",
    "    print(\"=======================================\")\n",
    "    print(y_df[y_df.owner == owner].topic_label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chrome Tab and Window Behavior Issues     947\n",
       "Build failures                            840\n",
       "Chrome stability issues                   487\n",
       "Layout Testing Issues                     400\n",
       "Chrome crash reports                      391\n",
       "Security and SSL issues                   372\n",
       "Input and keyboard issues                 370\n",
       "Webpage rendering regression issues       357\n",
       "Chrome sync issues                        354\n",
       "Shill WiFi configuration                  337\n",
       "iOS File Issues                           321\n",
       "Data Enhancement                          298\n",
       "Touch and Scroll Issues                   273\n",
       "DevTools Crashes                          260\n",
       "GPU rendering issues                      235\n",
       "Memory Leaks in WebCore and Blink         220\n",
       "Performance testing issues in Chromium    197\n",
       "WebRTC audio/video issues                 184\n",
       "Bookmark issues                           174\n",
       "Performance Regression in Blink            13\n",
       "Name: topic_label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.topic_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training topic distribution\n",
      "=======================================\n",
      "Chrome Tab and Window Behavior Issues     10\n",
      "Webpage rendering regression issues        5\n",
      "Memory Leaks in WebCore and Blink          4\n",
      "Chrome stability issues                    4\n",
      "DevTools Crashes                           3\n",
      "Data Enhancement                           3\n",
      "Input and keyboard issues                  3\n",
      "Touch and Scroll Issues                    2\n",
      "Bookmark issues                            1\n",
      "Security and SSL issues                    1\n",
      "Chrome sync issues                         1\n",
      "Layout Testing Issues                      1\n",
      "Build failures                             1\n",
      "iOS File Issues                            1\n",
      "Performance testing issues in Chromium     1\n",
      "Name: topic_label, dtype: int64\n",
      "\n",
      "\n",
      "Testing topic distribution\n",
      "=======================================\n",
      "Chrome Tab and Window Behavior Issues     17\n",
      "Webpage rendering regression issues        7\n",
      "DevTools Crashes                           6\n",
      "iOS File Issues                            6\n",
      "Touch and Scroll Issues                    4\n",
      "Input and keyboard issues                  4\n",
      "Data Enhancement                           4\n",
      "Chrome crash reports                       4\n",
      "Chrome stability issues                    3\n",
      "Memory Leaks in WebCore and Blink          2\n",
      "Chrome sync issues                         2\n",
      "Bookmark issues                            1\n",
      "Performance testing issues in Chromium     1\n",
      "Name: topic_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_topic_distribution(\"a...@chromium.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
