{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from loguru import logger\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, PreTrainedTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_split(x):\n",
    "    x_split = str(x).split(\",\")\n",
    "\n",
    "    for s in x_split:\n",
    "        if \"comp:\" in s.lower():\n",
    "            return s.strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/openj9_processed.csv\"\n",
    "\n",
    "raw_df = pd.read_csv(dataset_path)\n",
    "print(len(raw_df))\n",
    "raw_df = raw_df.rename(columns={\"assignees\": \"owner\", \"issue_body\": \"description\"})\n",
    "# df = df[df[\"owner\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All issues: 7348\n",
      "Excluding pull: 7348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7348/7348 [00:00<00:00, 100232.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues: 7348\n"
     ]
    }
   ],
   "source": [
    "special_tokens = {\n",
    "    \"hex\": \"[HEX]\",\n",
    "    \"timestamp\": \"[TIMESTAMP]\",\n",
    "    \"numeric\": \"[NUMERIC]\",\n",
    "    \"param\": \"[PARAM_VALUE]\",\n",
    "    \"version\": \"[VERSION]\",\n",
    "    \"ip\": \"[IP_ADDRESS]\",\n",
    "    \"filepath\": \"[FILE_PATH]\",\n",
    "    \"url\": \"[URL]\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_issue_description(text):\n",
    "    text = str(text)\n",
    "    cleaned_text = text.strip()\n",
    "    cleaned_text = re.sub(r'(https?|ftp):\\/\\/[^\\s/$.?#].[^\\s]*', special_tokens[\"url\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'0x[\\da-fA-F]+', special_tokens[\"hex\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b[0-9a-fA-F]{16}\\b', special_tokens[\"hex\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b.*/([^/]+)', rf\"{special_tokens['filepath']}/\\1\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\b([A-Za-z]:)?.*\\\\([^\\\\]+)\", rf\"{special_tokens['filepath']}/\\2\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', special_tokens[\"ip\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r\"(?<!\\w)\\d+\\.\\d+\\.\\d+(\\.\\d+)*(_\\d+)?(-[a-zA-Z]+\\d*)?(?!\\w)\", special_tokens[\"version\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}:\\d{4,} GMT\\b', special_tokens[\"timestamp\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}(\\.\\d{2,3})?\\b', special_tokens[\"timestamp\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d+Z\\b', special_tokens[\"timestamp\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\b[-+]?\\d*\\.\\d+([eE][-+]?\\d+)?\\b', special_tokens[\"numeric\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\d{4,}\\b', special_tokens[\"numeric\"], cleaned_text)\n",
    "    cleaned_text = re.sub(r'=\\s*-?\\d+', f'= {special_tokens[\"param\"]}', cleaned_text)\n",
    "    cleaned_text = re.sub(r'```', \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'-{3,}', \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'[\\*#=+\\-]{3,}', \"\", cleaned_text)\n",
    "    \n",
    "    for special_token in special_tokens.values():\n",
    "        sp_token = special_token[1:-1]\n",
    "        cleaned_text = re.sub(rf'\\[{sp_token}\\]\\s*(\\[{sp_token}\\]\\s*)+', f\"{special_token}\", cleaned_text)\n",
    "        \n",
    "    cleaned_text = re.sub(r'(\\r?\\n)+', \"\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'(?![\\r\\n])\\s+', \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_data(df):\n",
    "    df['text'] = df['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)\n",
    "    df[\"text\"] = df['text'].str.replace(\" +\", \" \", regex=True)\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_issue_description)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df[\"labels\"].notna()]\n",
    "    print(f\"All issues: {len(df)}\")\n",
    "    print(f\"Excluding pull: {len(df)}\")\n",
    "    df = df[~df[\"issue_url\"].str.contains(\"/pull/\")]\n",
    "    \n",
    "    df[\"component\"] = df[\"labels\"].apply(component_split)\n",
    "    \n",
    "    df[\"text\"] = df.progress_apply(\n",
    "            lambda x: \"Title: \"\n",
    "            + str(x[\"issue_title\"])\n",
    "            # + \"\\nIssue Labels: \"\n",
    "            # + str(x[\"labels\"])\n",
    "            # + \"\\nIssue Topic: \"\n",
    "            # + str(x[\"topic_label\"])\n",
    "            + \"\\nDescription: \"\n",
    "            + str(x[\"description\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "    \n",
    "    min_length = 15\n",
    "    df = df[df[\"text\"].str.len().gt(min_length)]\n",
    "\n",
    "    # df[\"owner_id\"] = pd.factorize(df[\"assignees\"])[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = prepare_dataframe(raw_df)\n",
    "df = clean_data(df)\n",
    "df = df.sort_values(by=\"issue_number\")\n",
    "\n",
    "num_issues = len(df)\n",
    "\n",
    "print(f\"Total number of issues: {num_issues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df[\"component\"].values:\n",
    "    if val is None:\n",
    "        continue\n",
    "    \n",
    "    split = val.split(\",\")\n",
    "    \n",
    "    for s in split:\n",
    "        components.add(s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_values = df[\"component\"].value_counts()\n",
    "filtered_components = component_values.index[component_values >= 20]\n",
    "\n",
    "df = df[df[\"component\"].isin(filtered_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"issue_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472 618\n"
     ]
    }
   ],
   "source": [
    "components = [\"comp:vm\", \"comp:jvmti\", \"comp:jclextensions\", \"comp:test\", \"comp:build\", \"comp:gc\"]\n",
    "filtered_df = df[df[\"component\"].isin(components)]\n",
    "\n",
    "# Splitting parition by size\n",
    "# total_data = len(filtered_df)\n",
    "# train_size = int(total_data*0.9)\n",
    "# test_size = total_data - train_size\n",
    "# df_train = filtered_df[:train_size]\n",
    "# df_test = filtered_df[train_size:]\n",
    "\n",
    "df_train, df_test = train_test_split(filtered_df, test_size=0.2)\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/component_training/df_train.csv\")\n",
    "# df_test.to_csv(\"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/component_training/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/component_training/df_train_summarized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/home/mdafifal.mamun/notebooks/triagerX/notebook/data/openj9/component_training/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train.issue_number).intersection(set(df_test.issue_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_state</th>\n",
       "      <th>creator</th>\n",
       "      <th>labels</th>\n",
       "      <th>owner</th>\n",
       "      <th>component</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6838</td>\n",
       "      <td>17078</td>\n",
       "      <td>[JDK20/FFI_Jtreg] Crash detected in StdLibTest</td>\n",
       "      <td>The crashed was detected in https://github.com...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>closed</td>\n",
       "      <td>ChengJin01</td>\n",
       "      <td>comp:vm, project:panama, test failure, jdk20</td>\n",
       "      <td>ChengJin01</td>\n",
       "      <td>comp:vm</td>\n",
       "      <td>Title: [JDK20/FFI_Jtreg] Crash detected in Std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6583</td>\n",
       "      <td>16503</td>\n",
       "      <td>cmdLineTester_criu_keepCheckpoint_2_FAILED org...</td>\n",
       "      <td>Failure link\\r\\n------------\\r\\n\\r\\nFrom [an i...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>open</td>\n",
       "      <td>JasonFengJ9</td>\n",
       "      <td>comp:test, test failure, criu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comp:test</td>\n",
       "      <td>Title: cmdLineTester_criu_keepCheckpoint_2_FAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>5822</td>\n",
       "      <td>Java 13, deprecate -Xverify:none and -noverify</td>\n",
       "      <td>The following OpenJDK Java 13 change in the re...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>closed</td>\n",
       "      <td>pshipton</td>\n",
       "      <td>comp:vm, doc:externals, jdk13</td>\n",
       "      <td>theresa-m</td>\n",
       "      <td>comp:vm</td>\n",
       "      <td>Title: Java 13, deprecate -Xverify:none and -n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>116</td>\n",
       "      <td>Travis PR builds broken</td>\n",
       "      <td>Travis pull request builds are broken. They ti...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>closed</td>\n",
       "      <td>dnakamura</td>\n",
       "      <td>comp:build</td>\n",
       "      <td>tajila</td>\n",
       "      <td>comp:build</td>\n",
       "      <td>Title: Travis PR builds broken\\nDescription: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2563</td>\n",
       "      <td>7364</td>\n",
       "      <td>ppc64le Calendar.getInstance incorrect timezone</td>\n",
       "      <td>https://ci.eclipse.org/openj9/job/Test_openjdk...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>closed</td>\n",
       "      <td>pshipton</td>\n",
       "      <td>comp:test, test failure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comp:test</td>\n",
       "      <td>Title: ppc64le Calendar.getInstance incorrect ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  issue_number  \\\n",
       "0        6838         17078   \n",
       "1        6583         16503   \n",
       "2        2018          5822   \n",
       "3          44           116   \n",
       "4        2563          7364   \n",
       "\n",
       "                                         issue_title  \\\n",
       "0     [JDK20/FFI_Jtreg] Crash detected in StdLibTest   \n",
       "1  cmdLineTester_criu_keepCheckpoint_2_FAILED org...   \n",
       "2    Java 13, deprecate -Xverify:none and -noverify    \n",
       "3                            Travis PR builds broken   \n",
       "4    ppc64le Calendar.getInstance incorrect timezone   \n",
       "\n",
       "                                         description  \\\n",
       "0  The crashed was detected in https://github.com...   \n",
       "1  Failure link\\r\\n------------\\r\\n\\r\\nFrom [an i...   \n",
       "2  The following OpenJDK Java 13 change in the re...   \n",
       "3  Travis pull request builds are broken. They ti...   \n",
       "4  https://ci.eclipse.org/openj9/job/Test_openjdk...   \n",
       "\n",
       "                                           issue_url issue_state      creator  \\\n",
       "0  https://github.com/eclipse-openj9/openj9/issue...      closed   ChengJin01   \n",
       "1  https://github.com/eclipse-openj9/openj9/issue...        open  JasonFengJ9   \n",
       "2  https://github.com/eclipse-openj9/openj9/issue...      closed     pshipton   \n",
       "3  https://github.com/eclipse-openj9/openj9/issue...      closed    dnakamura   \n",
       "4  https://github.com/eclipse-openj9/openj9/issue...      closed     pshipton   \n",
       "\n",
       "                                         labels       owner   component  \\\n",
       "0  comp:vm, project:panama, test failure, jdk20  ChengJin01     comp:vm   \n",
       "1                 comp:test, test failure, criu         NaN   comp:test   \n",
       "2                 comp:vm, doc:externals, jdk13   theresa-m     comp:vm   \n",
       "3                                    comp:build      tajila  comp:build   \n",
       "4                       comp:test, test failure         NaN   comp:test   \n",
       "\n",
       "                                                text  \n",
       "0  Title: [JDK20/FFI_Jtreg] Crash detected in Std...  \n",
       "1  Title: cmdLineTester_criu_keepCheckpoint_2_FAI...  \n",
       "2  Title: Java 13, deprecate -Xverify:none and -n...  \n",
       "3  Title: Travis PR builds broken\\nDescription: T...  \n",
       "4  Title: ppc64le Calendar.getInstance incorrect ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2472/2472 [00:00<00:00, 110620.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_train[\"description\"] = df_train[\"description\"].progress_apply(clean_issue_description)\n",
    "df_train[\"text\"] = df_train.progress_apply(\n",
    "        lambda x: \"Bug Title: \"\n",
    "        + str(x[\"issue_title\"])\n",
    "        # + \"\\nIssue Labels: \"\n",
    "        # + str(x[\"labels\"])\n",
    "        # + \"\\nIssue Topic: \"\n",
    "        # + str(x[\"topic_label\"])\n",
    "        # + \"\\nBug Summary: \"\n",
    "        # + str(x[\"summary\"]),\n",
    "        + \"\\nBug Description: \"\n",
    "        + str(x[\"description\"]),\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"text\"] = df_train[\"text\"].progress_apply(clean_issue_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component\n",
       "comp:vm               1433\n",
       "comp:test              476\n",
       "comp:build             316\n",
       "comp:gc                187\n",
       "comp:jclextensions      37\n",
       "comp:jvmti              23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.component.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component\n",
       "comp:vm               360\n",
       "comp:test              99\n",
       "comp:build             89\n",
       "comp:gc                58\n",
       "comp:jclextensions      7\n",
       "comp:jvmti              5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.component.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_train.component.unique()) == set(df_test.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate component ids\n",
    "label2idx = {label: idx for idx, label in enumerate(sorted(list(df_train[\"component\"].unique())))}\n",
    "df_train[\"component_id\"] = [label2idx[component] for component in df_train[\"component\"].values]\n",
    "df_test[\"component_id\"] = [label2idx[component] for component in df_test[\"component\"].values]\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=77, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1977, 495)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size 1977 495 618\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size\", len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        feature: str = \"text\",\n",
    "        target: str = \"component_id\",\n",
    "    ):\n",
    "        logger.debug(\"Generating torch dataset...\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = [label for label in df[target]]\n",
    "        # self.embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "        logger.debug(\"Tokenizing texts...\")\n",
    "        self.texts = [\n",
    "            (row[feature], self.tokenizer(\n",
    "                row[feature],\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ))\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_test.component.unique()) == set(df_val.component.unique()) == set(df_train.component.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df_train[\"component\"].unique())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triagerx.loss.loss_functions import *\n",
    "from triagerx.model.lbtp_bilstm import LBTPBiLSTM\n",
    "from triagerx.model.lbt_p_deberta import LBTPDeberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(df_train[\"component_id\"])\n",
    "num_samples = sum(class_counts)\n",
    "labels = df_train[\"component_id\"].to_list() # corresponding labels of samples\n",
    "\n",
    "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "unfrozen_layers=5\n",
    "\n",
    "\n",
    "model = LBTPDeberta(\n",
    "    len(df_train.component_id.unique()), \n",
    "    unfrozen_layers=unfrozen_layers, \n",
    "    dropout=0.2, \n",
    "    base_model=\"microsoft/deberta-base\"\n",
    ")\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.001)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=2, factor=0.1, threshold=1e-10)\n",
    "\n",
    "run_name = f\"comp_raw_data_{model.__class__.__name__}_u{unfrozen_layers}_{num_classes}_classes_{criterion.__class__.__name__}\"\n",
    "# weights_load_location = f\"/work/disa_lab/projects/triagerx/models/deberta_component_prediction.pt\"\n",
    "weights_save_location = f\"/work/disa_lab/projects/triagerx/models/{run_name}.pt\"\n",
    "\n",
    "\n",
    "# Load best checkpoint\n",
    "# model.load_state_dict(torch.load(weights_load_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens_dict = {\"additional_special_tokens\": list(special_tokens.values())}\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# model.base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-12 22:02:13.348\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-05-12 22:02:13.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n",
      "\u001b[32m2024-05-12 22:02:17.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-05-12 22:02:17.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prepare torch dataset from train and validation splits\n",
    "train = TriageDataset(df_train, tokenizer)\n",
    "val = TriageDataset(df_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafifaniks\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69833358f0345f1abb3d6727fe177e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111901166926449, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mdafifal.mamun/notebooks/triagerX/wandb/run-20240512_174509-kwrgbjpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj' target=\"_blank\">comp_sp_token_LBTPDeberta_u5_6_classes_CombinedLoss</a></strong> to <a href='https://wandb.ai/afifaniks/openj9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afifaniks/openj9' target=\"_blank\">https://wandb.ai/afifaniks/openj9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj' target=\"_blank\">https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa387366800>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"openj9\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=run_name, \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"Deberta-LBT-P\",\n",
    "    \"dataset\": \"openj9\",\n",
    "    \"epochs\": epochs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False if sampler else True,\n",
    "    sampler=sampler,\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-12 22:02:25.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mSelected compute device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.debug(f\"Selected compute device: {device}\")\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step(\n",
    "        epoch_num,\n",
    "        total_acc_train,\n",
    "        total_acc_val,\n",
    "        total_loss_train,\n",
    "        total_loss_val,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        topk,\n",
    "    ):\n",
    "        log = f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                    | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                    | Val Loss: {total_loss_val / len(validation_data): .3f} \\\n",
    "                    | Val Accuracy: {total_acc_val / len(validation_data): .3f} \\\n",
    "                    | Top 3: {topk} \\\n",
    "                    | Precision: {precision: .3f} \\\n",
    "                    | Recall: {recall: .3f} \\\n",
    "                    | F1-score: {f1_score: .3f}\"\n",
    "\n",
    "        logger.info(log)\n",
    "        wandb.log({\n",
    "            \"train_acc\": total_acc_train / len(train_data), \n",
    "            \"train_loss\": total_loss_train / len(train_data),\n",
    "            \"val_acc\": total_acc_val / len(validation_data),\n",
    "            \"val_loss\": total_loss_val / len(validation_data),\n",
    "            \"top3_acc\": topk,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1_score\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  6.96it/s]\n",
      "\u001b[32m2024-05-12 17:46:38.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 1 | Train Loss:  1.274                     | Train Accuracy:  0.212                     | Val Loss:  1.037                     | Val Accuracy:  0.222                     | Top 3: 0.6303030303030303                     | Precision:  0.166                     | Recall:  0.220                     | F1-score:  0.138\u001b[0m\n",
      "\u001b[32m2024-05-12 17:46:38.451\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:11<00:00,  2.79it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  6.97it/s]\n",
      "\u001b[32m2024-05-12 17:47:58.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 2 | Train Loss:  0.857                     | Train Accuracy:  0.470                     | Val Loss:  0.934                     | Val Accuracy:  0.313                     | Top 3: 0.7656565656565657                     | Precision:  0.261                     | Recall:  0.391                     | F1-score:  0.243\u001b[0m\n",
      "\u001b[32m2024-05-12 17:47:58.340\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.79it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 17:49:17.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 3 | Train Loss:  0.677                     | Train Accuracy:  0.625                     | Val Loss:  0.840                     | Val Accuracy:  0.394                     | Top 3: 0.8464646464646465                     | Precision:  0.303                     | Recall:  0.402                     | F1-score:  0.305\u001b[0m\n",
      "\u001b[32m2024-05-12 17:49:17.850\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 17:50:37.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 4 | Train Loss:  0.516                     | Train Accuracy:  0.735                     | Val Loss:  0.713                     | Val Accuracy:  0.558                     | Top 3: 0.9191919191919192                     | Precision:  0.380                     | Recall:  0.490                     | F1-score:  0.401\u001b[0m\n",
      "\u001b[32m2024-05-12 17:50:37.203\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 17:51:56.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 5 | Train Loss:  0.373                     | Train Accuracy:  0.852                     | Val Loss:  0.617                     | Val Accuracy:  0.663                     | Top 3: 0.9696969696969697                     | Precision:  0.473                     | Recall:  0.563                     | F1-score:  0.499\u001b[0m\n",
      "\u001b[32m2024-05-12 17:51:56.460\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 17:53:15.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 6 | Train Loss:  0.286                     | Train Accuracy:  0.891                     | Val Loss:  0.573                     | Val Accuracy:  0.675                     | Top 3: 0.9696969696969697                     | Precision:  0.556                     | Recall:  0.596                     | F1-score:  0.551\u001b[0m\n",
      "\u001b[32m2024-05-12 17:53:15.783\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 17:54:35.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 7 | Train Loss:  0.222                     | Train Accuracy:  0.939                     | Val Loss:  0.520                     | Val Accuracy:  0.701                     | Top 3: 0.9696969696969697                     | Precision:  0.584                     | Recall:  0.615                     | F1-score:  0.566\u001b[0m\n",
      "\u001b[32m2024-05-12 17:54:35.142\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 17:55:54.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 8 | Train Loss:  0.175                     | Train Accuracy:  0.950                     | Val Loss:  0.498                     | Val Accuracy:  0.717                     | Top 3: 0.9838383838383838                     | Precision:  0.582                     | Recall:  0.601                     | F1-score:  0.568\u001b[0m\n",
      "\u001b[32m2024-05-12 17:55:54.483\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 17:57:13.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 9 | Train Loss:  0.151                     | Train Accuracy:  0.957                     | Val Loss:  0.484                     | Val Accuracy:  0.713                     | Top 3: 0.9858585858585859                     | Precision:  0.565                     | Recall:  0.621                     | F1-score:  0.569\u001b[0m\n",
      "\u001b[32m2024-05-12 17:57:13.799\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 17:58:33.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 10 | Train Loss:  0.117                     | Train Accuracy:  0.964                     | Val Loss:  0.479                     | Val Accuracy:  0.727                     | Top 3: 0.9818181818181818                     | Precision:  0.582                     | Recall:  0.644                     | F1-score:  0.586\u001b[0m\n",
      "\u001b[32m2024-05-12 17:58:33.094\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 17:59:52.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 11 | Train Loss:  0.107                     | Train Accuracy:  0.976                     | Val Loss:  0.443                     | Val Accuracy:  0.741                     | Top 3: 0.9797979797979798                     | Precision:  0.679                     | Recall:  0.609                     | F1-score:  0.602\u001b[0m\n",
      "\u001b[32m2024-05-12 17:59:52.605\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:01:12.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 12 | Train Loss:  0.089                     | Train Accuracy:  0.979                     | Val Loss:  0.447                     | Val Accuracy:  0.754                     | Top 3: 0.9818181818181818                     | Precision:  0.665                     | Recall:  0.638                     | F1-score:  0.603\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 18:02:30.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 13 | Train Loss:  0.076                     | Train Accuracy:  0.987                     | Val Loss:  0.447                     | Val Accuracy:  0.731                     | Top 3: 0.9838383838383838                     | Precision:  0.628                     | Recall:  0.640                     | F1-score:  0.608\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 18:03:48.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 14 | Train Loss:  0.068                     | Train Accuracy:  0.990                     | Val Loss:  0.418                     | Val Accuracy:  0.754                     | Top 3: 0.9838383838383838                     | Precision:  0.673                     | Recall:  0.635                     | F1-score:  0.610\u001b[0m\n",
      "\u001b[32m2024-05-12 18:03:48.405\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:05:08.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 15 | Train Loss:  0.063                     | Train Accuracy:  0.988                     | Val Loss:  0.406                     | Val Accuracy:  0.790                     | Top 3: 0.9818181818181818                     | Precision:  0.659                     | Recall:  0.636                     | F1-score:  0.613\u001b[0m\n",
      "\u001b[32m2024-05-12 18:05:08.025\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:06:27.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 16 | Train Loss:  0.050                     | Train Accuracy:  0.994                     | Val Loss:  0.403                     | Val Accuracy:  0.772                     | Top 3: 0.9858585858585859                     | Precision:  0.697                     | Recall:  0.638                     | F1-score:  0.628\u001b[0m\n",
      "\u001b[32m2024-05-12 18:06:27.334\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 18:07:46.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 17 | Train Loss:  0.048                     | Train Accuracy:  0.994                     | Val Loss:  0.397                     | Val Accuracy:  0.772                     | Top 3: 0.9838383838383838                     | Precision:  0.698                     | Recall:  0.634                     | F1-score:  0.620\u001b[0m\n",
      "\u001b[32m2024-05-12 18:07:46.559\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "\u001b[32m2024-05-12 18:09:06.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 18 | Train Loss:  0.037                     | Train Accuracy:  0.997                     | Val Loss:  0.417                     | Val Accuracy:  0.766                     | Top 3: 0.98989898989899                     | Precision:  0.688                     | Recall:  0.634                     | F1-score:  0.586\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:10:23.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 19 | Train Loss:  0.036                     | Train Accuracy:  0.998                     | Val Loss:  0.382                     | Val Accuracy:  0.790                     | Top 3: 0.9858585858585859                     | Precision:  0.716                     | Recall:  0.620                     | F1-score:  0.623\u001b[0m\n",
      "\u001b[32m2024-05-12 18:10:23.989\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  6.99it/s]\n",
      "\u001b[32m2024-05-12 18:11:43.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 20 | Train Loss:  0.032                     | Train Accuracy:  0.997                     | Val Loss:  0.451                     | Val Accuracy:  0.737                     | Top 3: 0.9797979797979798                     | Precision:  0.676                     | Recall:  0.637                     | F1-score:  0.607\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:13:01.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 21 | Train Loss:  0.030                     | Train Accuracy:  0.995                     | Val Loss:  0.378                     | Val Accuracy:  0.796                     | Top 3: 0.9797979797979798                     | Precision:  0.767                     | Recall:  0.603                     | F1-score:  0.632\u001b[0m\n",
      "\u001b[32m2024-05-12 18:13:01.058\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[32m\u001b[1mFound new best model. Saving weights...\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:14:20.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 22 | Train Loss:  0.029                     | Train Accuracy:  0.996                     | Val Loss:  0.389                     | Val Accuracy:  0.788                     | Top 3: 0.9838383838383838                     | Precision:  0.729                     | Recall:  0.631                     | F1-score:  0.641\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 18:15:38.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 23 | Train Loss:  0.023                     | Train Accuracy:  0.999                     | Val Loss:  0.403                     | Val Accuracy:  0.786                     | Top 3: 0.9838383838383838                     | Precision:  0.713                     | Recall:  0.653                     | F1-score:  0.643\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:16:55.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 24 | Train Loss:  0.022                     | Train Accuracy:  0.998                     | Val Loss:  0.388                     | Val Accuracy:  0.788                     | Top 3: 0.9878787878787879                     | Precision:  0.731                     | Recall:  0.654                     | F1-score:  0.652\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:18:13.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 25 | Train Loss:  0.020                     | Train Accuracy:  0.999                     | Val Loss:  0.406                     | Val Accuracy:  0.792                     | Top 3: 0.9818181818181818                     | Precision:  0.716                     | Recall:  0.658                     | F1-score:  0.644\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:19:31.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 26 | Train Loss:  0.017                     | Train Accuracy:  0.998                     | Val Loss:  0.388                     | Val Accuracy:  0.796                     | Top 3: 0.9858585858585859                     | Precision:  0.735                     | Recall:  0.664                     | F1-score:  0.664\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:20:49.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 27 | Train Loss:  0.017                     | Train Accuracy:  0.998                     | Val Loss:  0.402                     | Val Accuracy:  0.778                     | Top 3: 0.9797979797979798                     | Precision:  0.718                     | Recall:  0.644                     | F1-score:  0.636\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:22:06.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 28 | Train Loss:  0.016                     | Train Accuracy:  1.000                     | Val Loss:  0.403                     | Val Accuracy:  0.788                     | Top 3: 0.9818181818181818                     | Precision:  0.720                     | Recall:  0.635                     | F1-score:  0.635\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 18:23:24.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 29 | Train Loss:  0.015                     | Train Accuracy:  0.999                     | Val Loss:  0.395                     | Val Accuracy:  0.792                     | Top 3: 0.9858585858585859                     | Precision:  0.722                     | Recall:  0.647                     | F1-score:  0.639\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n",
      "\u001b[32m2024-05-12 18:24:42.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 30 | Train Loss:  0.014                     | Train Accuracy:  0.999                     | Val Loss:  0.392                     | Val Accuracy:  0.796                     | Top 3: 0.9818181818181818                     | Precision:  0.755                     | Recall:  0.616                     | F1-score:  0.639\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.81it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:26:00.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 31 | Train Loss:  0.012                     | Train Accuracy:  1.000                     | Val Loss:  0.405                     | Val Accuracy:  0.792                     | Top 3: 0.9818181818181818                     | Precision:  0.742                     | Recall:  0.640                     | F1-score:  0.656\u001b[0m\n",
      "Training Steps: 100%|██████████| 198/198 [01:10<00:00,  2.80it/s]\n",
      "Validation Steps: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "\u001b[32m2024-05-12 18:27:17.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEpochs: 32 | Train Loss:  0.013                     | Train Accuracy:  0.999                     | Val Loss:  0.413                     | Val Accuracy:  0.790                     | Top 3: 0.9797979797979798                     | Precision:  0.748                     | Recall:  0.645                     | F1-score:  0.659\u001b[0m\n",
      "\u001b[32m2024-05-12 18:27:17.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mEarly stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader, desc=\"Training Steps\"):\n",
    "        # print(train_input)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_label = train_label.to(device)\n",
    "        mask = train_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "        input_id = train_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "        tok_type = train_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "        # repr = train_input[2].to(device)\n",
    "        # print(tok_type.shape, input_id.shape, mask.shape)\n",
    "        # print(repr.dtype, input_id.dtype, mask.dtype)\n",
    "\n",
    "        output = model(input_id, mask, tok_type)\n",
    "\n",
    "        batch_loss = criterion(output, train_label.long())\n",
    "        total_loss_train += batch_loss.item()\n",
    "\n",
    "        output = torch.sum(torch.stack(output), 0)\n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        \n",
    "        total_acc_train += acc\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    correct_top_k = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val_input, val_label in tqdm(val_dataloader, desc=\"Validation Steps\"):\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "            input_id = val_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "            tok_type = val_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "            # repr = val_input[2].to(device)\n",
    "\n",
    "            output = model(input_id, mask, tok_type)\n",
    "\n",
    "            batch_loss = criterion(output, val_label.long())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            output = torch.sum(torch.stack(output), 0)\n",
    "            _, top_k_predictions = output.topk(3, 1, True, True)\n",
    "\n",
    "            top_k_predictions = top_k_predictions.t()\n",
    "\n",
    "            correct_top_k += (\n",
    "                top_k_predictions.eq(\n",
    "                    val_label.view(1, -1).expand_as(top_k_predictions)\n",
    "                )\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "\n",
    "            all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.append(val_label.cpu().numpy())\n",
    "\n",
    "            total_acc_val += acc\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    top10 = correct_top_k / len(df_val)\n",
    "\n",
    "    log_step(\n",
    "        epoch_num,\n",
    "        total_acc_train,\n",
    "        total_acc_val,\n",
    "        total_loss_train,\n",
    "        total_loss_val,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        df_train,\n",
    "        df_val,\n",
    "        top10,\n",
    "    )\n",
    "\n",
    "    val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        patience_counter = 0\n",
    "        logger.success(\"Found new best model. Saving weights...\")\n",
    "        torch.save(model.state_dict(), weights_save_location)\n",
    "        best_loss = val_loss\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            logger.info(\"Early stopping...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0d3590b97b44b994f1d81560c5fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1-score</td><td>▁▂▃▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇████████████</td></tr><tr><td>precision</td><td>▁▂▃▃▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇██▇█▇█▇▇▇███</td></tr><tr><td>recall</td><td>▁▄▄▅▆▇▇▇▇█▇███████▇█▇▇███████▇██</td></tr><tr><td>top3_acc</td><td>▁▄▅▇████████████████████████████</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇▇█████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▅▆▇▇▇▇▇▇▇▇▇█████▇████████████</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1-score</td><td>0.65938</td></tr><tr><td>precision</td><td>0.7484</td></tr><tr><td>recall</td><td>0.64502</td></tr><tr><td>top3_acc</td><td>0.9798</td></tr><tr><td>train_acc</td><td>0.99949</td></tr><tr><td>train_loss</td><td>0.01301</td></tr><tr><td>val_acc</td><td>0.7899</td></tr><tr><td>val_loss</td><td>0.41259</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comp_sp_token_LBTPDeberta_u5_6_classes_CombinedLoss</strong> at: <a href='https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj' target=\"_blank\">https://wandb.ai/afifaniks/openj9/runs/kwrgbjpj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_174509-kwrgbjpj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_load_location = weights_save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load(weights_load_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/disa_lab/projects/triagerx/models/comp_raw_data_LBTPDeberta_u5_6_classes_CombinedLoss.pt'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [00:00<00:00, 63811.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_test[\"description\"] = df_test[\"description\"].progress_apply(clean_issue_description)\n",
    "df_test[\"text\"] = df_test.progress_apply(\n",
    "        lambda x: \"Bug Title: \"\n",
    "        + str(x[\"issue_title\"])\n",
    "        # + \"\\nIssue Labels: \"\n",
    "        # + str(x[\"labels\"])\n",
    "        # + \"\\nIssue Topic: \"\n",
    "        # + str(x[\"topic_label\"])\n",
    "        # + \"\\nBug Summary: \"\n",
    "        # + str(x[\"summary\"]),\n",
    "        + \"\\nBug Description: \"\n",
    "        + str(x[\"description\"]),\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-12 22:02:44.118\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m9\u001b[0m - \u001b[34m\u001b[1mGenerating torch dataset...\u001b[0m\n",
      "\u001b[32m2024-05-12 22:02:44.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mTokenizing texts...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_ds = TriageDataset(df_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(test_ds, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_val = 0\n",
    "total_loss_val = 0\n",
    "correct_top_k = 0\n",
    "correct_top_k_wo_sim = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "device=\"cuda\"\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for val_input, val_label in loader:\n",
    "        val_label = val_label.to(device)\n",
    "        mask = val_input[1][\"attention_mask\"].squeeze(1).to(device)\n",
    "        input_id = val_input[1][\"input_ids\"].squeeze(1).to(device)\n",
    "        tok_type = val_input[1][\"token_type_ids\"].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask, tok_type)\n",
    "\n",
    "        output = torch.sum(torch.stack(output), 0)\n",
    "\n",
    "        #wo similarity\n",
    "        _, top_k_wo_sim = output.topk(1, 1, True, True)\n",
    "\n",
    "        top_k_wo_sim = top_k_wo_sim.t()\n",
    "\n",
    "        correct_top_k_wo_sim += (\n",
    "            top_k_wo_sim.eq(\n",
    "                val_label.view(1, -1).expand_as(top_k_wo_sim)\n",
    "            )\n",
    "            .sum()\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        all_preds.append(output.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.append(val_label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction without Similarity: 469, 0.7588996763754046\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct Prediction without Similarity: {correct_top_k_wo_sim}, {correct_top_k_wo_sim / len(df_test)}\")\n",
    "# print(f\"Correct Prediction with Similarity: {correct_top_k}, {correct_top_k / len(y_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_np = np.concatenate(all_preds)\n",
    "all_labels_np = np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_preds_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76        89\n",
      "           1       0.69      0.53      0.60        58\n",
      "           2       0.50      0.29      0.36         7\n",
      "           3       1.00      0.20      0.33         5\n",
      "           4       0.56      0.67      0.61        99\n",
      "           5       0.84      0.83      0.84       360\n",
      "\n",
      "    accuracy                           0.76       618\n",
      "   macro avg       0.72      0.55      0.58       618\n",
      "weighted avg       0.77      0.76      0.76       618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels_np, all_preds_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m idx2label \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43my_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_df\u001b[38;5;241m.\u001b[39mowner_id\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_df' is not defined"
     ]
    }
   ],
   "source": [
    "idx2label = {\n",
    "    row[\"owner_id\"]: row[\"owner\"]\n",
    "    for _, row in y_df.iterrows()\n",
    "}\n",
    "\n",
    "labels = y_df.owner_id.to_list()\n",
    "labels = sorted(set(labels))\n",
    "labels = [f\"{idx}: {idx2label[idx]}\" for idx in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(owner):\n",
    "    print(\"Training topic distribution\")\n",
    "    print(\"=======================================\")\n",
    "    print(X_df[X_df.owner == owner].topic_label.value_counts())\n",
    "\n",
    "    print(\"\\n\\nTesting topic distribution\")\n",
    "    print(\"=======================================\")\n",
    "    print(y_df[y_df.owner == owner].topic_label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chrome Tab and Window Behavior Issues     947\n",
       "Build failures                            840\n",
       "Chrome stability issues                   487\n",
       "Layout Testing Issues                     400\n",
       "Chrome crash reports                      391\n",
       "Security and SSL issues                   372\n",
       "Input and keyboard issues                 370\n",
       "Webpage rendering regression issues       357\n",
       "Chrome sync issues                        354\n",
       "Shill WiFi configuration                  337\n",
       "iOS File Issues                           321\n",
       "Data Enhancement                          298\n",
       "Touch and Scroll Issues                   273\n",
       "DevTools Crashes                          260\n",
       "GPU rendering issues                      235\n",
       "Memory Leaks in WebCore and Blink         220\n",
       "Performance testing issues in Chromium    197\n",
       "WebRTC audio/video issues                 184\n",
       "Bookmark issues                           174\n",
       "Performance Regression in Blink            13\n",
       "Name: topic_label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.topic_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training topic distribution\n",
      "=======================================\n",
      "Chrome Tab and Window Behavior Issues     10\n",
      "Webpage rendering regression issues        5\n",
      "Memory Leaks in WebCore and Blink          4\n",
      "Chrome stability issues                    4\n",
      "DevTools Crashes                           3\n",
      "Data Enhancement                           3\n",
      "Input and keyboard issues                  3\n",
      "Touch and Scroll Issues                    2\n",
      "Bookmark issues                            1\n",
      "Security and SSL issues                    1\n",
      "Chrome sync issues                         1\n",
      "Layout Testing Issues                      1\n",
      "Build failures                             1\n",
      "iOS File Issues                            1\n",
      "Performance testing issues in Chromium     1\n",
      "Name: topic_label, dtype: int64\n",
      "\n",
      "\n",
      "Testing topic distribution\n",
      "=======================================\n",
      "Chrome Tab and Window Behavior Issues     17\n",
      "Webpage rendering regression issues        7\n",
      "DevTools Crashes                           6\n",
      "iOS File Issues                            6\n",
      "Touch and Scroll Issues                    4\n",
      "Input and keyboard issues                  4\n",
      "Data Enhancement                           4\n",
      "Chrome crash reports                       4\n",
      "Chrome stability issues                    3\n",
      "Memory Leaks in WebCore and Blink          2\n",
      "Chrome sync issues                         2\n",
      "Bookmark issues                            1\n",
      "Performance testing issues in Chromium     1\n",
      "Name: topic_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_topic_distribution(\"a...@chromium.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
