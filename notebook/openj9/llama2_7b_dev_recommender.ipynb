{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 --progress-bar off\n",
    "!pip install -q peft==0.4.0 --progress-bar off\n",
    "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
    "!pip install -q transformers==4.31.0 --progress-bar off\n",
    "!pip install -q trl==0.4.7 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\n",
    "\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# transformers parameters\n",
    "################################################################################\n",
    "\n",
    "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d741f372875442ec8beb8b0c29e9537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"openj9_issues_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files = dataset_path, split = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'issue_number': 10,\n",
       " 'issue_url': 'https://github.com/eclipse-openj9/openj9/pull/10',\n",
       " 'issue_title': 'Update README.md with proper AdoptOpenJDK link',\n",
       " 'issue_body': 'Also remove the the coming soon reference, since the OpenJ9 build is\\r\\nlive now.\\r\\n\\r\\nSigned-off-by: Peter Shipton <Peter_Shipton@ca.ibm.com>',\n",
       " 'issue_state': 'closed',\n",
       " 'creator': 'pshipton',\n",
       " 'comments': None,\n",
       " 'assignees': 'charliegracie'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumber of training prompts: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_dataset)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mColumn names are: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mcolumn_names\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Number of training prompts: {len(train_dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fengxue-IS', 'hzongaro', 'jdmpapin', 'amicic', 'dsouzai', 'tajila', 'gita-omr', '0xdaryl', 'a7ehuo', 'vijaysun-omr', 'llxia', 'JasonFengJ9', 'dmitripivkine', 'knn-k', 'DanHeidinga', 'keithc-ca', 'mpirvu', 'pshipton', 'babsingh', 'fjeremic', 'gacholio', 'ymanton', 'charliegracie', 'ChengJin01', 'joransiu', 'andrewcraik'}\n"
     ]
    }
   ],
   "source": [
    "developers_list = set(dataset[\"assignees\"])\n",
    "print(developers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend a developer to solve the issue from one of the 26 developers:\n",
      "\n",
      "fengxue-IS\n",
      "hzongaro\n",
      "jdmpapin\n",
      "amicic\n",
      "dsouzai\n",
      "tajila\n",
      "gita-omr\n",
      "0xdaryl\n",
      "a7ehuo\n",
      "vijaysun-omr\n",
      "llxia\n",
      "JasonFengJ9\n",
      "dmitripivkine\n",
      "knn-k\n",
      "DanHeidinga\n",
      "keithc-ca\n",
      "mpirvu\n",
      "pshipton\n",
      "babsingh\n",
      "fjeremic\n",
      "gacholio\n",
      "ymanton\n",
      "charliegracie\n",
      "ChengJin01\n",
      "joransiu\n",
      "andrewcraik\n"
     ]
    }
   ],
   "source": [
    "instruction_template = \"Recommend a developer to solve the issue from one of the 26 developers:\\n\\n\" + \"\\n\".join(developers_list)\n",
    "print(instruction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_context(sample):\n",
    "    return f\"Issue Title: {sample['issue_title']}\\n\\nIssue Description: {sample['issue_body']}\"\n",
    "\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{instruction_template}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{_get_input_context(sample)}\"\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['assignees']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_format(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{instruction_template}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{_get_input_context(sample)}\"\n",
    "    # response = f\"{RESPONSE_KEY}\\n{sample['assignees']}\"\n",
    "    # end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9c0b75dc44c75b49cc0f57c5938d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4237c3c7de4438818494f5775b45a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd691f6d47449e3814dcb7b4cf7add7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 33\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4602\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare the model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get LoRA module names\n",
    "    target_modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        train_dataset = dataset,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_steps = warmup_steps,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = fp16,\n",
    "            logging_steps = logging_steps,\n",
    "            output_dir = output_dir,\n",
    "            optim = optim,\n",
    "        ),\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training and log metrics\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 25\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA module names: ['down_proj', 'q_proj', 'o_proj', 'gate_proj', 'v_proj', 'up_proj', 'k_proj']\n",
      "All Parameters: 3,540,389,888 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291682911958425\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.545200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.02\n",
      "  total_flos               =   914526GF\n",
      "  train_loss               =     1.3512\n",
      "  train_runtime            = 0:01:00.31\n",
      "  train_samples_per_second =      1.658\n",
      "  train_steps_per_second   =      0.414\n",
      "{'train_runtime': 60.3147, 'train_samples_per_second': 1.658, 'train_steps_per_second': 0.414, 'total_flos': 981965375373312.0, 'train_loss': 1.3512493681907654, 'epoch': 0.02}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c8c3d87d6f4d85bb001c71bae10ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('results/openj9_llama2_7b/tokenizer_config.json',\n",
       " 'results/openj9_llama2_7b/special_tokens_map.json',\n",
       " 'results/openj9_llama2_7b/tokenizer.model',\n",
       " 'results/openj9_llama2_7b/added_tokens.json',\n",
       " 'results/openj9_llama2_7b/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fine-tuned weights\n",
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = device, torch_dtype = torch.bfloat16)\n",
    "# Merge the LoRA layers with the base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save fine-tuned model at a new location\n",
    "output_merged_dir = \"results/openj9_llama2_7b\"\n",
    "os.makedirs(output_merged_dir, exist_ok = True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization = True)\n",
    "\n",
    "# Save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "stop_list = ['\\nHuman:', '\\n```\\n', \"###\"]\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def inference(index, test_ds):\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, max_length=max_length, tokenizer=tokenizer, stopping_criteria=stopping_criteria)\n",
    "    result = pipe(create_query_format(test_ds[index])[\"text\"])\n",
    "\n",
    "    response_text = result[0][\"generated_text\"]\n",
    "    # pattern = r\"### Response:\\n.*?\\n\"\n",
    "\n",
    "    # response = re.findall(pattern, response_text, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    return {\n",
    "        \"LLM Response\": response_text,\n",
    "        \"Actual\": test_dataset[index][\"assignees\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 189\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: Assertion failure at ClassLoaderRememberedSet.cpp:248 \\n\\nIssue Description: Java -version output\\r\\n--------------------\\r\\n```\\r\\nJRE 1.8.0 Linux amd64-64 (build 8.0.7.5 - pxa6480sr7fp5ifix-20220310_01(SR7 FP5+IJ37785))\\r\\nIBM J9 VM(JRE 1.8.0 Linux amd64-64-Bit Compressed References 20220104_19630 (JIT enabled, AOT enabled)\\r\\nOpenJ9   - 2d4c7d9\\r\\nOMR      - 59845b7\\r\\nIBM      - 3c151c1)\\r\\n```\\r\\nSummary of problem\\r\\n------------------\\r\\nClient had an assert failure due to accessing a Class before it was fully initialized. \\r\\nj9mm.107             Assert     * ** ASSERTION FAILED ** at ClassLoaderRememberedSet.cpp:248: ((false && ((0 != (((((clazz)->classFlags))) & ((0x40)))))))\\r\\n\\r\\n\\r\\nDiagnostic files\\r\\n----------------\\r\\n\\r\\nDiagnostic:\\r\\n-------------\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r', 'Actual': 'gacholio'} \n",
      "\n",
      "\n",
      "Index: 228\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: JDK15 Valhalla ValueTypeTests_0_FAILED - Segmentation error vmState=0x00040000\\n\\nIssue Description: Failure link\\r\\n------------\\r\\n\\r\\nFrom an internal test `Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/111/`:\\r\\n```\\r\\n00:39:06  openjdk version \"16-internal\" 2021-03-16\\r\\n00:39:06  OpenJDK Runtime Environment (build 16-internal+0-adhoc.jenkins.BuildJDKnextx86-64linuxvalhallaNightly)\\r\\n00:39:06  Eclipse OpenJ9 VM (build master-cbf0b230e45, JRE 16 Linux amd64-64-Bit Compressed References 20201020_185 (JIT enabled, AOT enabled)\\r\\n00:39:06  OpenJ9   - cbf0b230e45\\r\\n00:39:06  OMR      - b7e8a5d7804\\r\\n00:39:06  JCL      - e27725787b3 based on jdk-16+20)\\r\\n```\\r\\nTo rebuild the failed tests in <Jenkins URL>=https://hyc-runtimes-jenkins.swg-devops.com/job/Grinder, use the following links:\\r\\n00:41:10  https://hyc-runtimes-jenkins.swg-devops.com/job/Grinder/parambuild/?JDK_VERSION=Valhalla&JDK_IMPL=openj9&BUILD_LIST=functional/Valhalla,functional/cmdline_options_tester,functional/cmdLineTests/valuetypeddrtests&PLATFORM=x86-64_linux&TARGET=ValueTypeTests_0\\r\\n\\r\\nOptional info\\r\\n-------------\\r\\n\\r\\n\\r\\nFailure output (captured from console output)\\r\\n---------------------------------------------\\r\\n```\\r\\n===============================================\\r\\nRunning test ValueTypeTests_0 ...\\r\\n===============================================\\r\\nValueTypeTests_0 Start Time: Wed Oct 21 00:40:32 2020 Epoch Time (ms): 1603255232723\\r\\nvariation: NoOptions\\r\\nJVM_OPTIONS:  \\r\\nJVMJ9VM193W Since Java 13 -Xverify:none and -noverify were deprecated for removal and may not be accepted options in the future.\\r\\n[IncludeExcludeTestAnnotationTransformer] [INFO] exclude file is /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/../TestConfig/resources/excludes/latest_exclude_Valhalla.txt\\r\\n[IncludeExcludeTestAnnotationTransformer] [INFO] Unable to open file /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/../TestConfig/resources/excludes/latest_exclude_Valhalla.txt, changed to read /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/../TestConfig/resources/excludes/latest_exclude_base.txt\\r\\n...\\r\\n... TestNG 6.14.2 by Cédric Beust (cedric@beust.com)\\r\\n...\\r\\n\\r\\nUnhandled exception\\r\\nType=Segmentation error vmState=0x00040000\\r\\nJ9Generic_Signal_Number=00000018 Signal_Number=0000000b Error_Value=00000000 Signal_Code=00000001\\r\\nHandler1=00007F0DEFAFDD20 Handler2=00007F0DEF3CCFD0 InaccessibleAddress=0000000000000008\\r\\nRDI=0000000000154E00 RSI=0000000000304F00 RAX=0000000000154E00 RBX=0000000000000000\\r\\nRCX=0000000000304F00 RDX=0000000000000002 R8=00000000020E0001 R9=0000000000000001\\r\\nR10=0000000000000000 R11=0000000000000000 R12=0000000000154E00 R13=00000000FFE4B200\\r\\nR14=0000000000304F00 R15=00000000000DE740\\r\\nRIP=00007F0DEFB3BCD6 GS=0000 FS=0000 RSP=00007F0DF4AE0B28\\r\\nEFlags=0000000000010202 CS=0033 RBP=00007F0DF4AE10A0 ERR=0000000000000004\\r\\nTRAPNO=000000000000000E OLDMASK=0000000000000000 CR2=0000000000000008\\r\\nxmm0 0000000000000000 (f: 0.000000, d: 0.000000e+00)\\r\\nxmm1 0000000000000000 (f: 0.000000, d: 0.000000e+00)\\r\\nxmm2 00007f0df4ae1190 (f: 4105048576.000000, d: 6.901994e-310)\\r\\nxmm3 00007f0df4ae1190 (f: 4105048576.000000, d: 6.901994e-310)\\r\\nxmm4 00007f0df4ae1190 (f: 4105048576.000000, d: 6.901994e-310)\\r\\nxmm5 0000000000000001 (f: 1.000000, d: 4.940656e-324)\\r\\nxmm6 00007f0df4ae1190 (f: 4105048576.000000, d: 6.901994e-310)\\r\\nxmm7 00000000ffe48138 (f: 4293165312.000000, d: 2.121106e-314)\\r\\nxmm8 000000003ed9d314 (f: 1054462720.000000, d: 5.209738e-315)\\r\\nxmm9 000000004a1e3868 (f: 1243494528.000000, d: 6.143679e-315)\\r\\nxmm10 000000003f800000 (f: 1065353216.000000, d: 5.263544e-315)\\r\\nxmm11 40fab162085afff3 (f: 140181488.000000, d: 1.093341e+05)\\r\\nxmm12 000000003e94e2b0 (f: 1049944768.000000, d: 5.187416e-315)\\r\\nxmm13 000000004636d18d (f: 1177997696.000000, d: 5.820082e-315)\\r\\nxmm14 000000003f7ffffa (f: 1065353216.000000, d: 5.263544e-315)\\r\\nxmm15 0000000046a05000 (f: 1184911360.000000, d: 5.854240e-315)\\r\\nModule=/home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdkbinary/j2sdk-image/lib/compressedrefs/libj9vm29.so\\r\\nModule_base_address=00007F0DEFA52000\\r\\nTarget=2_90_20201020_185 (Linux 4.4.0-171-generic)\\r\\nCPU=amd64 (4 logical CPUs) (0x1ee3f6000 RAM)\\r\\n----------- Stack Backtrace -----------\\r\\n(0x00007F0DEFB3BCD6 [libj9vm29.so+0xe9cd6])\\r\\n(0x00007F0DEFAD07E5 [libj9vm29.so+0x7e7e5])\\r\\n(0x00007F0DEFAD12D4 [libj9vm29.so+0x7f2d4])\\r\\n(0x00007F0DEFAD2A4B [libj9vm29.so+0x80a4b])\\r\\n(0x00007F0DEFA668BE [libj9vm29.so+0x148be])\\r\\n(0x00007F0DEFA64E10 [libj9vm29.so+0x12e10])\\r\\n(0x00007F0DEFB3A4E2 [libj9vm29.so+0xe84e2])\\r\\n---------------------------------------\\r\\nJVMDUMP039I Processing dump event \"gpf\", detail \"\" at 2020/10/21 00:40:34 - please wait.\\r\\nJVMDUMP032I JVM requested System dump using \\'/home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/core.20201021.004034.5238.0001.dmp\\' in response to an event\\r\\nJVMDUMP010I System dump written to /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/core.20201021.004034.5238.0001.dmp\\r\\nJVMDUMP032I JVM requested Java dump using \\'/home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/javacore.20201021.004034.5238.0002.txt\\' in response to an event\\r\\nJVMDUMP012E Error in Java dump: /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/javacore.20201021.004034.5238.0002.txt\\r\\nJVMDUMP032I JVM requested Snap dump using \\'/home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/Snap.20201021.004034.5238.0003.trc\\' in response to an event\\r\\nJVMDUMP010I Snap dump written to /home/jenkins/workspace/Test_openjdkValhalla_j9_sanity.functional_x86-64_linux_valhalla_Nightly_testList_0/openjdk-tests/TKG/test_output_16032552324687/ValueTypeTests_0/Snap.20201021.004034.5238.0003.trc\\r\\nJVMDUMP013I Processed dump event \"gpf\", detail \"\".\\r\\n\\r\\nValueTypeTests_0_FAILED\\r\\n```\\r\\n\\r\\n### End\\r\\n', 'Actual': 'tajila'} \n",
      "\n",
      "\n",
      "Index: 139\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: (0.22.0) Allow building of aarch64 for all current releases\\n\\nIssue Description: Replay of https://github.com/eclipse/openj9/pull/10614 for the 0.22 release.\\n\\nSigned-off-by: Cheng Jin <cheng.jin@ibm.com>\\n\\n### Response:\\nmpirvu\\n\\n### End of Instruction\\n\\n### End of Response\\n\\n### Start of Input:\\n\\nIssue Description: Replay of https://github.com/eclipse/openj9/pull/10614 for the 0.22 release.\\n\\nSigned-off-by: Cheng Jin <cheng.jin@ibm.com>\\n\\n### End of Input\\n\\n### Start of Response:\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n### End of Input\\n\\n### End of Response\\n\\n### Start of Input:\\n\\n##', 'Actual': 'keithc-ca'} \n",
      "\n",
      "\n",
      "Index: 22\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: Misc. interface dispatch fixes\\n\\nIssue Description: * Enable GC maps for virtual/interface snippets\\r\\n* Branch to interface snippet should not have register dependencies\\r\\n* Complete interface snippet printing\\r\\n* Decrement vft reference count on interface dispatch\\r\\n* Reverse order of argument spills for interface dispatches  \\r\\n\\r\\n### Response:\\n0xdaryl\\n\\n### End of Instruction\\n\\n### Start of Response:\\nmpirvu\\n\\n### End of Response\\n\\n### Start of Input:\\n\\nIssue Description: * Enable GC maps for virtual/interface snippets\\r\\n\\r\\nIssue Description: * Branch to interface snippet should not have register dependencies\\r\\n\\r\\nIssue Description: * Complete interface snippet printing\\r\\n\\r\\nIssue Description: * Decrement vft reference count on interface dispatch\\r\\n\\r\\nIssue Description: * Reverse order of argument spills for interface dispatches  \\r\\n\\n### End of Input\\n\\n### Start of Signature Block:\\nmpirvu\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n### Start of Signature Block:\\n\\n### End of Signature Block\\n\\n### Start of Comment Block:\\n\\n### End of Comment Block\\n\\n###', 'Actual': 'knn-k'} \n",
      "\n",
      "\n",
      "Index: 151\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: JITServer compilation fixes\\n\\nIssue Description: After a recent rebase on the master and the introduction of the `JITSERVER_SUPPORT` flag, JITServer was not compiling on certain platforms. This PR addresses those problems.\\r\\n\\r\\nSigned-off-by: Dhruv Chopra <Dhruv.C.Chopra@ibm.com>\\n\\n### Response:\\nfjeremic\\n\\n### End of Instruction\\n\\n### Enter your response here:\\n\\n### End of Response\\n\\n\\n\\n### Begin of Test Cases\\n\\n### Test Case 1:\\n\\n### Input:\\n\\n### End of Input\\n\\n### Output:\\n\\nDanHeidinga\\n\\n### End of Test Case 1\\n\\n### Test Case 2:\\n\\n### Input:\\n\\nIssue Description: After a recent rebase on the master and the introduction of the `JITSERVER_SUPPORT` flag, JITServer was not compiling on certain platforms. This PR addresses those problems.\\r\\n\\nSigned-off-by: Dhruv Chopra <Dhruv.C.Chopra@ibm.com>\\n\\n### End of Input\\n\\n### Output:\\n\\nDanHeidinga\\n\\n### End of Test Case 2\\n\\n### End of Test Cases\\n\\n### End of Instruction\\n\\n### Enter your response here:\\n\\n### End of Response\\n\\n\\n\\n### Begin of Help\\n\\n### HELP:\\nSend help to respond to this issue.\\n\\n### End of Help\\n\\n### End of Issue Response\\n', 'Actual': 'mpirvu'} \n",
      "\n",
      "\n",
      "Index: 108\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: AArch64: Stop using x27 and x28 in PicBuilder.spp\\n\\nIssue Description: This commit changes the registers used in PicBuilder.spp for AArch64.\\r\\nIt used to use x27 and x28 for temporary purpose.  They are preserved\\r\\nregisters in the PrivateLinkage, however, and they potentially hold\\r\\nvalues that are alive across method calls.  PicBuilder routines must\\r\\nnot destroy them.\\r\\n\\r\\nSigned-off-by: KONNO Kazuhiro <konno@jp.ibm.com>\\n\\n### Response:\\n0xdaryl\\n\\n### End of Instruction\\n\\n### Enter your response here:\\n0xdaryl\\n\\n### End of Response\\n\\n\\n', 'Actual': '0xdaryl'} \n",
      "\n",
      "\n",
      "Index: 8\n",
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: Investigation of pairing of internalEnterVMFromJNI/internalReleaseVMAccess\\n\\nIssue Description: There are quite a few places with this pairing:\\r\\n\\r\\n```\\r\\nthrowexception.c:throwNativeOOMError()\\r\\njniinv.c:protectedInternalAttachCurrentThread() Note: there is no code between.\\r\\nvm_scar.c:J9VMDllMain()\\r\\n```\\r\\n\\r\\nWill let these code intact until further investigation. \\r\\n\\r\\nSigned-off-by: Xiaoming Zhang <xiaoming.zhang@ibm.com>\\n\\n### Response:\\nmpirvu\\n\\n### End of Instruction\\n\\n### End of Response\\n\\n### Enter your name and press enter 3 times:\\nmpirvu\\n\\n### End of Chat', 'Actual': 'gacholio'} \n",
      "\n",
      "\n",
      "Index: 7\n",
      "{'LLM Response': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: Allow Reference.refersTo to be inlined under AOT\\n\\nIssue Description: The implementations of `TR_J9SharedCacheVM::isGetImplInliningSupported` and `TR_J9SharedCacheVM::isGetImplAndRefersToInliningSupported` return false unconditionally, preventing inlining of `java/lang/ref/Reference.refersTo` and `java/lang/ref/Reference.getImpl` under AOT compilation.  This change removes the implementations of those methods in `TR_J9SharedCacheVM` to allow inlining to happen under AOT, with an additional test in `TR_J9VM::inlineNativeCall` of whether the 'referent' field's offset could not be retrieved - which can occur under AOT with no symbol validation manager.\\n\\nSigned-off-by: Andrew Craik <andrew.craik@ibm.com>\\n\\n### Response:\\nmpirvu\\n\\n### End of Instruction\\n\\n### End of Response\\n\\n### Start of Input:\\n\\nIssue Description: The implementations of `TR_J9SharedCacheVM::isGetImplInliningSupported` and `TR_J9SharedCacheVM::isGetImplAndRefersToInliningSupported` return false unconditionally, preventing inlining of `java/lang/ref/Reference.refersTo` and `java/lang/ref/Reference.getImpl` under AOT compilation.  This change removes the implementations of those methods in `TR_J9SharedCacheVM` to allow inlining to happen under AOT, with an additional test in `TR_J9VM::inlineNativeCall` of whether the'referent' field's offset could not be retrieved - which can occur under AOT with no symbol validation manager.\\n\\nSigned-off-by: Andrew Craik <andrew.craik@ibm.com>\\n\\n### End of Input\\n\\n### Start of Signed-off-by\\n\\nSigned-off-by: Andrew Craik <andrew.craik@ibm.com>\\n\\n### End of Signed-off-by\\n\", 'Actual': 'dsouzai'} \n",
      "\n",
      "\n",
      "Index: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 13041, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM Response': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRecommend a developer to solve the issue from one of the 26 developers:\\n\\njoransiu\\nDanHeidinga\\ndmitripivkine\\nllxia\\nvijaysun-omr\\nkeithc-ca\\nJasonFengJ9\\nandrewcraik\\njdmpapin\\ngita-omr\\ngacholio\\ndsouzai\\na7ehuo\\namicic\\npshipton\\nknn-k\\nbabsingh\\nfengxue-IS\\nChengJin01\\nhzongaro\\ncharliegracie\\n0xdaryl\\nymanton\\nmpirvu\\ntajila\\nfjeremic\\n\\nInput:\\nIssue Title: CMake: Add zos support to libffi\\n\\nIssue Description: Signed-off-by: Devin Nakamura <devinn@ca.ibm.com>\\n\\n### Response:\\nmpirvu\\n\\n### End of Input:\\n\\n### End of Response:\\n\\n### Enter another response:\\n\\n### End of input\\n\\n### Enter your name and press enter 3 times:\\nmpirvu\\n\\n### Exiting\\n### Total responses: 1\\n### Response times: 2\\n### FPS: 0.5\\n### Throughput: 50.00 RPS\\n### CPU: 100.00%\\n### Memory: 100.00%\\n### Elapsed: 00:00:00.53\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory: 100.00MB\\n### Process finished with exit code 0\\n### Final Memory:', 'Actual': 'DanHeidinga'} \n",
      "\n",
      "\n",
      "Index: 55\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.28 GiB (GPU 0; 79.15 GiB total capacity; 48.48 GiB already allocated; 14.45 GiB free; 59.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data_index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(test_dataset) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex: \u001b[39m\u001b[39m{\u001b[39;00mdata_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m out \u001b[39m=\u001b[39m inference(data_index, test_dataset)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(out, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb Cell 32\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference\u001b[39m(index, test_ds):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     pipe \u001b[39m=\u001b[39m pipeline(task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, max_length\u001b[39m=\u001b[39mmax_length, tokenizer\u001b[39m=\u001b[39mtokenizer, stopping_criteria\u001b[39m=\u001b[39mstopping_criteria)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     result \u001b[39m=\u001b[39m pipe(create_query_format(test_ds[index])[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     response_text \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# pattern = r\"### Response:\\n.*?\\n\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22415243227d/home/mdafifal.mamun/notebooks/triagerX/notebook/llama2_7b_dev_recommender.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# response = re.findall(pattern, response_text, re.DOTALL | re.MULTILINE | re.IGNORECASE)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:200\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    160\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/pipelines/base.py:1122\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1115\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         )\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/pipelines/base.py:1129\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1128\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1129\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1130\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1131\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/pipelines/base.py:1028\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1027\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1028\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1029\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1030\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:261\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    260\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    262\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1589\u001b[0m         input_ids,\n\u001b[1;32m   1590\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1591\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1592\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1593\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1594\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1595\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1596\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1597\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1598\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1599\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1600\u001b[0m     )\n\u001b[1;32m   1602\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/generation/utils.py:2642\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2641\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2642\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2643\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2644\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2645\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2646\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2647\u001b[0m )\n\u001b[1;32m   2649\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2650\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    807\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    808\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    809\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    812\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    815\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    409\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    410\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    411\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    412\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    413\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:346\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    343\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    345\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    347\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m~/miniconda3/envs/triagerx/lib/python3.10/site-packages/torch/nn/functional.py:1845\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1844\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1846\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.28 GiB (GPU 0; 79.15 GiB total capacity; 48.48 GiB already allocated; 14.45 GiB free; 59.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range (10):\n",
    "    data_index = random.randint(0, len(test_dataset) - 1)\n",
    "    print(f\"Index: {data_index}\")\n",
    "    out = inference(data_index, test_dataset)\n",
    "    print(out, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a developer to solve the issue from one of the 26 developers:\n",
      "\n",
      "fengxue-IS\n",
      "hzongaro\n",
      "jdmpapin\n",
      "amicic\n",
      "dsouzai\n",
      "tajila\n",
      "gita-omr\n",
      "0xdaryl\n",
      "a7ehuo\n",
      "vijaysun-omr\n",
      "llxia\n",
      "JasonFengJ9\n",
      "dmitripivkine\n",
      "knn-k\n",
      "DanHeidinga\n",
      "keithc-ca\n",
      "mpirvu\n",
      "pshipton\n",
      "babsingh\n",
      "fjeremic\n",
      "gacholio\n",
      "ymanton\n",
      "charliegracie\n",
      "ChengJin01\n",
      "joransiu\n",
      "andrewcraik\n",
      "\n",
      "Input:\n",
      "Issue Title: Fix casting issues in generateRILInstruction() API\n",
      "\n",
      "Issue Description: Splitting the generateRILInstruction() API to handle the different\n",
      "types of immediates that RIL instructions can take separately.\n",
      "One kind is a pure immediate which is used for say an ADD\n",
      "instruction. The other is when the immediate is used as a relative\n",
      "offset for address calculation like LARL or BRASL instructions.\n",
      "This allows calls to the API to have more sensible casting.\n",
      "They are modified for this change and unsafe casting is switched\n",
      "to static casting and some code has its style improved.\n",
      "\n",
      "Signed-off-by: Simon Hirst <shirst@ca.ibm.com>\n",
      "\n",
      "### Response:\n",
      "fjeremic\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt_formats(dataset[22])[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 18 00:14:05 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              53W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
