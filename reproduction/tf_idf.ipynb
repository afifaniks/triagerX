{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mdafifal.mamun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mdafifal.mamun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues after processing: 14877\n",
      "Samples per block: 1487, Selected block: 9\n",
      "Training data: 9925, Validation data: 1118\n",
      "Number of developers in train: 40\n",
      "Number of developers in test: 22\n",
      "Train dataset size: 9925\n",
      "Test dataset size: 1118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have nltk resources downloaded (if not, run these lines once)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "df = pd.read_csv('/home/mdafifal.mamun/notebooks/triagerX/data/typescript/ts_bug_data.csv')\n",
    "\n",
    "# Combine issue_title and issue_body into a single feature\n",
    "df['text'] = df['issue_title'] + ' ' + df['issue_body']\n",
    "\n",
    "data = df[df[\"owner\"].notna()]\n",
    "\n",
    "df = df[df[\"owner\"].notna()]\n",
    "\n",
    "num_issues = len(df)\n",
    "print(f\"Total number of issues after processing: {num_issues}\")\n",
    "\n",
    "num_cv = 10\n",
    "block = 9\n",
    "\n",
    "samples_per_block = len(df) // num_cv\n",
    "sliced_df = df[: samples_per_block * (block + 1)]\n",
    "\n",
    "print(f\"Samples per block: {samples_per_block}, Selected block: {block}\")\n",
    "\n",
    "# Train and Validation preparation\n",
    "\n",
    "df_train = sliced_df[: samples_per_block * block]\n",
    "df_test = sliced_df[samples_per_block * block : samples_per_block * (block + 1)]\n",
    "\n",
    "sample_threshold = 20\n",
    "developers = df_train[\"owner\"].value_counts()\n",
    "filtered_developers = developers.index[developers >= sample_threshold]\n",
    "df_train = df_train[df_train[\"owner\"].isin(filtered_developers)]\n",
    "\n",
    "train_owners = set(df_train[\"owner\"])\n",
    "test_owners = set(df_test[\"owner\"])\n",
    "\n",
    "unwanted = list(test_owners - train_owners)\n",
    "\n",
    "df_test = df_test[~df_test[\"owner\"].isin(unwanted)]\n",
    "\n",
    "print(f\"Training data: {len(df_train)}, Validation data: {len(df_test)}\")\n",
    "print(f\"Number of developers in train: {len(df_train.owner.unique())}\")\n",
    "print(f\"Number of developers in test: {len(df_test.owner.unique())}\")\n",
    "\n",
    "print(f\"Train dataset size: {len(df_train)}\")\n",
    "print(f\"Test dataset size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.concat([df_train, df_test], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11043/11043 [00:09<00:00, 1220.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = str(text).lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization (split into words)\n",
    "    tokens = text.split()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join the tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "data['cleaned_text'] = data['text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Define the input features (X) and the target labels (y)\n",
    "X = data['cleaned_text']\n",
    "y = data['owner']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, and transform the test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.3601809954751131\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       a-tarasyuk       0.31      0.29      0.30        34\n",
      "       ahejlsberg       0.43      0.77      0.55       135\n",
      "           ajafff       0.00      0.00      0.00         3\n",
      "          amcasey       0.00      0.00      0.00        17\n",
      "         andarist       0.44      0.21      0.29        19\n",
      "     andrewbranch       0.36      0.29      0.32        62\n",
      "           aozgaa       0.00      0.00      0.00         6\n",
      "       armanio123       0.00      0.00      0.00         6\n",
      "          basarat       0.00      0.00      0.00         1\n",
      "           billti       0.00      0.00      0.00         3\n",
      "danielrosenwasser       0.27      0.19      0.22        78\n",
      "   dragomirtitian       0.00      0.00      0.00         5\n",
      "      elibarzilay       0.00      0.00      0.00        12\n",
      "         gabritto       0.00      0.00      0.00         8\n",
      "  graphemecluster       0.00      0.00      0.00         4\n",
      "         iisaduan       0.00      0.00      0.00         5\n",
      "       jakebailey       0.00      0.00      0.00        16\n",
      "     jessetrinity       0.00      0.00      0.00         7\n",
      "  joshuakgoldberg       0.00      0.00      0.00         5\n",
      "      jsonfreeman       0.00      0.00      0.00         5\n",
      "   kiaragrouwstra       0.00      0.00      0.00         3\n",
      "           kingwl       0.00      0.00      0.00        14\n",
      "          mhegazy       0.33      0.28      0.30        68\n",
      "       minestarks       0.00      0.00      0.00         5\n",
      "            mjbvz       0.00      0.00      0.00        15\n",
      "      navya9singh       0.00      0.00      0.00         9\n",
      "             orta       0.00      0.00      0.00        20\n",
      "     paulvanbrenk       0.42      0.33      0.37        15\n",
      "         rbuckton       0.44      0.47      0.46        85\n",
      "    ryancavanaugh       0.64      0.17      0.27        40\n",
      "         sandersn       0.26      0.55      0.35       110\n",
      "        saschanaz       0.00      0.00      0.00         7\n",
      "     sheetalkamat       0.44      0.58      0.50       100\n",
      "   uniqueiniquity       0.00      0.00      0.00         9\n",
      "          vladima       0.00      0.00      0.00        22\n",
      "        weswigham       0.32      0.51      0.39       111\n",
      "             yuit       0.00      0.00      0.00        17\n",
      "         zhengbli       1.00      0.06      0.11        17\n",
      "            zzzen       0.00      0.00      0.00         7\n",
      "\n",
      "         accuracy                           0.36      1105\n",
      "        macro avg       0.15      0.12      0.11      1105\n",
      "     weighted avg       0.31      0.36      0.31      1105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mdafifal.mamun/miniconda3/envs/triagerx/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM classifier\n",
    "classifier = SVC(probability=True)  # Set probability=True to enable probability estimates\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Overall Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores = classifier.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.3584\n",
      "Top-3 Accuracy: 0.5819\n",
      "Top-5 Accuracy: 0.6679\n",
      "Top-10 Accuracy: 0.8072\n",
      "Top-20 Accuracy: 0.9348\n"
     ]
    }
   ],
   "source": [
    "def top_n_accuracy_report(decision_scores, y_true, classifier, top_ns=[1, 3, 5, 10, 20]):\n",
    "    accuracies = {}\n",
    "    for n in top_ns:\n",
    "        # Get the top n predicted classes for each sample\n",
    "        top_n_indices = np.argsort(-decision_scores, axis=1)[:, :n]\n",
    "        \n",
    "        # Map indices to original class labels\n",
    "        top_n_labels = [[classifier.classes_[index] for index in indices] for indices in top_n_indices]\n",
    "\n",
    "        # Calculate accuracy based on whether true labels are in top n predictions\n",
    "        accuracy = np.mean([y_true.iloc[i] in top_n_labels[i] for i in range(len(y_true))])\n",
    "        accuracies[n] = accuracy\n",
    "    return accuracies\n",
    "\n",
    "# Calculate and display top 1, 3, 5, 10, 20 accuracies\n",
    "top_n_accuracies = top_n_accuracy_report(decision_scores, y_test, classifier)\n",
    "for n, acc in top_n_accuracies.items():\n",
    "    print(f\"Top-{n} Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
