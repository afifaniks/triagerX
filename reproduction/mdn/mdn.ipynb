{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from triagerx.dataset.text_processor import TextProcessor\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = str(text)  # In case, there is nan or something else\n",
    "    cleaned_text = text.strip()\n",
    "\n",
    "    cleaned_text = re.sub(r\"(https?|ftp):\\/\\/[^\\s/$.?#].[^\\s]*\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"0x[\\da-fA-F]+\", \"<hex>\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\b[0-9a-fA-F]{16}\\b\", \"<hex>\", cleaned_text)\n",
    "    cleaned_text = re.sub(\n",
    "        r\"\\b\\d{2}:\\d{2}:\\d{2}:\\d{4,} GMT\\b\",\n",
    "        \"\",\n",
    "        cleaned_text,\n",
    "    )\n",
    "    cleaned_text = re.sub(\n",
    "        r\"\\b\\d{2}:\\d{2}:\\d{2}(\\.\\d{2,3})?\\b\",\n",
    "        \"\",\n",
    "        cleaned_text,\n",
    "    )\n",
    "    cleaned_text = re.sub(\n",
    "        r\"\\b\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d+Z\\b\",\n",
    "        \"\",\n",
    "        cleaned_text,\n",
    "    )\n",
    "    cleaned_text = re.sub(r\"```\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"-{3,}\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"[\\*#=+\\-]{3,}\", \"\", cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r\"(\\r?\\n)+\", \"\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"(?![\\r\\n])\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/mdafifal.mamun/notebooks/triagerX/data/openj9/openj9_08122024.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={\"assignees\": \"owner\", \"issue_body\": \"description\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_number</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_state</th>\n",
       "      <th>creator</th>\n",
       "      <th>labels</th>\n",
       "      <th>owner</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Build instructions link in the README.md point...</td>\n",
       "      <td>The `Build instructions` link in the `README.m...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issues/2</td>\n",
       "      <td>closed</td>\n",
       "      <td>aarongraham9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gireeshpunathil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>FAQ link in the README is broken</td>\n",
       "      <td>FAQ link in the README leads to: http://www.ec...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issues/3</td>\n",
       "      <td>closed</td>\n",
       "      <td>dorrab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mpirvu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Link to DockerFile on build instruction page i...</td>\n",
       "      <td>Link for DockerFile on [build instruction page...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issues/5</td>\n",
       "      <td>closed</td>\n",
       "      <td>r30shah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r30shah</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>HOWTO Request: Managing changes across depende...</td>\n",
       "      <td>Like all projects, OpenJ9 builds on the should...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>open</td>\n",
       "      <td>mgaudet</td>\n",
       "      <td>question</td>\n",
       "      <td>hzongaro</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>Compilation Output is too Verbose</td>\n",
       "      <td>The output when compiling the OpenJ9 source co...</td>\n",
       "      <td>https://github.com/eclipse-openj9/openj9/issue...</td>\n",
       "      <td>closed</td>\n",
       "      <td>rservant</td>\n",
       "      <td>enhancement, comp:build</td>\n",
       "      <td>hzongaro</td>\n",
       "      <td>comp:build</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   issue_number                                        issue_title  \\\n",
       "0             2  Build instructions link in the README.md point...   \n",
       "1             3                  FAQ link in the README is broken    \n",
       "2             5  Link to DockerFile on build instruction page i...   \n",
       "3            11  HOWTO Request: Managing changes across depende...   \n",
       "4            13                  Compilation Output is too Verbose   \n",
       "\n",
       "                                         description  \\\n",
       "0  The `Build instructions` link in the `README.m...   \n",
       "1  FAQ link in the README leads to: http://www.ec...   \n",
       "2  Link for DockerFile on [build instruction page...   \n",
       "3  Like all projects, OpenJ9 builds on the should...   \n",
       "4  The output when compiling the OpenJ9 source co...   \n",
       "\n",
       "                                           issue_url issue_state  \\\n",
       "0  https://github.com/eclipse-openj9/openj9/issues/2      closed   \n",
       "1  https://github.com/eclipse-openj9/openj9/issues/3      closed   \n",
       "2  https://github.com/eclipse-openj9/openj9/issues/5      closed   \n",
       "3  https://github.com/eclipse-openj9/openj9/issue...        open   \n",
       "4  https://github.com/eclipse-openj9/openj9/issue...      closed   \n",
       "\n",
       "        creator                   labels            owner   component  \n",
       "0  aarongraham9                      NaN  gireeshpunathil         NaN  \n",
       "1        dorrab                      NaN           mpirvu         NaN  \n",
       "2       r30shah                      NaN          r30shah         NaN  \n",
       "3       mgaudet                 question         hzongaro         NaN  \n",
       "4      rservant  enhancement, comp:build         hzongaro  comp:build  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues after processing: 4595\n",
      "Training data: 3417, Validation data: 386\n",
      "Number of developers: 50\n",
      "Train dataset size: 3417\n",
      "Test dataset size: 386\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"] = df.apply(lambda row: f\"{row['issue_title']}\\n{row['description']}\", axis=1)\n",
    "\n",
    "df = df.sort_values(by=\"issue_number\")\n",
    "df = df[df[\"owner\"].notna()]\n",
    "test_size = 0.1\n",
    "\n",
    "num_issues = len(df)\n",
    "print(f\"Total number of issues after processing: {num_issues}\")\n",
    "\n",
    "df = df.sort_values(by=\"issue_number\")\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=test_size, shuffle=False)\n",
    "\n",
    "sample_threshold = 20\n",
    "developers = df_train[\"owner\"].value_counts()\n",
    "filtered_developers = developers.index[developers >= sample_threshold]\n",
    "df_train = df_train[df_train[\"owner\"].isin(filtered_developers)]\n",
    "\n",
    "train_owners = set(df_train[\"owner\"])\n",
    "test_owners = set(df_test[\"owner\"])\n",
    "\n",
    "unwanted = list(test_owners - train_owners)\n",
    "\n",
    "df_test = df_test[~df_test[\"owner\"].isin(unwanted)]\n",
    "\n",
    "print(f\"Training data: {len(df_train)}, Validation data: {len(df_test)}\")\n",
    "print(f\"Number of developers: {len(df_train.owner.unique())}\")\n",
    "\n",
    "print(f\"Train dataset size: {len(df_train)}\")\n",
    "print(f\"Test dataset size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_smoothed_unigram_models(term_frequencies, mu=0.1):\n",
    "    \"\"\" Compute smoothed unigram probabilities for all reports \"\"\"\n",
    "    K = len(term_frequencies)  # Total number of reports\n",
    "    all_terms = set(term for tf in term_frequencies for term in tf)\n",
    "    \n",
    "    # Compute global term frequencies across all reports\n",
    "    global_term_frequencies = Counter()\n",
    "    for tf in term_frequencies:\n",
    "        global_term_frequencies.update(tf)\n",
    "\n",
    "    smoothed_models = []\n",
    "    for k, tf_k in enumerate(term_frequencies):\n",
    "        total_words_k = sum(tf_k.values())\n",
    "\n",
    "        smoothed_model = {}\n",
    "        for term in all_terms:\n",
    "            tf_term_k = tf_k.get(term, 0)\n",
    "            global_tf_term = global_term_frequencies[term]\n",
    "\n",
    "            term_prob = (1 - mu) * (tf_term_k / total_words_k if total_words_k > 0 else 0) + \\\n",
    "                        mu * (global_tf_term / sum(global_term_frequencies.values()))\n",
    "            \n",
    "            smoothed_model[term] = term_prob\n",
    "\n",
    "        smoothed_models.append(smoothed_model)\n",
    "\n",
    "    return smoothed_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_frequencies(reports):\n",
    "    \"\"\" Compute term frequency for each report \"\"\"\n",
    "    term_frequencies = []\n",
    "    for report in reports:\n",
    "        term_frequencies.append(Counter(report))\n",
    "    return term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bug_reports = [preprocess_text(text) for text in df_train.text.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_counts(reports):\n",
    "    \"\"\"Creates word count dictionaries from tokenized reports.\"\"\"\n",
    "    all_reports_counts = []\n",
    "    for report in reports:\n",
    "        counts = Counter(report)\n",
    "        all_reports_counts.append(counts)\n",
    "    return all_reports_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_counts = create_report_counts(all_bug_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set([word for report in all_bug_reports for word in report])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def precompute_collection_probabilities(all_reports_counts, vocabulary):\n",
    "    \"\"\"Precomputes and stores collection-wide word probabilities.\"\"\"\n",
    "    collection_word_counts = {}\n",
    "    for report_counts in all_reports_counts:\n",
    "        for word, count in report_counts.items():\n",
    "            collection_word_counts[word] = collection_word_counts.get(word, 0) + count\n",
    "\n",
    "    total_words_in_collection = sum(collection_word_counts.values())\n",
    "\n",
    "    collection_probabilities = {}\n",
    "    for word, count in collection_word_counts.items():\n",
    "      collection_probabilities[word] = count / total_words_in_collection if total_words_in_collection > 0 else 0\n",
    "\n",
    "    return collection_probabilities\n",
    "\n",
    "def smoothed_unigram(word, report_k_counts, collection_probabilities, vocabulary, mu):\n",
    "    \"\"\"Calculates smoothed unigram probability using precomputed values.\"\"\"\n",
    "\n",
    "    report_k_total_words = sum(report_k_counts.values())\n",
    "    report_k_prob = report_k_counts.get(word, 0) / report_k_total_words if report_k_total_words > 0 else 0\n",
    "\n",
    "    collection_prob = collection_probabilities.get(word, 0)  # Retrieve precomputed value\n",
    "\n",
    "    return (1 - mu) * report_k_prob + mu * collection_prob\n",
    "\n",
    "\n",
    "def kl_divergence(report_q_probs, report_k_probs):\n",
    "    \"\"\"Calculates the KL-divergence between two probability distributions.\"\"\"\n",
    "    kl = 0.0\n",
    "    for word, prob_q in report_q_probs.items():\n",
    "        prob_k = report_k_probs.get(word, 0)  # Handle words not in report_k\n",
    "        if prob_q > 0 and prob_k > 0:  # Avoid log(0) errors and division by 0\n",
    "          kl += prob_q * np.log(prob_q / prob_k)\n",
    "        elif prob_q > 0: # only add if prob_q is not 0\n",
    "          kl += prob_q * np.log(prob_q / 1e-10) # a small value to avoid log(0)\n",
    "    return kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_probabilities = precompute_collection_probabilities(all_reports_counts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_reports = [preprocess_text(text) for text in df_test.text.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_scores(query_report):\n",
    "    query_report_counts = Counter(query_report)\n",
    "    mu = 0.1\n",
    "    kl_scores = []\n",
    "\n",
    "    query_report_probs = {}\n",
    "    for word in vocabulary:\n",
    "      query_report_probs[word] = smoothed_unigram(word, query_report_counts, collection_probabilities, vocabulary, mu)\n",
    "\n",
    "    for i, report_k_counts in enumerate(all_reports_counts):\n",
    "        report_k_probs = {}\n",
    "        for word in vocabulary:\n",
    "          report_k_probs[word] = smoothed_unigram(word, report_k_counts, collection_probabilities, vocabulary, mu)\n",
    "\n",
    "        kl = kl_divergence(query_report_probs, report_k_probs)\n",
    "        kl_scores.append(kl)\n",
    "\n",
    "    return kl_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_contribution_data(\n",
    "        issue_number: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieves the contribution data for a given issue number.\n",
    "\n",
    "        Args:\n",
    "            issue_number (int): The issue number.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Tuple[str, str]]]: A dictionary containing contribution data.\n",
    "        \"\"\"\n",
    "        contributions = {}\n",
    "        issue_file = f\"{issue_number}.json\"\n",
    "        last_assignment = None\n",
    "\n",
    "        with open(os.path.join(\"/home/mdafifal.mamun/notebooks/triagerX/data/openj9/issue_data\", issue_file), \"r\") as file:\n",
    "            issue = json.load(file)\n",
    "            assignees = issue.get(\"assignees\", [])\n",
    "            assignee_logins = (\n",
    "                [(assignee[\"login\"], None) for assignee in assignees]\n",
    "                if assignees\n",
    "                else []\n",
    "            )\n",
    "            contributions[\"direct_assignment\"] = assignee_logins\n",
    "            timeline = issue.get(\"timeline_data\", [])\n",
    "            pull_requests, commits, discussion = [], [], []\n",
    "\n",
    "            for timeline_event in timeline:\n",
    "                event = timeline_event.get(\"event\")\n",
    "                created_at = timeline_event.get(\"created_at\")\n",
    "                actor = timeline_event.get(\"actor\", {})\n",
    "\n",
    "                if not actor:\n",
    "                    continue\n",
    "\n",
    "                actor = actor.get(\"login\")\n",
    "\n",
    "                if event == \"cross-referenced\" and timeline_event[\"source\"].get(\n",
    "                    \"issue\", {}\n",
    "                ).get(\"pull_request\"):\n",
    "                    pull_requests.append((actor, created_at))\n",
    "                    last_assignment = actor\n",
    "                elif event == \"referenced\" and timeline_event.get(\"commit_url\"):\n",
    "                    commits.append((actor, created_at))\n",
    "                    last_assignment = actor\n",
    "                elif event == \"commented\":\n",
    "                    discussion.append((actor, created_at))\n",
    "\n",
    "            contributions[\"pull_request\"] = pull_requests\n",
    "            contributions[\"commits\"] = commits\n",
    "            contributions[\"discussion\"] = discussion\n",
    "            contributions[\"last_assignment\"] = (\n",
    "                [(last_assignment, None)] if last_assignment else []\n",
    "            )\n",
    "\n",
    "        return contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def construct_mdn(reports, developers, df_train):\n",
    "    \"\"\"Constructs the Multi-Developer Network (MDN) using commit and discussion data.\"\"\"\n",
    "\n",
    "    network = defaultdict(lambda: {\"commits\": defaultdict(int), \"comments\": defaultdict(int)})\n",
    "\n",
    "    for report_id in reports:\n",
    "        report_data = get_contribution_data(report_id)  # Get report data from DataFrame\n",
    "        owner = df_train[df_train[\"issue_number\"] == report_id].iloc[0][\"owner\"] # Get the assignee/owner\n",
    "\n",
    "        for commit_info in report_data[\"commits\"]:\n",
    "            committer = commit_info[0]\n",
    "            if committer != owner and committer in developers: # Only add if the committer is not the owner\n",
    "                network[committer][\"commits\"][owner] += 1  # Increment commit count\n",
    "\n",
    "        for comment_info in report_data[\"discussion\"]:\n",
    "            commenter = comment_info[0]\n",
    "            if commenter != owner and commenter in developers:  # Only add if commenter is not the owner\n",
    "                network[commenter][\"comments\"][owner] += 1 # Increment comment count\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def normalize_edge_weights(network):\n",
    "    \"\"\"Normalizes edge weights (commits and comments) in the MDN.\"\"\"\n",
    "\n",
    "    # 1. Calculate Maximum Commit and Comment Counts Globally\n",
    "    max_commit = 0  # Initialize globally\n",
    "    max_comment = 0  # Initialize globally\n",
    "\n",
    "    for dev1, interactions in network.items():\n",
    "        for dev2, commits in interactions[\"commits\"].items():\n",
    "            max_commit = max(max_commit, commits)  # Find global max commit\n",
    "        for dev2, comments in interactions[\"comments\"].items():\n",
    "            max_comment = max(max_comment, comments)  # Find global max comment\n",
    "\n",
    "    for dev1, interactions in network.items():\n",
    "        for dev2 in interactions[\"commits\"]:\n",
    "            network[dev1][\"commits\"][dev2] = network[dev1][\"commits\"][dev2] / max_commit + 0.00001\n",
    "\n",
    "        for dev2 in interactions[\"comments\"]:\n",
    "            network[dev1][\"comments\"][dev2] = network[dev1][\"comments\"][dev2] / max_comment + 0.00001\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "def calculate_developer_rank(developer_name, network):\n",
    "    \"\"\"Calculates the ranking score for a developer.\"\"\"\n",
    "\n",
    "    interactions = network.get(developer_name, {\"commits\": {}, \"comments\": {}})\n",
    "    rank = 0\n",
    "    for dev2, commit_weight in interactions[\"commits\"].items():\n",
    "        comment_weight = interactions[\"comments\"].get(dev2, 0)\n",
    "        rank += commit_weight + comment_weight\n",
    "\n",
    "    return rank\n",
    "\n",
    "def rank_developers(network):\n",
    "    \"\"\"Ranks developers based on their ranking scores.\"\"\"\n",
    "\n",
    "    developer_ranks = [(dev, calculate_developer_rank(dev, network)) for dev in network]\n",
    "    developer_ranks.sort(key=lambda x: x[1], reverse=True)\n",
    "    return developer_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for bug_id: 0\n",
      "{'keithc-ca', 'ghost', 'andrewcraik', 'Thihup', 'tajila', 'R-Santhir', 'nbhuiyan', 'fjeremic', 'thallium', 'knn-k', 'AdamBrousseau', 'zl-wang', 'fengxue-IS', 'jdmpapin', 'Bromarv', 'cathyzhyi', 'hangshao0', 'edmathew234', 'Spencer-Comin', '0xdaryl', 'apaj', 'shingarov', 'JasonFengJ9', 'mpirvu', 'janvrany', 'DingliZhang', 'mstoodle', 'acrowthe', 'renfeiw', 'karianna', 'harryyu1994', 'babsingh', 'dsouzai', 'smlambert', 'llxia', 'VermaSh', 'ChengJin01', 'PascalSchumacher', 'alistair23', 'DanHeidinga', 'gacholio', 'PTamis', 'pshipton', 'mleipe', 'sxa', 'AlenBadel', 'vijaysun-omr'}\n",
      "Predicting for bug_id: 1\n",
      "{'pdbain-ibm'}\n",
      "Predicting for bug_id: 2\n",
      "{'pshipton'}\n",
      "Predicting for bug_id: 3\n",
      "{'dsouzai', 'keithc-ca', 'tajila', '0xdaryl', 'jdmpapin', 'JasonFengJ9', 'llxia', 'mpirvu', 'a7ehuo', 'ymanton', 'pshipton', 'mingweiarthurli', 'LongyuZhang', 'hzongaro', 'babsingh'}\n",
      "Predicting for bug_id: 4\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "\n",
    "for bug_id in range(5):\n",
    "    print(\"Predicting for bug_id:\", bug_id)\n",
    "    query_report = all_test_reports[bug_id]\n",
    "    kl_scores = get_kl_scores(query_report)\n",
    "    kl_scores = dict(zip(df_train.issue_number.tolist(), kl_scores))\n",
    "    top_similar_issues = sorted(kl_scores.items(), key=lambda x: x[1])[:20]\n",
    "\n",
    "    # Check if component is not null\n",
    "    query_component = df_test.iloc[bug_id][\"component\"]\n",
    "    similar_component_issues = None\n",
    "    if not pd.isnull(query_component):\n",
    "        similar_component_issues = df_train[df_train[\"component\"] == query_component]\n",
    "\n",
    "    developer_set = set()\n",
    "    similar_issue_set = set()\n",
    "\n",
    "    for issue_number, _ in top_similar_issues:\n",
    "        contributions = get_contribution_data(issue_number)\n",
    "        similar_issue_set.add(issue_number)\n",
    "        for key, value in contributions.items():\n",
    "            for contributor, _ in value:\n",
    "                developer_set.add(contributor)\n",
    "\n",
    "    developer_set2 = set()\n",
    "    # Iterate top similar issues by similar_component_issues\n",
    "    if similar_component_issues is not None:\n",
    "        for issue_number in similar_component_issues.issue_number.tolist():\n",
    "            similar_issue_set.add(issue_number)\n",
    "            contributions = get_contribution_data(issue_number)\n",
    "            for key, value in contributions.items():\n",
    "                for contributor, _ in value:\n",
    "                    developer_set2.add(contributor)\n",
    "                    developer_set = developer_set.intersection(developer_set2) \n",
    "\n",
    "    MDN = construct_mdn(similar_issue_set, developer_set, df_train)\n",
    "    normalized_MDN = normalize_edge_weights(MDN)\n",
    "\n",
    "    ranked_developers = rank_developers(normalized_MDN)\n",
    "    ranks.append(ranked_developers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.2\n",
      "Top-2 accuracy: 0.2\n",
      "Top-3 accuracy: 0.2\n",
      "Top-4 accuracy: 0.2\n",
      "Top-5 accuracy: 0.4\n",
      "Top-6 accuracy: 0.4\n",
      "Top-7 accuracy: 0.4\n",
      "Top-8 accuracy: 0.4\n",
      "Top-9 accuracy: 0.4\n",
      "Top-10 accuracy: 0.4\n",
      "Top-11 accuracy: 0.4\n",
      "Top-12 accuracy: 0.4\n",
      "Top-13 accuracy: 0.4\n",
      "Top-14 accuracy: 0.4\n",
      "Top-15 accuracy: 0.4\n",
      "Top-16 accuracy: 0.4\n",
      "Top-17 accuracy: 0.4\n",
      "Top-18 accuracy: 0.4\n",
      "Top-19 accuracy: 0.4\n",
      "Top-20 accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Compute topk accuracy\n",
    "topk = 20\n",
    "\n",
    "for k in range(1, topk + 1):\n",
    "    predicted_devs = []\n",
    "    for rank in ranks:\n",
    "        predicted_devs.append([dev.lower() for dev, _ in rank[:k]])\n",
    "\n",
    "    owners = df_test.owner.tolist()[:5]\n",
    "    topk_accuracy = sum([1 for owner, rank in zip(owners, predicted_devs) if owner in rank]) / len(owners)\n",
    "    print(f\"Top-{k} accuracy: {topk_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
